{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c88015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skmultilearn.problem_transform import LabelPowerset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c488831a",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72191837",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./WineQT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871f1d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fixed acidity             8.311111\n",
      "volatile acidity          0.531339\n",
      "citric acid               0.268364\n",
      "residual sugar            2.532152\n",
      "chlorides                 0.086933\n",
      "free sulfur dioxide      15.615486\n",
      "total sulfur dioxide     45.914698\n",
      "density                   0.996730\n",
      "pH                        3.311015\n",
      "sulphates                 0.657708\n",
      "alcohol                  10.442111\n",
      "quality                   5.657043\n",
      "Id                      804.969379\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(df,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f65ef495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing total number of labels\n",
    "np.unique(df['quality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af167cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the important meaningful attributes\n",
    "attb_df = df[[\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d4f3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "      <td>1143.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.311111</td>\n",
       "      <td>0.531339</td>\n",
       "      <td>0.268364</td>\n",
       "      <td>2.532152</td>\n",
       "      <td>0.086933</td>\n",
       "      <td>15.615486</td>\n",
       "      <td>45.914698</td>\n",
       "      <td>0.996730</td>\n",
       "      <td>3.311015</td>\n",
       "      <td>0.657708</td>\n",
       "      <td>10.442111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.747595</td>\n",
       "      <td>0.179633</td>\n",
       "      <td>0.196686</td>\n",
       "      <td>1.355917</td>\n",
       "      <td>0.047267</td>\n",
       "      <td>10.250486</td>\n",
       "      <td>32.782130</td>\n",
       "      <td>0.001925</td>\n",
       "      <td>0.156664</td>\n",
       "      <td>0.170399</td>\n",
       "      <td>1.082196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.392500</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.995570</td>\n",
       "      <td>3.205000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>0.996680</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.100000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.997845</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1143.000000       1143.000000  1143.000000     1143.000000   \n",
       "mean        8.311111          0.531339     0.268364        2.532152   \n",
       "std         1.747595          0.179633     0.196686        1.355917   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.392500     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.250000        2.200000   \n",
       "75%         9.100000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1143.000000          1143.000000           1143.000000  1143.000000   \n",
       "mean      0.086933            15.615486             45.914698     0.996730   \n",
       "std       0.047267            10.250486             32.782130     0.001925   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             21.000000     0.995570   \n",
       "50%       0.079000            13.000000             37.000000     0.996680   \n",
       "75%       0.090000            21.000000             61.000000     0.997845   \n",
       "max       0.611000            68.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol  \n",
       "count  1143.000000  1143.000000  1143.000000  \n",
       "mean      3.311015     0.657708    10.442111  \n",
       "std       0.156664     0.170399     1.082196  \n",
       "min       2.740000     0.330000     8.400000  \n",
       "25%       3.205000     0.550000     9.500000  \n",
       "50%       3.310000     0.620000    10.200000  \n",
       "75%       3.400000     0.730000    11.100000  \n",
       "max       4.010000     2.000000    14.900000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attb_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9dfe14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_label = df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9946e258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a bar graph\n",
    "dict_label = dict(dict_label)\n",
    "plt.bar(list(dict_label.keys()),list(dict_label.values()))\n",
    "plt.title('Occurances of labels in Wine dataset')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe3470d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = np.array(df)\n",
    "\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "\"\"\"\n",
    "CHECK AGAIN BOOKMARK\n",
    "\"\"\"\n",
    "\n",
    "# Compute the correlation matrix using numpy\n",
    "corr_matrix = np.corrcoef(dataset[:,:11], rowvar=False)\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04bc6051",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,:11]\n",
    "y = dataset[:,11]\n",
    "\n",
    "# standardize\n",
    "# calc z score for each entry columnwise\n",
    "X = (X - np.mean(X,axis=0))/np.std(X,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d12261d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1143, 11)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d0770f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets keep the split as train : 0.7, val : 0.1, test : 0.2\n",
    "train_ratio = 0.65\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3470d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = dataset.shape[0]\n",
    "\n",
    "train_X = X[:int(train_ratio*entries)]\n",
    "val_X = X[int(train_ratio*entries):int((train_ratio+val_ratio)*entries)]\n",
    "test_X = X[int((train_ratio+val_ratio)*entries):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "834dc290",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = y[:int(train_ratio*entries)]\n",
    "val_y = y[int(train_ratio*entries):int((train_ratio+val_ratio)*entries)]\n",
    "test_y = y[int((train_ratio+val_ratio)*entries):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b6cc52f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(742, 11)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "859da014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "044a63d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(742,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42911bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c21403f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68310d63",
   "metadata": {},
   "source": [
    "<h3>Model training and WandB tuning</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64341fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 4. 5. 6. 7. 8.]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b23a2918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# taken help from chat GPT\n",
    "\n",
    "class MLPClassifier:\n",
    "    def __init__(self, learning_rate=0.01, activation='sigmoid', optimizer='sgd', hidden_layers=[10], epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.epochs = epochs\n",
    "        self.activation_functions = {\n",
    "            'sigmoid': (self._sigmoid, self._sigmoid_derivative),\n",
    "            'tanh': (np.tanh, self._tanh_derivative),\n",
    "            'relu': (self._relu, self._relu_derivative)\n",
    "        }\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "\n",
    "    def _relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def _relu_derivative(self, z):\n",
    "        z[z <= 0] = 0\n",
    "        z[z > 0] = 1\n",
    "        return z\n",
    "\n",
    "    def _tanh_derivative(self, z):\n",
    "        return 1.0 - z**2\n",
    "\n",
    "    def softmax(self, z):\n",
    "        exp_scores = np.exp(z - np.max(z))  # Stability trick to avoid overflow\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def _initialize_weights(self, input_size, output_size):\n",
    "        layer_sizes = [input_size] + self.hidden_layers + [output_size]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01)\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i+1])))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activations = [X]\n",
    "        Zs = []\n",
    "\n",
    "        activation_func, _ = self.activation_functions[self.activation]\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            Z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            Zs.append(Z)\n",
    "            if i == len(self.weights) - 1:  # If it's the last layer, apply softmax\n",
    "                activations.append(self.softmax(Z))\n",
    "            else:\n",
    "                activations.append(activation_func(Z))\n",
    "\n",
    "        return activations, Zs\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        correct_logprobs = -np.log(y_pred[range(m), np.argmax(y_true, axis=1)])\n",
    "        loss = np.sum(correct_logprobs) / m\n",
    "        return loss\n",
    "\n",
    "    def back_propagation(self, y, activations, Zs):\n",
    "        m = y.shape[0]\n",
    "        grads = {\"dw\": [], \"db\": []}\n",
    "\n",
    "        dZ = activations[-1] - y\n",
    "        dW = np.dot(activations[-2].T, dZ) / m\n",
    "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        grads[\"dw\"].append(dW)\n",
    "        grads[\"db\"].append(db)\n",
    "\n",
    "        _, activation_derivative = self.activation_functions[self.activation]\n",
    "\n",
    "        for i in range(len(self.hidden_layers), 0, -1):\n",
    "            dZ = np.dot(dZ, self.weights[i].T) * activation_derivative(activations[i])\n",
    "            dW = np.dot(activations[i-1].T, dZ) / m\n",
    "            db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            grads[\"dw\"].insert(0, dW)\n",
    "            grads[\"db\"].insert(0, db)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_weights(self, grads):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads[\"dw\"][i]\n",
    "            self.biases[i] -= self.learning_rate * grads[\"db\"][i]\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        self._initialize_weights(X_train.shape[1], y_train.shape[1])\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            if self.optimizer == 'sgd':\n",
    "                for i in range(X_train.shape[0]):\n",
    "                    activations, Zs = self.forward_propagation(X_train[i:i+1])\n",
    "                    grads = self.back_propagation(y_train[i:i+1], activations, Zs)\n",
    "                    self.update_weights(grads)\n",
    "            elif self.optimizer == 'bgd':\n",
    "                activations, Zs = self.forward_propagation(X_train)\n",
    "                grads = self.back_propagation(y_train, activations, Zs)\n",
    "                self.update_weights(grads)\n",
    "            elif self.optimizer == 'mbgd':\n",
    "                batch_size = 32\n",
    "                for i in range(0, X_train.shape[0], batch_size):\n",
    "                    activations, Zs = self.forward_propagation(X_train[i:i+batch_size])\n",
    "                    grads = self.back_propagation(y_train[i:i+batch_size], activations, Zs)\n",
    "                    self.update_weights(grads)\n",
    "\n",
    "            train_activations, _ = self.forward_propagation(X_train)\n",
    "            train_loss = self.compute_loss(y_train, train_activations[-1])\n",
    "            train_accuracy = np.mean(np.argmax(train_activations[-1], axis=1) == np.argmax(y_train, axis=1))\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_activations, _ = self.forward_propagation(X_val)\n",
    "                val_loss = self.compute_loss(y_val, val_activations[-1])\n",
    "                val_accuracy = np.mean(np.argmax(val_activations[-1], axis=1) == np.argmax(y_val, axis=1))\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs} - train_loss: {train_loss:.4f} - train_accuracy: {train_accuracy:.4f} - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs} - train_loss: {train_loss:.4f} - train_accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "            # wandb.log({\n",
    "            #     \"epoch\": epoch,\n",
    "            #     \"train_loss\": train_loss,\n",
    "            #     \"train_accuracy\": train_accuracy,\n",
    "            #     \"val_loss\": val_loss,\n",
    "            #     \"val_accuracy\": val_accuracy\n",
    "            # })\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        return np.argmax(activations[-1], axis=1)\n",
    "\n",
    "    def inference(self, test_X, test_Y):\n",
    "        y_pred = self.predict(test_X)\n",
    "        y_true_labels = np.argmax(test_Y, axis=1)\n",
    "        return classification_report(y_true_labels, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9ac1c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(742, 11) (742, 6) (172, 11) (172, 6)\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "train_y = train_y.reshape((-1,1))\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_true = one_hot_encoder.fit_transform(train_y)\n",
    "\n",
    "val_y = val_y.reshape((-1,1))\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_true_val = one_hot_encoder.fit_transform(val_y)\n",
    "\n",
    "print(train_X.shape,y_true.shape,val_X.shape,y_true_val.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e97e9c9",
   "metadata": {},
   "source": [
    "### Training on the dataset and reporting validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7a67173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000 - train_loss: 1.2078 - train_accuracy: 0.4111 - val_loss: 1.1714 - val_accuracy: 0.4419\n",
      "Epoch 2/1000 - train_loss: 1.1893 - train_accuracy: 0.4111 - val_loss: 1.1565 - val_accuracy: 0.4419\n",
      "Epoch 3/1000 - train_loss: 1.1844 - train_accuracy: 0.4111 - val_loss: 1.1559 - val_accuracy: 0.4419\n",
      "Epoch 4/1000 - train_loss: 1.1823 - train_accuracy: 0.4111 - val_loss: 1.1575 - val_accuracy: 0.4419\n",
      "Epoch 5/1000 - train_loss: 1.1811 - train_accuracy: 0.4111 - val_loss: 1.1594 - val_accuracy: 0.4419\n",
      "Epoch 6/1000 - train_loss: 1.1803 - train_accuracy: 0.4111 - val_loss: 1.1612 - val_accuracy: 0.4419\n",
      "Epoch 7/1000 - train_loss: 1.1797 - train_accuracy: 0.4111 - val_loss: 1.1628 - val_accuracy: 0.4419\n",
      "Epoch 8/1000 - train_loss: 1.1792 - train_accuracy: 0.4111 - val_loss: 1.1642 - val_accuracy: 0.4419\n",
      "Epoch 9/1000 - train_loss: 1.1789 - train_accuracy: 0.4111 - val_loss: 1.1654 - val_accuracy: 0.4419\n",
      "Epoch 10/1000 - train_loss: 1.1786 - train_accuracy: 0.4111 - val_loss: 1.1665 - val_accuracy: 0.4419\n",
      "Epoch 11/1000 - train_loss: 1.1783 - train_accuracy: 0.4111 - val_loss: 1.1675 - val_accuracy: 0.4419\n",
      "Epoch 12/1000 - train_loss: 1.1780 - train_accuracy: 0.4111 - val_loss: 1.1683 - val_accuracy: 0.4419\n",
      "Epoch 13/1000 - train_loss: 1.1778 - train_accuracy: 0.4111 - val_loss: 1.1691 - val_accuracy: 0.4419\n",
      "Epoch 14/1000 - train_loss: 1.1775 - train_accuracy: 0.4111 - val_loss: 1.1698 - val_accuracy: 0.4419\n",
      "Epoch 15/1000 - train_loss: 1.1773 - train_accuracy: 0.4111 - val_loss: 1.1705 - val_accuracy: 0.4419\n",
      "Epoch 16/1000 - train_loss: 1.1770 - train_accuracy: 0.4111 - val_loss: 1.1710 - val_accuracy: 0.4419\n",
      "Epoch 17/1000 - train_loss: 1.1767 - train_accuracy: 0.4111 - val_loss: 1.1715 - val_accuracy: 0.4419\n",
      "Epoch 18/1000 - train_loss: 1.1764 - train_accuracy: 0.4111 - val_loss: 1.1720 - val_accuracy: 0.4419\n",
      "Epoch 19/1000 - train_loss: 1.1761 - train_accuracy: 0.4111 - val_loss: 1.1724 - val_accuracy: 0.4419\n",
      "Epoch 20/1000 - train_loss: 1.1757 - train_accuracy: 0.4111 - val_loss: 1.1727 - val_accuracy: 0.4419\n",
      "Epoch 21/1000 - train_loss: 1.1752 - train_accuracy: 0.4111 - val_loss: 1.1729 - val_accuracy: 0.4419\n",
      "Epoch 22/1000 - train_loss: 1.1747 - train_accuracy: 0.4111 - val_loss: 1.1731 - val_accuracy: 0.4419\n",
      "Epoch 23/1000 - train_loss: 1.1740 - train_accuracy: 0.4111 - val_loss: 1.1731 - val_accuracy: 0.4419\n",
      "Epoch 24/1000 - train_loss: 1.1732 - train_accuracy: 0.4111 - val_loss: 1.1731 - val_accuracy: 0.4419\n",
      "Epoch 25/1000 - train_loss: 1.1723 - train_accuracy: 0.4111 - val_loss: 1.1728 - val_accuracy: 0.4419\n",
      "Epoch 26/1000 - train_loss: 1.1711 - train_accuracy: 0.4111 - val_loss: 1.1724 - val_accuracy: 0.4419\n",
      "Epoch 27/1000 - train_loss: 1.1696 - train_accuracy: 0.4111 - val_loss: 1.1718 - val_accuracy: 0.4419\n",
      "Epoch 28/1000 - train_loss: 1.1677 - train_accuracy: 0.4111 - val_loss: 1.1709 - val_accuracy: 0.4419\n",
      "Epoch 29/1000 - train_loss: 1.1654 - train_accuracy: 0.4111 - val_loss: 1.1696 - val_accuracy: 0.4419\n",
      "Epoch 30/1000 - train_loss: 1.1625 - train_accuracy: 0.4111 - val_loss: 1.1678 - val_accuracy: 0.4419\n",
      "Epoch 31/1000 - train_loss: 1.1588 - train_accuracy: 0.4111 - val_loss: 1.1655 - val_accuracy: 0.4419\n",
      "Epoch 32/1000 - train_loss: 1.1543 - train_accuracy: 0.4097 - val_loss: 1.1624 - val_accuracy: 0.4419\n",
      "Epoch 33/1000 - train_loss: 1.1486 - train_accuracy: 0.4151 - val_loss: 1.1584 - val_accuracy: 0.4477\n",
      "Epoch 34/1000 - train_loss: 1.1417 - train_accuracy: 0.4434 - val_loss: 1.1534 - val_accuracy: 0.4593\n",
      "Epoch 35/1000 - train_loss: 1.1334 - train_accuracy: 0.4663 - val_loss: 1.1473 - val_accuracy: 0.4767\n",
      "Epoch 36/1000 - train_loss: 1.1236 - train_accuracy: 0.4879 - val_loss: 1.1400 - val_accuracy: 0.5174\n",
      "Epoch 37/1000 - train_loss: 1.1124 - train_accuracy: 0.5000 - val_loss: 1.1314 - val_accuracy: 0.5349\n",
      "Epoch 38/1000 - train_loss: 1.1000 - train_accuracy: 0.5094 - val_loss: 1.1219 - val_accuracy: 0.5407\n",
      "Epoch 39/1000 - train_loss: 1.0866 - train_accuracy: 0.5364 - val_loss: 1.1115 - val_accuracy: 0.5349\n",
      "Epoch 40/1000 - train_loss: 1.0727 - train_accuracy: 0.5418 - val_loss: 1.1007 - val_accuracy: 0.5465\n",
      "Epoch 41/1000 - train_loss: 1.0588 - train_accuracy: 0.5526 - val_loss: 1.0899 - val_accuracy: 0.5465\n",
      "Epoch 42/1000 - train_loss: 1.0453 - train_accuracy: 0.5674 - val_loss: 1.0795 - val_accuracy: 0.5640\n",
      "Epoch 43/1000 - train_loss: 1.0327 - train_accuracy: 0.5728 - val_loss: 1.0698 - val_accuracy: 0.5581\n",
      "Epoch 44/1000 - train_loss: 1.0213 - train_accuracy: 0.5660 - val_loss: 1.0612 - val_accuracy: 0.5581\n",
      "Epoch 45/1000 - train_loss: 1.0111 - train_accuracy: 0.5674 - val_loss: 1.0536 - val_accuracy: 0.5581\n",
      "Epoch 46/1000 - train_loss: 1.0022 - train_accuracy: 0.5728 - val_loss: 1.0471 - val_accuracy: 0.5640\n",
      "Epoch 47/1000 - train_loss: 0.9945 - train_accuracy: 0.5755 - val_loss: 1.0416 - val_accuracy: 0.5581\n",
      "Epoch 48/1000 - train_loss: 0.9880 - train_accuracy: 0.5809 - val_loss: 1.0370 - val_accuracy: 0.5581\n",
      "Epoch 49/1000 - train_loss: 0.9824 - train_accuracy: 0.5863 - val_loss: 1.0332 - val_accuracy: 0.5756\n",
      "Epoch 50/1000 - train_loss: 0.9777 - train_accuracy: 0.5903 - val_loss: 1.0300 - val_accuracy: 0.5814\n",
      "Epoch 51/1000 - train_loss: 0.9736 - train_accuracy: 0.5930 - val_loss: 1.0273 - val_accuracy: 0.5814\n",
      "Epoch 52/1000 - train_loss: 0.9701 - train_accuracy: 0.5943 - val_loss: 1.0250 - val_accuracy: 0.5930\n",
      "Epoch 53/1000 - train_loss: 0.9670 - train_accuracy: 0.5943 - val_loss: 1.0230 - val_accuracy: 0.5930\n",
      "Epoch 54/1000 - train_loss: 0.9643 - train_accuracy: 0.5943 - val_loss: 1.0212 - val_accuracy: 0.5930\n",
      "Epoch 55/1000 - train_loss: 0.9619 - train_accuracy: 0.5930 - val_loss: 1.0197 - val_accuracy: 0.5930\n",
      "Epoch 56/1000 - train_loss: 0.9598 - train_accuracy: 0.5984 - val_loss: 1.0182 - val_accuracy: 0.5930\n",
      "Epoch 57/1000 - train_loss: 0.9578 - train_accuracy: 0.6011 - val_loss: 1.0169 - val_accuracy: 0.5872\n",
      "Epoch 58/1000 - train_loss: 0.9561 - train_accuracy: 0.5997 - val_loss: 1.0157 - val_accuracy: 0.5988\n",
      "Epoch 59/1000 - train_loss: 0.9544 - train_accuracy: 0.5984 - val_loss: 1.0146 - val_accuracy: 0.5930\n",
      "Epoch 60/1000 - train_loss: 0.9529 - train_accuracy: 0.5984 - val_loss: 1.0136 - val_accuracy: 0.5930\n",
      "Epoch 61/1000 - train_loss: 0.9515 - train_accuracy: 0.5984 - val_loss: 1.0127 - val_accuracy: 0.5930\n",
      "Epoch 62/1000 - train_loss: 0.9502 - train_accuracy: 0.6011 - val_loss: 1.0118 - val_accuracy: 0.5930\n",
      "Epoch 63/1000 - train_loss: 0.9489 - train_accuracy: 0.5984 - val_loss: 1.0110 - val_accuracy: 0.5930\n",
      "Epoch 64/1000 - train_loss: 0.9477 - train_accuracy: 0.5984 - val_loss: 1.0103 - val_accuracy: 0.5988\n",
      "Epoch 65/1000 - train_loss: 0.9467 - train_accuracy: 0.5997 - val_loss: 1.0097 - val_accuracy: 0.5988\n",
      "Epoch 66/1000 - train_loss: 0.9456 - train_accuracy: 0.5997 - val_loss: 1.0091 - val_accuracy: 0.5988\n",
      "Epoch 67/1000 - train_loss: 0.9447 - train_accuracy: 0.5997 - val_loss: 1.0087 - val_accuracy: 0.5988\n",
      "Epoch 68/1000 - train_loss: 0.9438 - train_accuracy: 0.6011 - val_loss: 1.0083 - val_accuracy: 0.5988\n",
      "Epoch 69/1000 - train_loss: 0.9429 - train_accuracy: 0.6011 - val_loss: 1.0079 - val_accuracy: 0.6047\n",
      "Epoch 70/1000 - train_loss: 0.9422 - train_accuracy: 0.6011 - val_loss: 1.0077 - val_accuracy: 0.6047\n",
      "Epoch 71/1000 - train_loss: 0.9414 - train_accuracy: 0.6024 - val_loss: 1.0075 - val_accuracy: 0.6047\n",
      "Epoch 72/1000 - train_loss: 0.9407 - train_accuracy: 0.6038 - val_loss: 1.0073 - val_accuracy: 0.6047\n",
      "Epoch 73/1000 - train_loss: 0.9401 - train_accuracy: 0.6038 - val_loss: 1.0072 - val_accuracy: 0.6047\n",
      "Epoch 74/1000 - train_loss: 0.9395 - train_accuracy: 0.6038 - val_loss: 1.0071 - val_accuracy: 0.6047\n",
      "Epoch 75/1000 - train_loss: 0.9389 - train_accuracy: 0.6038 - val_loss: 1.0071 - val_accuracy: 0.6105\n",
      "Epoch 76/1000 - train_loss: 0.9384 - train_accuracy: 0.6038 - val_loss: 1.0071 - val_accuracy: 0.6105\n",
      "Epoch 77/1000 - train_loss: 0.9378 - train_accuracy: 0.6038 - val_loss: 1.0071 - val_accuracy: 0.6105\n",
      "Epoch 78/1000 - train_loss: 0.9373 - train_accuracy: 0.6038 - val_loss: 1.0071 - val_accuracy: 0.6105\n",
      "Epoch 79/1000 - train_loss: 0.9369 - train_accuracy: 0.6051 - val_loss: 1.0071 - val_accuracy: 0.6105\n",
      "Epoch 80/1000 - train_loss: 0.9364 - train_accuracy: 0.6038 - val_loss: 1.0071 - val_accuracy: 0.6163\n",
      "Epoch 81/1000 - train_loss: 0.9360 - train_accuracy: 0.6078 - val_loss: 1.0071 - val_accuracy: 0.6163\n",
      "Epoch 82/1000 - train_loss: 0.9355 - train_accuracy: 0.6119 - val_loss: 1.0072 - val_accuracy: 0.6163\n",
      "Epoch 83/1000 - train_loss: 0.9351 - train_accuracy: 0.6119 - val_loss: 1.0072 - val_accuracy: 0.6105\n",
      "Epoch 84/1000 - train_loss: 0.9347 - train_accuracy: 0.6105 - val_loss: 1.0072 - val_accuracy: 0.6105\n",
      "Epoch 85/1000 - train_loss: 0.9343 - train_accuracy: 0.6105 - val_loss: 1.0072 - val_accuracy: 0.6105\n",
      "Epoch 86/1000 - train_loss: 0.9340 - train_accuracy: 0.6105 - val_loss: 1.0073 - val_accuracy: 0.6105\n",
      "Epoch 87/1000 - train_loss: 0.9336 - train_accuracy: 0.6092 - val_loss: 1.0073 - val_accuracy: 0.6163\n",
      "Epoch 88/1000 - train_loss: 0.9332 - train_accuracy: 0.6119 - val_loss: 1.0073 - val_accuracy: 0.6163\n",
      "Epoch 89/1000 - train_loss: 0.9329 - train_accuracy: 0.6119 - val_loss: 1.0074 - val_accuracy: 0.6163\n",
      "Epoch 90/1000 - train_loss: 0.9325 - train_accuracy: 0.6119 - val_loss: 1.0074 - val_accuracy: 0.6163\n",
      "Epoch 91/1000 - train_loss: 0.9322 - train_accuracy: 0.6119 - val_loss: 1.0074 - val_accuracy: 0.6163\n",
      "Epoch 92/1000 - train_loss: 0.9318 - train_accuracy: 0.6105 - val_loss: 1.0074 - val_accuracy: 0.6163\n",
      "Epoch 93/1000 - train_loss: 0.9315 - train_accuracy: 0.6105 - val_loss: 1.0074 - val_accuracy: 0.6163\n",
      "Epoch 94/1000 - train_loss: 0.9311 - train_accuracy: 0.6119 - val_loss: 1.0075 - val_accuracy: 0.6163\n",
      "Epoch 95/1000 - train_loss: 0.9308 - train_accuracy: 0.6119 - val_loss: 1.0075 - val_accuracy: 0.6163\n",
      "Epoch 96/1000 - train_loss: 0.9305 - train_accuracy: 0.6119 - val_loss: 1.0075 - val_accuracy: 0.6163\n",
      "Epoch 97/1000 - train_loss: 0.9301 - train_accuracy: 0.6132 - val_loss: 1.0075 - val_accuracy: 0.6163\n",
      "Epoch 98/1000 - train_loss: 0.9298 - train_accuracy: 0.6119 - val_loss: 1.0075 - val_accuracy: 0.6163\n",
      "Epoch 99/1000 - train_loss: 0.9294 - train_accuracy: 0.6119 - val_loss: 1.0075 - val_accuracy: 0.6163\n",
      "Epoch 100/1000 - train_loss: 0.9290 - train_accuracy: 0.6105 - val_loss: 1.0075 - val_accuracy: 0.6163\n",
      "Epoch 101/1000 - train_loss: 0.9287 - train_accuracy: 0.6105 - val_loss: 1.0075 - val_accuracy: 0.6163\n",
      "Epoch 102/1000 - train_loss: 0.9283 - train_accuracy: 0.6119 - val_loss: 1.0076 - val_accuracy: 0.6163\n",
      "Epoch 103/1000 - train_loss: 0.9279 - train_accuracy: 0.6119 - val_loss: 1.0076 - val_accuracy: 0.6163\n",
      "Epoch 104/1000 - train_loss: 0.9276 - train_accuracy: 0.6132 - val_loss: 1.0076 - val_accuracy: 0.6279\n",
      "Epoch 105/1000 - train_loss: 0.9272 - train_accuracy: 0.6132 - val_loss: 1.0076 - val_accuracy: 0.6279\n",
      "Epoch 106/1000 - train_loss: 0.9268 - train_accuracy: 0.6132 - val_loss: 1.0076 - val_accuracy: 0.6279\n",
      "Epoch 107/1000 - train_loss: 0.9264 - train_accuracy: 0.6159 - val_loss: 1.0076 - val_accuracy: 0.6279\n",
      "Epoch 108/1000 - train_loss: 0.9260 - train_accuracy: 0.6159 - val_loss: 1.0077 - val_accuracy: 0.6279\n",
      "Epoch 109/1000 - train_loss: 0.9257 - train_accuracy: 0.6173 - val_loss: 1.0077 - val_accuracy: 0.6279\n",
      "Epoch 110/1000 - train_loss: 0.9253 - train_accuracy: 0.6186 - val_loss: 1.0077 - val_accuracy: 0.6279\n",
      "Epoch 111/1000 - train_loss: 0.9249 - train_accuracy: 0.6186 - val_loss: 1.0077 - val_accuracy: 0.6279\n",
      "Epoch 112/1000 - train_loss: 0.9245 - train_accuracy: 0.6199 - val_loss: 1.0077 - val_accuracy: 0.6279\n",
      "Epoch 113/1000 - train_loss: 0.9241 - train_accuracy: 0.6213 - val_loss: 1.0078 - val_accuracy: 0.6279\n",
      "Epoch 114/1000 - train_loss: 0.9237 - train_accuracy: 0.6213 - val_loss: 1.0078 - val_accuracy: 0.6279\n",
      "Epoch 115/1000 - train_loss: 0.9233 - train_accuracy: 0.6253 - val_loss: 1.0078 - val_accuracy: 0.6279\n",
      "Epoch 116/1000 - train_loss: 0.9228 - train_accuracy: 0.6253 - val_loss: 1.0077 - val_accuracy: 0.6279\n",
      "Epoch 117/1000 - train_loss: 0.9224 - train_accuracy: 0.6253 - val_loss: 1.0077 - val_accuracy: 0.6337\n",
      "Epoch 118/1000 - train_loss: 0.9220 - train_accuracy: 0.6253 - val_loss: 1.0077 - val_accuracy: 0.6337\n",
      "Epoch 119/1000 - train_loss: 0.9215 - train_accuracy: 0.6253 - val_loss: 1.0076 - val_accuracy: 0.6337\n",
      "Epoch 120/1000 - train_loss: 0.9211 - train_accuracy: 0.6253 - val_loss: 1.0075 - val_accuracy: 0.6279\n",
      "Epoch 121/1000 - train_loss: 0.9206 - train_accuracy: 0.6280 - val_loss: 1.0075 - val_accuracy: 0.6279\n",
      "Epoch 122/1000 - train_loss: 0.9201 - train_accuracy: 0.6280 - val_loss: 1.0074 - val_accuracy: 0.6279\n",
      "Epoch 123/1000 - train_loss: 0.9196 - train_accuracy: 0.6280 - val_loss: 1.0072 - val_accuracy: 0.6279\n",
      "Epoch 124/1000 - train_loss: 0.9191 - train_accuracy: 0.6280 - val_loss: 1.0071 - val_accuracy: 0.6279\n",
      "Epoch 125/1000 - train_loss: 0.9186 - train_accuracy: 0.6294 - val_loss: 1.0070 - val_accuracy: 0.6279\n",
      "Epoch 126/1000 - train_loss: 0.9180 - train_accuracy: 0.6267 - val_loss: 1.0068 - val_accuracy: 0.6279\n",
      "Epoch 127/1000 - train_loss: 0.9175 - train_accuracy: 0.6267 - val_loss: 1.0066 - val_accuracy: 0.6279\n",
      "Epoch 128/1000 - train_loss: 0.9169 - train_accuracy: 0.6280 - val_loss: 1.0064 - val_accuracy: 0.6279\n",
      "Epoch 129/1000 - train_loss: 0.9163 - train_accuracy: 0.6294 - val_loss: 1.0062 - val_accuracy: 0.6337\n",
      "Epoch 130/1000 - train_loss: 0.9157 - train_accuracy: 0.6280 - val_loss: 1.0060 - val_accuracy: 0.6337\n",
      "Epoch 131/1000 - train_loss: 0.9151 - train_accuracy: 0.6267 - val_loss: 1.0058 - val_accuracy: 0.6337\n",
      "Epoch 132/1000 - train_loss: 0.9144 - train_accuracy: 0.6294 - val_loss: 1.0056 - val_accuracy: 0.6337\n",
      "Epoch 133/1000 - train_loss: 0.9138 - train_accuracy: 0.6294 - val_loss: 1.0054 - val_accuracy: 0.6337\n",
      "Epoch 134/1000 - train_loss: 0.9131 - train_accuracy: 0.6294 - val_loss: 1.0052 - val_accuracy: 0.6279\n",
      "Epoch 135/1000 - train_loss: 0.9124 - train_accuracy: 0.6294 - val_loss: 1.0049 - val_accuracy: 0.6279\n",
      "Epoch 136/1000 - train_loss: 0.9117 - train_accuracy: 0.6294 - val_loss: 1.0047 - val_accuracy: 0.6337\n",
      "Epoch 137/1000 - train_loss: 0.9109 - train_accuracy: 0.6321 - val_loss: 1.0045 - val_accuracy: 0.6337\n",
      "Epoch 138/1000 - train_loss: 0.9102 - train_accuracy: 0.6307 - val_loss: 1.0043 - val_accuracy: 0.6337\n",
      "Epoch 139/1000 - train_loss: 0.9095 - train_accuracy: 0.6307 - val_loss: 1.0041 - val_accuracy: 0.6337\n",
      "Epoch 140/1000 - train_loss: 0.9087 - train_accuracy: 0.6321 - val_loss: 1.0038 - val_accuracy: 0.6395\n",
      "Epoch 141/1000 - train_loss: 0.9080 - train_accuracy: 0.6334 - val_loss: 1.0036 - val_accuracy: 0.6395\n",
      "Epoch 142/1000 - train_loss: 0.9072 - train_accuracy: 0.6321 - val_loss: 1.0034 - val_accuracy: 0.6395\n",
      "Epoch 143/1000 - train_loss: 0.9064 - train_accuracy: 0.6334 - val_loss: 1.0032 - val_accuracy: 0.6395\n",
      "Epoch 144/1000 - train_loss: 0.9056 - train_accuracy: 0.6334 - val_loss: 1.0030 - val_accuracy: 0.6395\n",
      "Epoch 145/1000 - train_loss: 0.9049 - train_accuracy: 0.6348 - val_loss: 1.0028 - val_accuracy: 0.6395\n",
      "Epoch 146/1000 - train_loss: 0.9041 - train_accuracy: 0.6334 - val_loss: 1.0026 - val_accuracy: 0.6395\n",
      "Epoch 147/1000 - train_loss: 0.9033 - train_accuracy: 0.6348 - val_loss: 1.0024 - val_accuracy: 0.6395\n",
      "Epoch 148/1000 - train_loss: 0.9025 - train_accuracy: 0.6361 - val_loss: 1.0022 - val_accuracy: 0.6395\n",
      "Epoch 149/1000 - train_loss: 0.9017 - train_accuracy: 0.6402 - val_loss: 1.0020 - val_accuracy: 0.6395\n",
      "Epoch 150/1000 - train_loss: 0.9009 - train_accuracy: 0.6388 - val_loss: 1.0018 - val_accuracy: 0.6395\n",
      "Epoch 151/1000 - train_loss: 0.9000 - train_accuracy: 0.6388 - val_loss: 1.0017 - val_accuracy: 0.6395\n",
      "Epoch 152/1000 - train_loss: 0.8992 - train_accuracy: 0.6361 - val_loss: 1.0015 - val_accuracy: 0.6395\n",
      "Epoch 153/1000 - train_loss: 0.8984 - train_accuracy: 0.6388 - val_loss: 1.0013 - val_accuracy: 0.6337\n",
      "Epoch 154/1000 - train_loss: 0.8976 - train_accuracy: 0.6388 - val_loss: 1.0011 - val_accuracy: 0.6337\n",
      "Epoch 155/1000 - train_loss: 0.8967 - train_accuracy: 0.6388 - val_loss: 1.0009 - val_accuracy: 0.6337\n",
      "Epoch 156/1000 - train_loss: 0.8959 - train_accuracy: 0.6429 - val_loss: 1.0007 - val_accuracy: 0.6337\n",
      "Epoch 157/1000 - train_loss: 0.8950 - train_accuracy: 0.6415 - val_loss: 1.0005 - val_accuracy: 0.6279\n",
      "Epoch 158/1000 - train_loss: 0.8942 - train_accuracy: 0.6429 - val_loss: 1.0002 - val_accuracy: 0.6279\n",
      "Epoch 159/1000 - train_loss: 0.8933 - train_accuracy: 0.6415 - val_loss: 1.0000 - val_accuracy: 0.6279\n",
      "Epoch 160/1000 - train_loss: 0.8924 - train_accuracy: 0.6388 - val_loss: 0.9998 - val_accuracy: 0.6279\n",
      "Epoch 161/1000 - train_loss: 0.8915 - train_accuracy: 0.6402 - val_loss: 0.9996 - val_accuracy: 0.6279\n",
      "Epoch 162/1000 - train_loss: 0.8906 - train_accuracy: 0.6402 - val_loss: 0.9993 - val_accuracy: 0.6279\n",
      "Epoch 163/1000 - train_loss: 0.8897 - train_accuracy: 0.6415 - val_loss: 0.9991 - val_accuracy: 0.6279\n",
      "Epoch 164/1000 - train_loss: 0.8888 - train_accuracy: 0.6429 - val_loss: 0.9988 - val_accuracy: 0.6279\n",
      "Epoch 165/1000 - train_loss: 0.8878 - train_accuracy: 0.6442 - val_loss: 0.9986 - val_accuracy: 0.6337\n",
      "Epoch 166/1000 - train_loss: 0.8869 - train_accuracy: 0.6442 - val_loss: 0.9983 - val_accuracy: 0.6337\n",
      "Epoch 167/1000 - train_loss: 0.8860 - train_accuracy: 0.6442 - val_loss: 0.9981 - val_accuracy: 0.6337\n",
      "Epoch 168/1000 - train_loss: 0.8850 - train_accuracy: 0.6442 - val_loss: 0.9978 - val_accuracy: 0.6337\n",
      "Epoch 169/1000 - train_loss: 0.8840 - train_accuracy: 0.6456 - val_loss: 0.9976 - val_accuracy: 0.6337\n",
      "Epoch 170/1000 - train_loss: 0.8830 - train_accuracy: 0.6456 - val_loss: 0.9973 - val_accuracy: 0.6337\n",
      "Epoch 171/1000 - train_loss: 0.8821 - train_accuracy: 0.6456 - val_loss: 0.9970 - val_accuracy: 0.6279\n",
      "Epoch 172/1000 - train_loss: 0.8811 - train_accuracy: 0.6496 - val_loss: 0.9968 - val_accuracy: 0.6337\n",
      "Epoch 173/1000 - train_loss: 0.8801 - train_accuracy: 0.6509 - val_loss: 0.9965 - val_accuracy: 0.6337\n",
      "Epoch 174/1000 - train_loss: 0.8791 - train_accuracy: 0.6523 - val_loss: 0.9962 - val_accuracy: 0.6395\n",
      "Epoch 175/1000 - train_loss: 0.8780 - train_accuracy: 0.6509 - val_loss: 0.9960 - val_accuracy: 0.6395\n",
      "Epoch 176/1000 - train_loss: 0.8770 - train_accuracy: 0.6496 - val_loss: 0.9957 - val_accuracy: 0.6395\n",
      "Epoch 177/1000 - train_loss: 0.8760 - train_accuracy: 0.6482 - val_loss: 0.9955 - val_accuracy: 0.6337\n",
      "Epoch 178/1000 - train_loss: 0.8750 - train_accuracy: 0.6469 - val_loss: 0.9952 - val_accuracy: 0.6337\n",
      "Epoch 179/1000 - train_loss: 0.8739 - train_accuracy: 0.6456 - val_loss: 0.9950 - val_accuracy: 0.6337\n",
      "Epoch 180/1000 - train_loss: 0.8729 - train_accuracy: 0.6456 - val_loss: 0.9948 - val_accuracy: 0.6337\n",
      "Epoch 181/1000 - train_loss: 0.8719 - train_accuracy: 0.6469 - val_loss: 0.9946 - val_accuracy: 0.6337\n",
      "Epoch 182/1000 - train_loss: 0.8709 - train_accuracy: 0.6469 - val_loss: 0.9944 - val_accuracy: 0.6279\n",
      "Epoch 183/1000 - train_loss: 0.8698 - train_accuracy: 0.6456 - val_loss: 0.9942 - val_accuracy: 0.6279\n",
      "Epoch 184/1000 - train_loss: 0.8688 - train_accuracy: 0.6482 - val_loss: 0.9940 - val_accuracy: 0.6279\n",
      "Epoch 185/1000 - train_loss: 0.8678 - train_accuracy: 0.6482 - val_loss: 0.9939 - val_accuracy: 0.6279\n",
      "Epoch 186/1000 - train_loss: 0.8668 - train_accuracy: 0.6482 - val_loss: 0.9937 - val_accuracy: 0.6279\n",
      "Epoch 187/1000 - train_loss: 0.8658 - train_accuracy: 0.6482 - val_loss: 0.9936 - val_accuracy: 0.6279\n",
      "Epoch 188/1000 - train_loss: 0.8648 - train_accuracy: 0.6496 - val_loss: 0.9935 - val_accuracy: 0.6279\n",
      "Epoch 189/1000 - train_loss: 0.8638 - train_accuracy: 0.6496 - val_loss: 0.9934 - val_accuracy: 0.6279\n",
      "Epoch 190/1000 - train_loss: 0.8628 - train_accuracy: 0.6509 - val_loss: 0.9933 - val_accuracy: 0.6337\n",
      "Epoch 191/1000 - train_loss: 0.8619 - train_accuracy: 0.6509 - val_loss: 0.9932 - val_accuracy: 0.6395\n",
      "Epoch 192/1000 - train_loss: 0.8609 - train_accuracy: 0.6496 - val_loss: 0.9931 - val_accuracy: 0.6337\n",
      "Epoch 193/1000 - train_loss: 0.8600 - train_accuracy: 0.6496 - val_loss: 0.9931 - val_accuracy: 0.6279\n",
      "Epoch 194/1000 - train_loss: 0.8590 - train_accuracy: 0.6482 - val_loss: 0.9931 - val_accuracy: 0.6279\n",
      "Epoch 195/1000 - train_loss: 0.8581 - train_accuracy: 0.6469 - val_loss: 0.9930 - val_accuracy: 0.6279\n",
      "Epoch 196/1000 - train_loss: 0.8572 - train_accuracy: 0.6482 - val_loss: 0.9930 - val_accuracy: 0.6279\n",
      "Epoch 197/1000 - train_loss: 0.8563 - train_accuracy: 0.6496 - val_loss: 0.9930 - val_accuracy: 0.6279\n",
      "Epoch 198/1000 - train_loss: 0.8554 - train_accuracy: 0.6482 - val_loss: 0.9930 - val_accuracy: 0.6279\n",
      "Epoch 199/1000 - train_loss: 0.8545 - train_accuracy: 0.6482 - val_loss: 0.9931 - val_accuracy: 0.6279\n",
      "Epoch 200/1000 - train_loss: 0.8537 - train_accuracy: 0.6536 - val_loss: 0.9931 - val_accuracy: 0.6221\n",
      "Epoch 201/1000 - train_loss: 0.8528 - train_accuracy: 0.6509 - val_loss: 0.9931 - val_accuracy: 0.6221\n",
      "Epoch 202/1000 - train_loss: 0.8520 - train_accuracy: 0.6509 - val_loss: 0.9932 - val_accuracy: 0.6221\n",
      "Epoch 203/1000 - train_loss: 0.8512 - train_accuracy: 0.6509 - val_loss: 0.9932 - val_accuracy: 0.6221\n",
      "Epoch 204/1000 - train_loss: 0.8504 - train_accuracy: 0.6509 - val_loss: 0.9933 - val_accuracy: 0.6221\n",
      "Epoch 205/1000 - train_loss: 0.8496 - train_accuracy: 0.6496 - val_loss: 0.9933 - val_accuracy: 0.6279\n",
      "Epoch 206/1000 - train_loss: 0.8489 - train_accuracy: 0.6496 - val_loss: 0.9934 - val_accuracy: 0.6279\n",
      "Epoch 207/1000 - train_loss: 0.8481 - train_accuracy: 0.6536 - val_loss: 0.9935 - val_accuracy: 0.6337\n",
      "Epoch 208/1000 - train_loss: 0.8474 - train_accuracy: 0.6536 - val_loss: 0.9935 - val_accuracy: 0.6337\n",
      "Epoch 209/1000 - train_loss: 0.8467 - train_accuracy: 0.6536 - val_loss: 0.9936 - val_accuracy: 0.6337\n",
      "Epoch 210/1000 - train_loss: 0.8460 - train_accuracy: 0.6523 - val_loss: 0.9937 - val_accuracy: 0.6279\n",
      "Epoch 211/1000 - train_loss: 0.8453 - train_accuracy: 0.6550 - val_loss: 0.9938 - val_accuracy: 0.6105\n",
      "Epoch 212/1000 - train_loss: 0.8446 - train_accuracy: 0.6550 - val_loss: 0.9939 - val_accuracy: 0.6105\n",
      "Epoch 213/1000 - train_loss: 0.8439 - train_accuracy: 0.6563 - val_loss: 0.9940 - val_accuracy: 0.6105\n",
      "Epoch 214/1000 - train_loss: 0.8433 - train_accuracy: 0.6563 - val_loss: 0.9941 - val_accuracy: 0.6047\n",
      "Epoch 215/1000 - train_loss: 0.8427 - train_accuracy: 0.6577 - val_loss: 0.9942 - val_accuracy: 0.6047\n",
      "Epoch 216/1000 - train_loss: 0.8420 - train_accuracy: 0.6590 - val_loss: 0.9943 - val_accuracy: 0.6047\n",
      "Epoch 217/1000 - train_loss: 0.8414 - train_accuracy: 0.6577 - val_loss: 0.9944 - val_accuracy: 0.6047\n",
      "Epoch 218/1000 - train_loss: 0.8408 - train_accuracy: 0.6590 - val_loss: 0.9945 - val_accuracy: 0.6047\n",
      "Epoch 219/1000 - train_loss: 0.8402 - train_accuracy: 0.6590 - val_loss: 0.9946 - val_accuracy: 0.6047\n",
      "Epoch 220/1000 - train_loss: 0.8397 - train_accuracy: 0.6604 - val_loss: 0.9947 - val_accuracy: 0.6047\n",
      "Epoch 221/1000 - train_loss: 0.8391 - train_accuracy: 0.6617 - val_loss: 0.9948 - val_accuracy: 0.6047\n",
      "Epoch 222/1000 - train_loss: 0.8386 - train_accuracy: 0.6631 - val_loss: 0.9949 - val_accuracy: 0.5988\n",
      "Epoch 223/1000 - train_loss: 0.8380 - train_accuracy: 0.6617 - val_loss: 0.9950 - val_accuracy: 0.5988\n",
      "Epoch 224/1000 - train_loss: 0.8375 - train_accuracy: 0.6644 - val_loss: 0.9951 - val_accuracy: 0.5988\n",
      "Epoch 225/1000 - train_loss: 0.8370 - train_accuracy: 0.6644 - val_loss: 0.9952 - val_accuracy: 0.5988\n",
      "Epoch 226/1000 - train_loss: 0.8365 - train_accuracy: 0.6671 - val_loss: 0.9953 - val_accuracy: 0.6047\n",
      "Epoch 227/1000 - train_loss: 0.8360 - train_accuracy: 0.6671 - val_loss: 0.9955 - val_accuracy: 0.6105\n",
      "Epoch 228/1000 - train_loss: 0.8355 - train_accuracy: 0.6671 - val_loss: 0.9956 - val_accuracy: 0.6105\n",
      "Epoch 229/1000 - train_loss: 0.8350 - train_accuracy: 0.6685 - val_loss: 0.9957 - val_accuracy: 0.6105\n",
      "Epoch 230/1000 - train_loss: 0.8345 - train_accuracy: 0.6712 - val_loss: 0.9958 - val_accuracy: 0.6105\n",
      "Epoch 231/1000 - train_loss: 0.8341 - train_accuracy: 0.6712 - val_loss: 0.9960 - val_accuracy: 0.6105\n",
      "Epoch 232/1000 - train_loss: 0.8336 - train_accuracy: 0.6725 - val_loss: 0.9961 - val_accuracy: 0.6105\n",
      "Epoch 233/1000 - train_loss: 0.8331 - train_accuracy: 0.6725 - val_loss: 0.9962 - val_accuracy: 0.6105\n",
      "Epoch 234/1000 - train_loss: 0.8327 - train_accuracy: 0.6725 - val_loss: 0.9964 - val_accuracy: 0.6047\n",
      "Epoch 235/1000 - train_loss: 0.8323 - train_accuracy: 0.6752 - val_loss: 0.9965 - val_accuracy: 0.6163\n",
      "Epoch 236/1000 - train_loss: 0.8318 - train_accuracy: 0.6739 - val_loss: 0.9967 - val_accuracy: 0.6163\n",
      "Epoch 237/1000 - train_loss: 0.8314 - train_accuracy: 0.6739 - val_loss: 0.9968 - val_accuracy: 0.6105\n",
      "Epoch 238/1000 - train_loss: 0.8310 - train_accuracy: 0.6725 - val_loss: 0.9970 - val_accuracy: 0.6105\n",
      "Epoch 239/1000 - train_loss: 0.8306 - train_accuracy: 0.6725 - val_loss: 0.9971 - val_accuracy: 0.6105\n",
      "Epoch 240/1000 - train_loss: 0.8302 - train_accuracy: 0.6725 - val_loss: 0.9973 - val_accuracy: 0.6105\n",
      "Epoch 241/1000 - train_loss: 0.8298 - train_accuracy: 0.6712 - val_loss: 0.9975 - val_accuracy: 0.6105\n",
      "Epoch 242/1000 - train_loss: 0.8294 - train_accuracy: 0.6712 - val_loss: 0.9976 - val_accuracy: 0.6105\n",
      "Epoch 243/1000 - train_loss: 0.8290 - train_accuracy: 0.6712 - val_loss: 0.9978 - val_accuracy: 0.6105\n",
      "Epoch 244/1000 - train_loss: 0.8286 - train_accuracy: 0.6712 - val_loss: 0.9980 - val_accuracy: 0.6105\n",
      "Epoch 245/1000 - train_loss: 0.8282 - train_accuracy: 0.6712 - val_loss: 0.9982 - val_accuracy: 0.6105\n",
      "Epoch 246/1000 - train_loss: 0.8279 - train_accuracy: 0.6712 - val_loss: 0.9984 - val_accuracy: 0.6105\n",
      "Epoch 247/1000 - train_loss: 0.8275 - train_accuracy: 0.6698 - val_loss: 0.9986 - val_accuracy: 0.6047\n",
      "Epoch 248/1000 - train_loss: 0.8271 - train_accuracy: 0.6698 - val_loss: 0.9988 - val_accuracy: 0.6047\n",
      "Epoch 249/1000 - train_loss: 0.8267 - train_accuracy: 0.6725 - val_loss: 0.9990 - val_accuracy: 0.6047\n",
      "Epoch 250/1000 - train_loss: 0.8264 - train_accuracy: 0.6712 - val_loss: 0.9992 - val_accuracy: 0.6047\n",
      "Epoch 251/1000 - train_loss: 0.8260 - train_accuracy: 0.6712 - val_loss: 0.9994 - val_accuracy: 0.6047\n",
      "Epoch 252/1000 - train_loss: 0.8256 - train_accuracy: 0.6725 - val_loss: 0.9996 - val_accuracy: 0.6105\n",
      "Epoch 253/1000 - train_loss: 0.8253 - train_accuracy: 0.6725 - val_loss: 0.9999 - val_accuracy: 0.6105\n",
      "Epoch 254/1000 - train_loss: 0.8249 - train_accuracy: 0.6725 - val_loss: 1.0001 - val_accuracy: 0.6105\n",
      "Epoch 255/1000 - train_loss: 0.8246 - train_accuracy: 0.6725 - val_loss: 1.0003 - val_accuracy: 0.6105\n",
      "Epoch 256/1000 - train_loss: 0.8242 - train_accuracy: 0.6752 - val_loss: 1.0006 - val_accuracy: 0.6163\n",
      "Epoch 257/1000 - train_loss: 0.8238 - train_accuracy: 0.6752 - val_loss: 1.0008 - val_accuracy: 0.6163\n",
      "Epoch 258/1000 - train_loss: 0.8235 - train_accuracy: 0.6752 - val_loss: 1.0011 - val_accuracy: 0.6163\n",
      "Epoch 259/1000 - train_loss: 0.8231 - train_accuracy: 0.6752 - val_loss: 1.0013 - val_accuracy: 0.6221\n",
      "Epoch 260/1000 - train_loss: 0.8228 - train_accuracy: 0.6752 - val_loss: 1.0016 - val_accuracy: 0.6163\n",
      "Epoch 261/1000 - train_loss: 0.8224 - train_accuracy: 0.6752 - val_loss: 1.0018 - val_accuracy: 0.6163\n",
      "Epoch 262/1000 - train_loss: 0.8221 - train_accuracy: 0.6752 - val_loss: 1.0021 - val_accuracy: 0.6163\n",
      "Epoch 263/1000 - train_loss: 0.8217 - train_accuracy: 0.6765 - val_loss: 1.0024 - val_accuracy: 0.6163\n",
      "Epoch 264/1000 - train_loss: 0.8214 - train_accuracy: 0.6752 - val_loss: 1.0026 - val_accuracy: 0.6163\n",
      "Epoch 265/1000 - train_loss: 0.8210 - train_accuracy: 0.6752 - val_loss: 1.0029 - val_accuracy: 0.6163\n",
      "Epoch 266/1000 - train_loss: 0.8207 - train_accuracy: 0.6779 - val_loss: 1.0032 - val_accuracy: 0.6163\n",
      "Epoch 267/1000 - train_loss: 0.8203 - train_accuracy: 0.6792 - val_loss: 1.0035 - val_accuracy: 0.6163\n",
      "Epoch 268/1000 - train_loss: 0.8200 - train_accuracy: 0.6792 - val_loss: 1.0038 - val_accuracy: 0.6105\n",
      "Epoch 269/1000 - train_loss: 0.8196 - train_accuracy: 0.6792 - val_loss: 1.0041 - val_accuracy: 0.6105\n",
      "Epoch 270/1000 - train_loss: 0.8193 - train_accuracy: 0.6792 - val_loss: 1.0044 - val_accuracy: 0.6105\n",
      "Epoch 271/1000 - train_loss: 0.8189 - train_accuracy: 0.6792 - val_loss: 1.0047 - val_accuracy: 0.6105\n",
      "Epoch 272/1000 - train_loss: 0.8186 - train_accuracy: 0.6792 - val_loss: 1.0050 - val_accuracy: 0.6105\n",
      "Epoch 273/1000 - train_loss: 0.8182 - train_accuracy: 0.6792 - val_loss: 1.0053 - val_accuracy: 0.6105\n",
      "Epoch 274/1000 - train_loss: 0.8179 - train_accuracy: 0.6792 - val_loss: 1.0056 - val_accuracy: 0.6105\n",
      "Epoch 275/1000 - train_loss: 0.8175 - train_accuracy: 0.6792 - val_loss: 1.0059 - val_accuracy: 0.6105\n",
      "Epoch 276/1000 - train_loss: 0.8171 - train_accuracy: 0.6792 - val_loss: 1.0063 - val_accuracy: 0.6047\n",
      "Epoch 277/1000 - train_loss: 0.8168 - train_accuracy: 0.6792 - val_loss: 1.0066 - val_accuracy: 0.6047\n",
      "Epoch 278/1000 - train_loss: 0.8164 - train_accuracy: 0.6792 - val_loss: 1.0069 - val_accuracy: 0.6047\n",
      "Epoch 279/1000 - train_loss: 0.8161 - train_accuracy: 0.6792 - val_loss: 1.0072 - val_accuracy: 0.6047\n",
      "Epoch 280/1000 - train_loss: 0.8157 - train_accuracy: 0.6792 - val_loss: 1.0076 - val_accuracy: 0.6047\n",
      "Epoch 281/1000 - train_loss: 0.8153 - train_accuracy: 0.6806 - val_loss: 1.0079 - val_accuracy: 0.6047\n",
      "Epoch 282/1000 - train_loss: 0.8150 - train_accuracy: 0.6806 - val_loss: 1.0082 - val_accuracy: 0.6047\n",
      "Epoch 283/1000 - train_loss: 0.8146 - train_accuracy: 0.6792 - val_loss: 1.0086 - val_accuracy: 0.6047\n",
      "Epoch 284/1000 - train_loss: 0.8142 - train_accuracy: 0.6792 - val_loss: 1.0089 - val_accuracy: 0.6047\n",
      "Epoch 285/1000 - train_loss: 0.8138 - train_accuracy: 0.6779 - val_loss: 1.0092 - val_accuracy: 0.6047\n",
      "Epoch 286/1000 - train_loss: 0.8135 - train_accuracy: 0.6765 - val_loss: 1.0096 - val_accuracy: 0.6047\n",
      "Epoch 287/1000 - train_loss: 0.8131 - train_accuracy: 0.6765 - val_loss: 1.0099 - val_accuracy: 0.6105\n",
      "Epoch 288/1000 - train_loss: 0.8127 - train_accuracy: 0.6792 - val_loss: 1.0102 - val_accuracy: 0.6105\n",
      "Epoch 289/1000 - train_loss: 0.8123 - train_accuracy: 0.6792 - val_loss: 1.0106 - val_accuracy: 0.6105\n",
      "Epoch 290/1000 - train_loss: 0.8120 - train_accuracy: 0.6792 - val_loss: 1.0109 - val_accuracy: 0.6105\n",
      "Epoch 291/1000 - train_loss: 0.8116 - train_accuracy: 0.6779 - val_loss: 1.0113 - val_accuracy: 0.6105\n",
      "Epoch 292/1000 - train_loss: 0.8112 - train_accuracy: 0.6792 - val_loss: 1.0116 - val_accuracy: 0.6105\n",
      "Epoch 293/1000 - train_loss: 0.8108 - train_accuracy: 0.6792 - val_loss: 1.0119 - val_accuracy: 0.6047\n",
      "Epoch 294/1000 - train_loss: 0.8104 - train_accuracy: 0.6792 - val_loss: 1.0123 - val_accuracy: 0.6047\n",
      "Epoch 295/1000 - train_loss: 0.8100 - train_accuracy: 0.6792 - val_loss: 1.0126 - val_accuracy: 0.6047\n",
      "Epoch 296/1000 - train_loss: 0.8096 - train_accuracy: 0.6819 - val_loss: 1.0129 - val_accuracy: 0.6163\n",
      "Epoch 297/1000 - train_loss: 0.8092 - train_accuracy: 0.6819 - val_loss: 1.0133 - val_accuracy: 0.6163\n",
      "Epoch 298/1000 - train_loss: 0.8089 - train_accuracy: 0.6819 - val_loss: 1.0136 - val_accuracy: 0.6163\n",
      "Epoch 299/1000 - train_loss: 0.8085 - train_accuracy: 0.6819 - val_loss: 1.0139 - val_accuracy: 0.6163\n",
      "Epoch 300/1000 - train_loss: 0.8081 - train_accuracy: 0.6819 - val_loss: 1.0142 - val_accuracy: 0.6163\n",
      "Epoch 301/1000 - train_loss: 0.8077 - train_accuracy: 0.6833 - val_loss: 1.0145 - val_accuracy: 0.6163\n",
      "Epoch 302/1000 - train_loss: 0.8073 - train_accuracy: 0.6860 - val_loss: 1.0149 - val_accuracy: 0.6163\n",
      "Epoch 303/1000 - train_loss: 0.8069 - train_accuracy: 0.6860 - val_loss: 1.0152 - val_accuracy: 0.6163\n",
      "Epoch 304/1000 - train_loss: 0.8065 - train_accuracy: 0.6860 - val_loss: 1.0155 - val_accuracy: 0.6163\n",
      "Epoch 305/1000 - train_loss: 0.8061 - train_accuracy: 0.6860 - val_loss: 1.0158 - val_accuracy: 0.6163\n",
      "Epoch 306/1000 - train_loss: 0.8057 - train_accuracy: 0.6860 - val_loss: 1.0161 - val_accuracy: 0.6163\n",
      "Epoch 307/1000 - train_loss: 0.8053 - train_accuracy: 0.6860 - val_loss: 1.0164 - val_accuracy: 0.6163\n",
      "Epoch 308/1000 - train_loss: 0.8049 - train_accuracy: 0.6860 - val_loss: 1.0167 - val_accuracy: 0.6163\n",
      "Epoch 309/1000 - train_loss: 0.8045 - train_accuracy: 0.6873 - val_loss: 1.0169 - val_accuracy: 0.6163\n",
      "Epoch 310/1000 - train_loss: 0.8041 - train_accuracy: 0.6873 - val_loss: 1.0172 - val_accuracy: 0.6163\n",
      "Epoch 311/1000 - train_loss: 0.8037 - train_accuracy: 0.6860 - val_loss: 1.0175 - val_accuracy: 0.6163\n",
      "Epoch 312/1000 - train_loss: 0.8033 - train_accuracy: 0.6860 - val_loss: 1.0178 - val_accuracy: 0.6163\n",
      "Epoch 313/1000 - train_loss: 0.8029 - train_accuracy: 0.6887 - val_loss: 1.0180 - val_accuracy: 0.6221\n",
      "Epoch 314/1000 - train_loss: 0.8025 - train_accuracy: 0.6887 - val_loss: 1.0183 - val_accuracy: 0.6221\n",
      "Epoch 315/1000 - train_loss: 0.8021 - train_accuracy: 0.6887 - val_loss: 1.0185 - val_accuracy: 0.6221\n",
      "Epoch 316/1000 - train_loss: 0.8018 - train_accuracy: 0.6887 - val_loss: 1.0188 - val_accuracy: 0.6221\n",
      "Epoch 317/1000 - train_loss: 0.8014 - train_accuracy: 0.6873 - val_loss: 1.0190 - val_accuracy: 0.6221\n",
      "Epoch 318/1000 - train_loss: 0.8010 - train_accuracy: 0.6887 - val_loss: 1.0193 - val_accuracy: 0.6221\n",
      "Epoch 319/1000 - train_loss: 0.8006 - train_accuracy: 0.6887 - val_loss: 1.0195 - val_accuracy: 0.6221\n",
      "Epoch 320/1000 - train_loss: 0.8002 - train_accuracy: 0.6887 - val_loss: 1.0197 - val_accuracy: 0.6279\n",
      "Epoch 321/1000 - train_loss: 0.7998 - train_accuracy: 0.6887 - val_loss: 1.0199 - val_accuracy: 0.6279\n",
      "Epoch 322/1000 - train_loss: 0.7994 - train_accuracy: 0.6887 - val_loss: 1.0202 - val_accuracy: 0.6279\n",
      "Epoch 323/1000 - train_loss: 0.7990 - train_accuracy: 0.6887 - val_loss: 1.0204 - val_accuracy: 0.6279\n",
      "Epoch 324/1000 - train_loss: 0.7986 - train_accuracy: 0.6887 - val_loss: 1.0206 - val_accuracy: 0.6279\n",
      "Epoch 325/1000 - train_loss: 0.7983 - train_accuracy: 0.6887 - val_loss: 1.0208 - val_accuracy: 0.6337\n",
      "Epoch 326/1000 - train_loss: 0.7979 - train_accuracy: 0.6873 - val_loss: 1.0210 - val_accuracy: 0.6337\n",
      "Epoch 327/1000 - train_loss: 0.7975 - train_accuracy: 0.6873 - val_loss: 1.0211 - val_accuracy: 0.6337\n",
      "Epoch 328/1000 - train_loss: 0.7971 - train_accuracy: 0.6860 - val_loss: 1.0213 - val_accuracy: 0.6337\n",
      "Epoch 329/1000 - train_loss: 0.7967 - train_accuracy: 0.6873 - val_loss: 1.0215 - val_accuracy: 0.6337\n",
      "Epoch 330/1000 - train_loss: 0.7964 - train_accuracy: 0.6873 - val_loss: 1.0216 - val_accuracy: 0.6337\n",
      "Epoch 331/1000 - train_loss: 0.7960 - train_accuracy: 0.6873 - val_loss: 1.0218 - val_accuracy: 0.6337\n",
      "Epoch 332/1000 - train_loss: 0.7956 - train_accuracy: 0.6873 - val_loss: 1.0220 - val_accuracy: 0.6337\n",
      "Epoch 333/1000 - train_loss: 0.7952 - train_accuracy: 0.6873 - val_loss: 1.0221 - val_accuracy: 0.6337\n",
      "Epoch 334/1000 - train_loss: 0.7949 - train_accuracy: 0.6860 - val_loss: 1.0223 - val_accuracy: 0.6337\n",
      "Epoch 335/1000 - train_loss: 0.7945 - train_accuracy: 0.6860 - val_loss: 1.0224 - val_accuracy: 0.6337\n",
      "Epoch 336/1000 - train_loss: 0.7941 - train_accuracy: 0.6873 - val_loss: 1.0225 - val_accuracy: 0.6337\n",
      "Epoch 337/1000 - train_loss: 0.7938 - train_accuracy: 0.6873 - val_loss: 1.0226 - val_accuracy: 0.6337\n",
      "Epoch 338/1000 - train_loss: 0.7934 - train_accuracy: 0.6873 - val_loss: 1.0228 - val_accuracy: 0.6337\n",
      "Epoch 339/1000 - train_loss: 0.7931 - train_accuracy: 0.6887 - val_loss: 1.0229 - val_accuracy: 0.6337\n",
      "Epoch 340/1000 - train_loss: 0.7927 - train_accuracy: 0.6887 - val_loss: 1.0230 - val_accuracy: 0.6337\n",
      "Epoch 341/1000 - train_loss: 0.7924 - train_accuracy: 0.6887 - val_loss: 1.0231 - val_accuracy: 0.6337\n",
      "Epoch 342/1000 - train_loss: 0.7920 - train_accuracy: 0.6887 - val_loss: 1.0232 - val_accuracy: 0.6395\n",
      "Epoch 343/1000 - train_loss: 0.7916 - train_accuracy: 0.6887 - val_loss: 1.0233 - val_accuracy: 0.6395\n",
      "Epoch 344/1000 - train_loss: 0.7913 - train_accuracy: 0.6887 - val_loss: 1.0234 - val_accuracy: 0.6395\n",
      "Epoch 345/1000 - train_loss: 0.7909 - train_accuracy: 0.6887 - val_loss: 1.0235 - val_accuracy: 0.6395\n",
      "Epoch 346/1000 - train_loss: 0.7906 - train_accuracy: 0.6887 - val_loss: 1.0236 - val_accuracy: 0.6395\n",
      "Epoch 347/1000 - train_loss: 0.7903 - train_accuracy: 0.6887 - val_loss: 1.0236 - val_accuracy: 0.6395\n",
      "Epoch 348/1000 - train_loss: 0.7899 - train_accuracy: 0.6887 - val_loss: 1.0237 - val_accuracy: 0.6395\n",
      "Epoch 349/1000 - train_loss: 0.7896 - train_accuracy: 0.6914 - val_loss: 1.0238 - val_accuracy: 0.6395\n",
      "Epoch 350/1000 - train_loss: 0.7892 - train_accuracy: 0.6914 - val_loss: 1.0239 - val_accuracy: 0.6395\n",
      "Epoch 351/1000 - train_loss: 0.7889 - train_accuracy: 0.6914 - val_loss: 1.0239 - val_accuracy: 0.6395\n",
      "Epoch 352/1000 - train_loss: 0.7885 - train_accuracy: 0.6914 - val_loss: 1.0240 - val_accuracy: 0.6395\n",
      "Epoch 353/1000 - train_loss: 0.7882 - train_accuracy: 0.6900 - val_loss: 1.0240 - val_accuracy: 0.6395\n",
      "Epoch 354/1000 - train_loss: 0.7879 - train_accuracy: 0.6914 - val_loss: 1.0241 - val_accuracy: 0.6395\n",
      "Epoch 355/1000 - train_loss: 0.7875 - train_accuracy: 0.6914 - val_loss: 1.0241 - val_accuracy: 0.6395\n",
      "Epoch 356/1000 - train_loss: 0.7872 - train_accuracy: 0.6914 - val_loss: 1.0242 - val_accuracy: 0.6395\n",
      "Epoch 357/1000 - train_loss: 0.7868 - train_accuracy: 0.6914 - val_loss: 1.0242 - val_accuracy: 0.6395\n",
      "Epoch 358/1000 - train_loss: 0.7865 - train_accuracy: 0.6900 - val_loss: 1.0243 - val_accuracy: 0.6395\n",
      "Epoch 359/1000 - train_loss: 0.7862 - train_accuracy: 0.6900 - val_loss: 1.0243 - val_accuracy: 0.6395\n",
      "Epoch 360/1000 - train_loss: 0.7858 - train_accuracy: 0.6900 - val_loss: 1.0243 - val_accuracy: 0.6395\n",
      "Epoch 361/1000 - train_loss: 0.7855 - train_accuracy: 0.6900 - val_loss: 1.0244 - val_accuracy: 0.6337\n",
      "Epoch 362/1000 - train_loss: 0.7852 - train_accuracy: 0.6914 - val_loss: 1.0244 - val_accuracy: 0.6395\n",
      "Epoch 363/1000 - train_loss: 0.7848 - train_accuracy: 0.6914 - val_loss: 1.0244 - val_accuracy: 0.6395\n",
      "Epoch 364/1000 - train_loss: 0.7845 - train_accuracy: 0.6914 - val_loss: 1.0244 - val_accuracy: 0.6395\n",
      "Epoch 365/1000 - train_loss: 0.7842 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6395\n",
      "Epoch 366/1000 - train_loss: 0.7838 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6395\n",
      "Epoch 367/1000 - train_loss: 0.7835 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6395\n",
      "Epoch 368/1000 - train_loss: 0.7831 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6395\n",
      "Epoch 369/1000 - train_loss: 0.7828 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6395\n",
      "Epoch 370/1000 - train_loss: 0.7825 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6395\n",
      "Epoch 371/1000 - train_loss: 0.7821 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6279\n",
      "Epoch 372/1000 - train_loss: 0.7818 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6279\n",
      "Epoch 373/1000 - train_loss: 0.7815 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6279\n",
      "Epoch 374/1000 - train_loss: 0.7811 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6279\n",
      "Epoch 375/1000 - train_loss: 0.7808 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6279\n",
      "Epoch 376/1000 - train_loss: 0.7804 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6279\n",
      "Epoch 377/1000 - train_loss: 0.7801 - train_accuracy: 0.6914 - val_loss: 1.0245 - val_accuracy: 0.6279\n",
      "Epoch 378/1000 - train_loss: 0.7797 - train_accuracy: 0.6887 - val_loss: 1.0245 - val_accuracy: 0.6279\n",
      "Epoch 379/1000 - train_loss: 0.7794 - train_accuracy: 0.6887 - val_loss: 1.0245 - val_accuracy: 0.6279\n",
      "Epoch 380/1000 - train_loss: 0.7790 - train_accuracy: 0.6887 - val_loss: 1.0244 - val_accuracy: 0.6279\n",
      "Epoch 381/1000 - train_loss: 0.7787 - train_accuracy: 0.6887 - val_loss: 1.0244 - val_accuracy: 0.6279\n",
      "Epoch 382/1000 - train_loss: 0.7784 - train_accuracy: 0.6873 - val_loss: 1.0244 - val_accuracy: 0.6279\n",
      "Epoch 383/1000 - train_loss: 0.7780 - train_accuracy: 0.6873 - val_loss: 1.0244 - val_accuracy: 0.6279\n",
      "Epoch 384/1000 - train_loss: 0.7777 - train_accuracy: 0.6887 - val_loss: 1.0244 - val_accuracy: 0.6279\n",
      "Epoch 385/1000 - train_loss: 0.7773 - train_accuracy: 0.6887 - val_loss: 1.0243 - val_accuracy: 0.6279\n",
      "Epoch 386/1000 - train_loss: 0.7770 - train_accuracy: 0.6887 - val_loss: 1.0243 - val_accuracy: 0.6279\n",
      "Epoch 387/1000 - train_loss: 0.7766 - train_accuracy: 0.6887 - val_loss: 1.0243 - val_accuracy: 0.6279\n",
      "Epoch 388/1000 - train_loss: 0.7763 - train_accuracy: 0.6887 - val_loss: 1.0243 - val_accuracy: 0.6279\n",
      "Epoch 389/1000 - train_loss: 0.7759 - train_accuracy: 0.6900 - val_loss: 1.0242 - val_accuracy: 0.6279\n",
      "Epoch 390/1000 - train_loss: 0.7755 - train_accuracy: 0.6900 - val_loss: 1.0242 - val_accuracy: 0.6279\n",
      "Epoch 391/1000 - train_loss: 0.7752 - train_accuracy: 0.6914 - val_loss: 1.0242 - val_accuracy: 0.6279\n",
      "Epoch 392/1000 - train_loss: 0.7748 - train_accuracy: 0.6914 - val_loss: 1.0242 - val_accuracy: 0.6279\n",
      "Epoch 393/1000 - train_loss: 0.7745 - train_accuracy: 0.6914 - val_loss: 1.0241 - val_accuracy: 0.6279\n",
      "Epoch 394/1000 - train_loss: 0.7741 - train_accuracy: 0.6914 - val_loss: 1.0241 - val_accuracy: 0.6279\n",
      "Epoch 395/1000 - train_loss: 0.7738 - train_accuracy: 0.6914 - val_loss: 1.0241 - val_accuracy: 0.6279\n",
      "Epoch 396/1000 - train_loss: 0.7734 - train_accuracy: 0.6914 - val_loss: 1.0241 - val_accuracy: 0.6279\n",
      "Epoch 397/1000 - train_loss: 0.7731 - train_accuracy: 0.6914 - val_loss: 1.0240 - val_accuracy: 0.6337\n",
      "Epoch 398/1000 - train_loss: 0.7727 - train_accuracy: 0.6914 - val_loss: 1.0240 - val_accuracy: 0.6337\n",
      "Epoch 399/1000 - train_loss: 0.7724 - train_accuracy: 0.6914 - val_loss: 1.0240 - val_accuracy: 0.6337\n",
      "Epoch 400/1000 - train_loss: 0.7720 - train_accuracy: 0.6914 - val_loss: 1.0240 - val_accuracy: 0.6337\n",
      "Epoch 401/1000 - train_loss: 0.7717 - train_accuracy: 0.6927 - val_loss: 1.0239 - val_accuracy: 0.6337\n",
      "Epoch 402/1000 - train_loss: 0.7713 - train_accuracy: 0.6927 - val_loss: 1.0239 - val_accuracy: 0.6337\n",
      "Epoch 403/1000 - train_loss: 0.7710 - train_accuracy: 0.6941 - val_loss: 1.0239 - val_accuracy: 0.6337\n",
      "Epoch 404/1000 - train_loss: 0.7706 - train_accuracy: 0.6941 - val_loss: 1.0238 - val_accuracy: 0.6279\n",
      "Epoch 405/1000 - train_loss: 0.7703 - train_accuracy: 0.6914 - val_loss: 1.0238 - val_accuracy: 0.6279\n",
      "Epoch 406/1000 - train_loss: 0.7699 - train_accuracy: 0.6914 - val_loss: 1.0238 - val_accuracy: 0.6279\n",
      "Epoch 407/1000 - train_loss: 0.7696 - train_accuracy: 0.6914 - val_loss: 1.0238 - val_accuracy: 0.6279\n",
      "Epoch 408/1000 - train_loss: 0.7692 - train_accuracy: 0.6914 - val_loss: 1.0237 - val_accuracy: 0.6279\n",
      "Epoch 409/1000 - train_loss: 0.7689 - train_accuracy: 0.6914 - val_loss: 1.0237 - val_accuracy: 0.6279\n",
      "Epoch 410/1000 - train_loss: 0.7685 - train_accuracy: 0.6900 - val_loss: 1.0237 - val_accuracy: 0.6279\n",
      "Epoch 411/1000 - train_loss: 0.7682 - train_accuracy: 0.6900 - val_loss: 1.0236 - val_accuracy: 0.6279\n",
      "Epoch 412/1000 - train_loss: 0.7678 - train_accuracy: 0.6900 - val_loss: 1.0236 - val_accuracy: 0.6279\n",
      "Epoch 413/1000 - train_loss: 0.7675 - train_accuracy: 0.6927 - val_loss: 1.0236 - val_accuracy: 0.6279\n",
      "Epoch 414/1000 - train_loss: 0.7671 - train_accuracy: 0.6927 - val_loss: 1.0236 - val_accuracy: 0.6279\n",
      "Epoch 415/1000 - train_loss: 0.7668 - train_accuracy: 0.6927 - val_loss: 1.0235 - val_accuracy: 0.6279\n",
      "Epoch 416/1000 - train_loss: 0.7665 - train_accuracy: 0.6941 - val_loss: 1.0235 - val_accuracy: 0.6279\n",
      "Epoch 417/1000 - train_loss: 0.7661 - train_accuracy: 0.6941 - val_loss: 1.0235 - val_accuracy: 0.6279\n",
      "Epoch 418/1000 - train_loss: 0.7658 - train_accuracy: 0.6941 - val_loss: 1.0234 - val_accuracy: 0.6279\n",
      "Epoch 419/1000 - train_loss: 0.7654 - train_accuracy: 0.6941 - val_loss: 1.0234 - val_accuracy: 0.6279\n",
      "Epoch 420/1000 - train_loss: 0.7651 - train_accuracy: 0.6941 - val_loss: 1.0234 - val_accuracy: 0.6279\n",
      "Epoch 421/1000 - train_loss: 0.7648 - train_accuracy: 0.6941 - val_loss: 1.0233 - val_accuracy: 0.6279\n",
      "Epoch 422/1000 - train_loss: 0.7644 - train_accuracy: 0.6941 - val_loss: 1.0233 - val_accuracy: 0.6279\n",
      "Epoch 423/1000 - train_loss: 0.7641 - train_accuracy: 0.6941 - val_loss: 1.0233 - val_accuracy: 0.6279\n",
      "Epoch 424/1000 - train_loss: 0.7637 - train_accuracy: 0.6941 - val_loss: 1.0232 - val_accuracy: 0.6279\n",
      "Epoch 425/1000 - train_loss: 0.7634 - train_accuracy: 0.6954 - val_loss: 1.0232 - val_accuracy: 0.6279\n",
      "Epoch 426/1000 - train_loss: 0.7631 - train_accuracy: 0.6954 - val_loss: 1.0232 - val_accuracy: 0.6279\n",
      "Epoch 427/1000 - train_loss: 0.7627 - train_accuracy: 0.6954 - val_loss: 1.0232 - val_accuracy: 0.6279\n",
      "Epoch 428/1000 - train_loss: 0.7624 - train_accuracy: 0.6954 - val_loss: 1.0231 - val_accuracy: 0.6279\n",
      "Epoch 429/1000 - train_loss: 0.7621 - train_accuracy: 0.6981 - val_loss: 1.0231 - val_accuracy: 0.6279\n",
      "Epoch 430/1000 - train_loss: 0.7617 - train_accuracy: 0.6981 - val_loss: 1.0231 - val_accuracy: 0.6279\n",
      "Epoch 431/1000 - train_loss: 0.7614 - train_accuracy: 0.6981 - val_loss: 1.0231 - val_accuracy: 0.6279\n",
      "Epoch 432/1000 - train_loss: 0.7611 - train_accuracy: 0.6995 - val_loss: 1.0230 - val_accuracy: 0.6279\n",
      "Epoch 433/1000 - train_loss: 0.7608 - train_accuracy: 0.6995 - val_loss: 1.0230 - val_accuracy: 0.6279\n",
      "Epoch 434/1000 - train_loss: 0.7604 - train_accuracy: 0.6995 - val_loss: 1.0230 - val_accuracy: 0.6279\n",
      "Epoch 435/1000 - train_loss: 0.7601 - train_accuracy: 0.6995 - val_loss: 1.0230 - val_accuracy: 0.6279\n",
      "Epoch 436/1000 - train_loss: 0.7598 - train_accuracy: 0.7008 - val_loss: 1.0230 - val_accuracy: 0.6279\n",
      "Epoch 437/1000 - train_loss: 0.7595 - train_accuracy: 0.7008 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 438/1000 - train_loss: 0.7591 - train_accuracy: 0.6995 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 439/1000 - train_loss: 0.7588 - train_accuracy: 0.7008 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 440/1000 - train_loss: 0.7585 - train_accuracy: 0.7022 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 441/1000 - train_loss: 0.7582 - train_accuracy: 0.7022 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 442/1000 - train_loss: 0.7579 - train_accuracy: 0.7022 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 443/1000 - train_loss: 0.7575 - train_accuracy: 0.7022 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 444/1000 - train_loss: 0.7572 - train_accuracy: 0.7035 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 445/1000 - train_loss: 0.7569 - train_accuracy: 0.7035 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 446/1000 - train_loss: 0.7566 - train_accuracy: 0.7035 - val_loss: 1.0228 - val_accuracy: 0.6279\n",
      "Epoch 447/1000 - train_loss: 0.7562 - train_accuracy: 0.7035 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 448/1000 - train_loss: 0.7559 - train_accuracy: 0.7035 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 449/1000 - train_loss: 0.7556 - train_accuracy: 0.7035 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 450/1000 - train_loss: 0.7553 - train_accuracy: 0.7062 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 451/1000 - train_loss: 0.7549 - train_accuracy: 0.7062 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 452/1000 - train_loss: 0.7546 - train_accuracy: 0.7062 - val_loss: 1.0229 - val_accuracy: 0.6279\n",
      "Epoch 453/1000 - train_loss: 0.7543 - train_accuracy: 0.7062 - val_loss: 1.0230 - val_accuracy: 0.6279\n",
      "Epoch 454/1000 - train_loss: 0.7540 - train_accuracy: 0.7062 - val_loss: 1.0230 - val_accuracy: 0.6279\n",
      "Epoch 455/1000 - train_loss: 0.7536 - train_accuracy: 0.7062 - val_loss: 1.0230 - val_accuracy: 0.6279\n",
      "Epoch 456/1000 - train_loss: 0.7533 - train_accuracy: 0.7062 - val_loss: 1.0231 - val_accuracy: 0.6279\n",
      "Epoch 457/1000 - train_loss: 0.7530 - train_accuracy: 0.7075 - val_loss: 1.0231 - val_accuracy: 0.6279\n",
      "Epoch 458/1000 - train_loss: 0.7527 - train_accuracy: 0.7075 - val_loss: 1.0232 - val_accuracy: 0.6279\n",
      "Epoch 459/1000 - train_loss: 0.7523 - train_accuracy: 0.7089 - val_loss: 1.0233 - val_accuracy: 0.6279\n",
      "Epoch 460/1000 - train_loss: 0.7520 - train_accuracy: 0.7089 - val_loss: 1.0234 - val_accuracy: 0.6279\n",
      "Epoch 461/1000 - train_loss: 0.7517 - train_accuracy: 0.7089 - val_loss: 1.0235 - val_accuracy: 0.6279\n",
      "Epoch 462/1000 - train_loss: 0.7514 - train_accuracy: 0.7089 - val_loss: 1.0236 - val_accuracy: 0.6279\n",
      "Epoch 463/1000 - train_loss: 0.7510 - train_accuracy: 0.7089 - val_loss: 1.0237 - val_accuracy: 0.6279\n",
      "Epoch 464/1000 - train_loss: 0.7507 - train_accuracy: 0.7075 - val_loss: 1.0238 - val_accuracy: 0.6279\n",
      "Epoch 465/1000 - train_loss: 0.7504 - train_accuracy: 0.7075 - val_loss: 1.0239 - val_accuracy: 0.6279\n",
      "Epoch 466/1000 - train_loss: 0.7501 - train_accuracy: 0.7075 - val_loss: 1.0240 - val_accuracy: 0.6279\n",
      "Epoch 467/1000 - train_loss: 0.7498 - train_accuracy: 0.7062 - val_loss: 1.0242 - val_accuracy: 0.6279\n",
      "Epoch 468/1000 - train_loss: 0.7494 - train_accuracy: 0.7062 - val_loss: 1.0243 - val_accuracy: 0.6279\n",
      "Epoch 469/1000 - train_loss: 0.7491 - train_accuracy: 0.7062 - val_loss: 1.0245 - val_accuracy: 0.6221\n",
      "Epoch 470/1000 - train_loss: 0.7488 - train_accuracy: 0.7062 - val_loss: 1.0246 - val_accuracy: 0.6163\n",
      "Epoch 471/1000 - train_loss: 0.7485 - train_accuracy: 0.7062 - val_loss: 1.0248 - val_accuracy: 0.6163\n",
      "Epoch 472/1000 - train_loss: 0.7482 - train_accuracy: 0.7075 - val_loss: 1.0250 - val_accuracy: 0.6163\n",
      "Epoch 473/1000 - train_loss: 0.7479 - train_accuracy: 0.7075 - val_loss: 1.0252 - val_accuracy: 0.6163\n",
      "Epoch 474/1000 - train_loss: 0.7476 - train_accuracy: 0.7075 - val_loss: 1.0253 - val_accuracy: 0.6163\n",
      "Epoch 475/1000 - train_loss: 0.7472 - train_accuracy: 0.7075 - val_loss: 1.0255 - val_accuracy: 0.6163\n",
      "Epoch 476/1000 - train_loss: 0.7469 - train_accuracy: 0.7089 - val_loss: 1.0257 - val_accuracy: 0.6163\n",
      "Epoch 477/1000 - train_loss: 0.7466 - train_accuracy: 0.7089 - val_loss: 1.0259 - val_accuracy: 0.6163\n",
      "Epoch 478/1000 - train_loss: 0.7463 - train_accuracy: 0.7089 - val_loss: 1.0261 - val_accuracy: 0.6163\n",
      "Epoch 479/1000 - train_loss: 0.7460 - train_accuracy: 0.7089 - val_loss: 1.0263 - val_accuracy: 0.6163\n",
      "Epoch 480/1000 - train_loss: 0.7457 - train_accuracy: 0.7102 - val_loss: 1.0265 - val_accuracy: 0.6221\n",
      "Epoch 481/1000 - train_loss: 0.7454 - train_accuracy: 0.7102 - val_loss: 1.0267 - val_accuracy: 0.6221\n",
      "Epoch 482/1000 - train_loss: 0.7451 - train_accuracy: 0.7102 - val_loss: 1.0269 - val_accuracy: 0.6221\n",
      "Epoch 483/1000 - train_loss: 0.7448 - train_accuracy: 0.7102 - val_loss: 1.0271 - val_accuracy: 0.6221\n",
      "Epoch 484/1000 - train_loss: 0.7446 - train_accuracy: 0.7089 - val_loss: 1.0273 - val_accuracy: 0.6221\n",
      "Epoch 485/1000 - train_loss: 0.7443 - train_accuracy: 0.7089 - val_loss: 1.0275 - val_accuracy: 0.6221\n",
      "Epoch 486/1000 - train_loss: 0.7440 - train_accuracy: 0.7089 - val_loss: 1.0277 - val_accuracy: 0.6221\n",
      "Epoch 487/1000 - train_loss: 0.7437 - train_accuracy: 0.7089 - val_loss: 1.0279 - val_accuracy: 0.6221\n",
      "Epoch 488/1000 - train_loss: 0.7434 - train_accuracy: 0.7089 - val_loss: 1.0281 - val_accuracy: 0.6279\n",
      "Epoch 489/1000 - train_loss: 0.7431 - train_accuracy: 0.7089 - val_loss: 1.0283 - val_accuracy: 0.6279\n",
      "Epoch 490/1000 - train_loss: 0.7428 - train_accuracy: 0.7102 - val_loss: 1.0285 - val_accuracy: 0.6279\n",
      "Epoch 491/1000 - train_loss: 0.7426 - train_accuracy: 0.7102 - val_loss: 1.0287 - val_accuracy: 0.6279\n",
      "Epoch 492/1000 - train_loss: 0.7423 - train_accuracy: 0.7102 - val_loss: 1.0289 - val_accuracy: 0.6279\n",
      "Epoch 493/1000 - train_loss: 0.7420 - train_accuracy: 0.7116 - val_loss: 1.0291 - val_accuracy: 0.6279\n",
      "Epoch 494/1000 - train_loss: 0.7417 - train_accuracy: 0.7116 - val_loss: 1.0293 - val_accuracy: 0.6279\n",
      "Epoch 495/1000 - train_loss: 0.7414 - train_accuracy: 0.7116 - val_loss: 1.0295 - val_accuracy: 0.6279\n",
      "Epoch 496/1000 - train_loss: 0.7412 - train_accuracy: 0.7116 - val_loss: 1.0297 - val_accuracy: 0.6279\n",
      "Epoch 497/1000 - train_loss: 0.7409 - train_accuracy: 0.7116 - val_loss: 1.0299 - val_accuracy: 0.6279\n",
      "Epoch 498/1000 - train_loss: 0.7406 - train_accuracy: 0.7116 - val_loss: 1.0301 - val_accuracy: 0.6279\n",
      "Epoch 499/1000 - train_loss: 0.7404 - train_accuracy: 0.7116 - val_loss: 1.0302 - val_accuracy: 0.6279\n",
      "Epoch 500/1000 - train_loss: 0.7401 - train_accuracy: 0.7116 - val_loss: 1.0304 - val_accuracy: 0.6279\n",
      "Epoch 501/1000 - train_loss: 0.7398 - train_accuracy: 0.7116 - val_loss: 1.0306 - val_accuracy: 0.6279\n",
      "Epoch 502/1000 - train_loss: 0.7396 - train_accuracy: 0.7116 - val_loss: 1.0308 - val_accuracy: 0.6279\n",
      "Epoch 503/1000 - train_loss: 0.7393 - train_accuracy: 0.7116 - val_loss: 1.0310 - val_accuracy: 0.6279\n",
      "Epoch 504/1000 - train_loss: 0.7390 - train_accuracy: 0.7129 - val_loss: 1.0312 - val_accuracy: 0.6279\n",
      "Epoch 505/1000 - train_loss: 0.7388 - train_accuracy: 0.7129 - val_loss: 1.0313 - val_accuracy: 0.6279\n",
      "Epoch 506/1000 - train_loss: 0.7385 - train_accuracy: 0.7129 - val_loss: 1.0315 - val_accuracy: 0.6279\n",
      "Epoch 507/1000 - train_loss: 0.7382 - train_accuracy: 0.7129 - val_loss: 1.0317 - val_accuracy: 0.6279\n",
      "Epoch 508/1000 - train_loss: 0.7380 - train_accuracy: 0.7129 - val_loss: 1.0319 - val_accuracy: 0.6279\n",
      "Epoch 509/1000 - train_loss: 0.7377 - train_accuracy: 0.7129 - val_loss: 1.0320 - val_accuracy: 0.6279\n",
      "Epoch 510/1000 - train_loss: 0.7374 - train_accuracy: 0.7116 - val_loss: 1.0322 - val_accuracy: 0.6279\n",
      "Epoch 511/1000 - train_loss: 0.7372 - train_accuracy: 0.7116 - val_loss: 1.0324 - val_accuracy: 0.6279\n",
      "Epoch 512/1000 - train_loss: 0.7369 - train_accuracy: 0.7116 - val_loss: 1.0325 - val_accuracy: 0.6279\n",
      "Epoch 513/1000 - train_loss: 0.7367 - train_accuracy: 0.7116 - val_loss: 1.0327 - val_accuracy: 0.6279\n",
      "Epoch 514/1000 - train_loss: 0.7364 - train_accuracy: 0.7102 - val_loss: 1.0329 - val_accuracy: 0.6279\n",
      "Epoch 515/1000 - train_loss: 0.7361 - train_accuracy: 0.7102 - val_loss: 1.0330 - val_accuracy: 0.6279\n",
      "Epoch 516/1000 - train_loss: 0.7359 - train_accuracy: 0.7129 - val_loss: 1.0332 - val_accuracy: 0.6279\n",
      "Epoch 517/1000 - train_loss: 0.7356 - train_accuracy: 0.7129 - val_loss: 1.0333 - val_accuracy: 0.6279\n",
      "Epoch 518/1000 - train_loss: 0.7354 - train_accuracy: 0.7116 - val_loss: 1.0335 - val_accuracy: 0.6279\n",
      "Epoch 519/1000 - train_loss: 0.7351 - train_accuracy: 0.7116 - val_loss: 1.0336 - val_accuracy: 0.6279\n",
      "Epoch 520/1000 - train_loss: 0.7349 - train_accuracy: 0.7116 - val_loss: 1.0338 - val_accuracy: 0.6279\n",
      "Epoch 521/1000 - train_loss: 0.7346 - train_accuracy: 0.7116 - val_loss: 1.0339 - val_accuracy: 0.6279\n",
      "Epoch 522/1000 - train_loss: 0.7343 - train_accuracy: 0.7116 - val_loss: 1.0341 - val_accuracy: 0.6279\n",
      "Epoch 523/1000 - train_loss: 0.7341 - train_accuracy: 0.7116 - val_loss: 1.0342 - val_accuracy: 0.6221\n",
      "Epoch 524/1000 - train_loss: 0.7338 - train_accuracy: 0.7116 - val_loss: 1.0344 - val_accuracy: 0.6221\n",
      "Epoch 525/1000 - train_loss: 0.7336 - train_accuracy: 0.7129 - val_loss: 1.0345 - val_accuracy: 0.6221\n",
      "Epoch 526/1000 - train_loss: 0.7333 - train_accuracy: 0.7129 - val_loss: 1.0347 - val_accuracy: 0.6221\n",
      "Epoch 527/1000 - train_loss: 0.7331 - train_accuracy: 0.7129 - val_loss: 1.0348 - val_accuracy: 0.6221\n",
      "Epoch 528/1000 - train_loss: 0.7328 - train_accuracy: 0.7129 - val_loss: 1.0350 - val_accuracy: 0.6221\n",
      "Epoch 529/1000 - train_loss: 0.7325 - train_accuracy: 0.7129 - val_loss: 1.0351 - val_accuracy: 0.6221\n",
      "Epoch 530/1000 - train_loss: 0.7323 - train_accuracy: 0.7129 - val_loss: 1.0352 - val_accuracy: 0.6221\n",
      "Epoch 531/1000 - train_loss: 0.7320 - train_accuracy: 0.7129 - val_loss: 1.0354 - val_accuracy: 0.6221\n",
      "Epoch 532/1000 - train_loss: 0.7318 - train_accuracy: 0.7129 - val_loss: 1.0355 - val_accuracy: 0.6221\n",
      "Epoch 533/1000 - train_loss: 0.7315 - train_accuracy: 0.7129 - val_loss: 1.0357 - val_accuracy: 0.6221\n",
      "Epoch 534/1000 - train_loss: 0.7313 - train_accuracy: 0.7129 - val_loss: 1.0358 - val_accuracy: 0.6221\n",
      "Epoch 535/1000 - train_loss: 0.7310 - train_accuracy: 0.7129 - val_loss: 1.0359 - val_accuracy: 0.6221\n",
      "Epoch 536/1000 - train_loss: 0.7308 - train_accuracy: 0.7129 - val_loss: 1.0361 - val_accuracy: 0.6221\n",
      "Epoch 537/1000 - train_loss: 0.7305 - train_accuracy: 0.7143 - val_loss: 1.0362 - val_accuracy: 0.6279\n",
      "Epoch 538/1000 - train_loss: 0.7302 - train_accuracy: 0.7143 - val_loss: 1.0364 - val_accuracy: 0.6279\n",
      "Epoch 539/1000 - train_loss: 0.7300 - train_accuracy: 0.7143 - val_loss: 1.0365 - val_accuracy: 0.6279\n",
      "Epoch 540/1000 - train_loss: 0.7297 - train_accuracy: 0.7143 - val_loss: 1.0366 - val_accuracy: 0.6279\n",
      "Epoch 541/1000 - train_loss: 0.7295 - train_accuracy: 0.7143 - val_loss: 1.0368 - val_accuracy: 0.6279\n",
      "Epoch 542/1000 - train_loss: 0.7292 - train_accuracy: 0.7156 - val_loss: 1.0369 - val_accuracy: 0.6279\n",
      "Epoch 543/1000 - train_loss: 0.7290 - train_accuracy: 0.7156 - val_loss: 1.0370 - val_accuracy: 0.6279\n",
      "Epoch 544/1000 - train_loss: 0.7287 - train_accuracy: 0.7183 - val_loss: 1.0372 - val_accuracy: 0.6279\n",
      "Epoch 545/1000 - train_loss: 0.7285 - train_accuracy: 0.7197 - val_loss: 1.0373 - val_accuracy: 0.6279\n",
      "Epoch 546/1000 - train_loss: 0.7282 - train_accuracy: 0.7197 - val_loss: 1.0375 - val_accuracy: 0.6279\n",
      "Epoch 547/1000 - train_loss: 0.7279 - train_accuracy: 0.7197 - val_loss: 1.0376 - val_accuracy: 0.6279\n",
      "Epoch 548/1000 - train_loss: 0.7277 - train_accuracy: 0.7210 - val_loss: 1.0377 - val_accuracy: 0.6279\n",
      "Epoch 549/1000 - train_loss: 0.7274 - train_accuracy: 0.7210 - val_loss: 1.0379 - val_accuracy: 0.6279\n",
      "Epoch 550/1000 - train_loss: 0.7272 - train_accuracy: 0.7210 - val_loss: 1.0380 - val_accuracy: 0.6279\n",
      "Epoch 551/1000 - train_loss: 0.7269 - train_accuracy: 0.7210 - val_loss: 1.0381 - val_accuracy: 0.6279\n",
      "Epoch 552/1000 - train_loss: 0.7267 - train_accuracy: 0.7210 - val_loss: 1.0383 - val_accuracy: 0.6279\n",
      "Epoch 553/1000 - train_loss: 0.7264 - train_accuracy: 0.7210 - val_loss: 1.0384 - val_accuracy: 0.6279\n",
      "Epoch 554/1000 - train_loss: 0.7262 - train_accuracy: 0.7224 - val_loss: 1.0385 - val_accuracy: 0.6279\n",
      "Epoch 555/1000 - train_loss: 0.7259 - train_accuracy: 0.7224 - val_loss: 1.0386 - val_accuracy: 0.6279\n",
      "Epoch 556/1000 - train_loss: 0.7257 - train_accuracy: 0.7224 - val_loss: 1.0388 - val_accuracy: 0.6279\n",
      "Epoch 557/1000 - train_loss: 0.7254 - train_accuracy: 0.7224 - val_loss: 1.0389 - val_accuracy: 0.6279\n",
      "Epoch 558/1000 - train_loss: 0.7251 - train_accuracy: 0.7224 - val_loss: 1.0390 - val_accuracy: 0.6279\n",
      "Epoch 559/1000 - train_loss: 0.7249 - train_accuracy: 0.7224 - val_loss: 1.0392 - val_accuracy: 0.6279\n",
      "Epoch 560/1000 - train_loss: 0.7246 - train_accuracy: 0.7210 - val_loss: 1.0393 - val_accuracy: 0.6279\n",
      "Epoch 561/1000 - train_loss: 0.7244 - train_accuracy: 0.7210 - val_loss: 1.0394 - val_accuracy: 0.6279\n",
      "Epoch 562/1000 - train_loss: 0.7241 - train_accuracy: 0.7210 - val_loss: 1.0395 - val_accuracy: 0.6279\n",
      "Epoch 563/1000 - train_loss: 0.7239 - train_accuracy: 0.7210 - val_loss: 1.0397 - val_accuracy: 0.6279\n",
      "Epoch 564/1000 - train_loss: 0.7236 - train_accuracy: 0.7210 - val_loss: 1.0398 - val_accuracy: 0.6279\n",
      "Epoch 565/1000 - train_loss: 0.7234 - train_accuracy: 0.7183 - val_loss: 1.0399 - val_accuracy: 0.6279\n",
      "Epoch 566/1000 - train_loss: 0.7231 - train_accuracy: 0.7183 - val_loss: 1.0401 - val_accuracy: 0.6279\n",
      "Epoch 567/1000 - train_loss: 0.7229 - train_accuracy: 0.7156 - val_loss: 1.0402 - val_accuracy: 0.6279\n",
      "Epoch 568/1000 - train_loss: 0.7226 - train_accuracy: 0.7156 - val_loss: 1.0403 - val_accuracy: 0.6279\n",
      "Epoch 569/1000 - train_loss: 0.7224 - train_accuracy: 0.7156 - val_loss: 1.0404 - val_accuracy: 0.6279\n",
      "Epoch 570/1000 - train_loss: 0.7221 - train_accuracy: 0.7156 - val_loss: 1.0406 - val_accuracy: 0.6279\n",
      "Epoch 571/1000 - train_loss: 0.7219 - train_accuracy: 0.7170 - val_loss: 1.0407 - val_accuracy: 0.6279\n",
      "Epoch 572/1000 - train_loss: 0.7216 - train_accuracy: 0.7170 - val_loss: 1.0408 - val_accuracy: 0.6279\n",
      "Epoch 573/1000 - train_loss: 0.7213 - train_accuracy: 0.7170 - val_loss: 1.0410 - val_accuracy: 0.6279\n",
      "Epoch 574/1000 - train_loss: 0.7211 - train_accuracy: 0.7170 - val_loss: 1.0411 - val_accuracy: 0.6279\n",
      "Epoch 575/1000 - train_loss: 0.7208 - train_accuracy: 0.7170 - val_loss: 1.0412 - val_accuracy: 0.6279\n",
      "Epoch 576/1000 - train_loss: 0.7206 - train_accuracy: 0.7183 - val_loss: 1.0414 - val_accuracy: 0.6279\n",
      "Epoch 577/1000 - train_loss: 0.7203 - train_accuracy: 0.7183 - val_loss: 1.0415 - val_accuracy: 0.6279\n",
      "Epoch 578/1000 - train_loss: 0.7201 - train_accuracy: 0.7183 - val_loss: 1.0417 - val_accuracy: 0.6279\n",
      "Epoch 579/1000 - train_loss: 0.7198 - train_accuracy: 0.7183 - val_loss: 1.0418 - val_accuracy: 0.6279\n",
      "Epoch 580/1000 - train_loss: 0.7195 - train_accuracy: 0.7183 - val_loss: 1.0419 - val_accuracy: 0.6279\n",
      "Epoch 581/1000 - train_loss: 0.7193 - train_accuracy: 0.7170 - val_loss: 1.0420 - val_accuracy: 0.6279\n",
      "Epoch 582/1000 - train_loss: 0.7190 - train_accuracy: 0.7170 - val_loss: 1.0422 - val_accuracy: 0.6279\n",
      "Epoch 583/1000 - train_loss: 0.7188 - train_accuracy: 0.7170 - val_loss: 1.0423 - val_accuracy: 0.6279\n",
      "Epoch 584/1000 - train_loss: 0.7185 - train_accuracy: 0.7170 - val_loss: 1.0424 - val_accuracy: 0.6279\n",
      "Epoch 585/1000 - train_loss: 0.7182 - train_accuracy: 0.7156 - val_loss: 1.0425 - val_accuracy: 0.6279\n",
      "Epoch 586/1000 - train_loss: 0.7180 - train_accuracy: 0.7156 - val_loss: 1.0426 - val_accuracy: 0.6279\n",
      "Epoch 587/1000 - train_loss: 0.7177 - train_accuracy: 0.7170 - val_loss: 1.0427 - val_accuracy: 0.6279\n",
      "Epoch 588/1000 - train_loss: 0.7175 - train_accuracy: 0.7170 - val_loss: 1.0428 - val_accuracy: 0.6279\n",
      "Epoch 589/1000 - train_loss: 0.7172 - train_accuracy: 0.7170 - val_loss: 1.0428 - val_accuracy: 0.6279\n",
      "Epoch 590/1000 - train_loss: 0.7169 - train_accuracy: 0.7183 - val_loss: 1.0429 - val_accuracy: 0.6279\n",
      "Epoch 591/1000 - train_loss: 0.7167 - train_accuracy: 0.7183 - val_loss: 1.0430 - val_accuracy: 0.6279\n",
      "Epoch 592/1000 - train_loss: 0.7164 - train_accuracy: 0.7170 - val_loss: 1.0431 - val_accuracy: 0.6279\n",
      "Epoch 593/1000 - train_loss: 0.7162 - train_accuracy: 0.7170 - val_loss: 1.0431 - val_accuracy: 0.6279\n",
      "Epoch 594/1000 - train_loss: 0.7159 - train_accuracy: 0.7156 - val_loss: 1.0432 - val_accuracy: 0.6279\n",
      "Epoch 595/1000 - train_loss: 0.7156 - train_accuracy: 0.7156 - val_loss: 1.0433 - val_accuracy: 0.6279\n",
      "Epoch 596/1000 - train_loss: 0.7154 - train_accuracy: 0.7156 - val_loss: 1.0434 - val_accuracy: 0.6279\n",
      "Epoch 597/1000 - train_loss: 0.7151 - train_accuracy: 0.7170 - val_loss: 1.0434 - val_accuracy: 0.6279\n",
      "Epoch 598/1000 - train_loss: 0.7148 - train_accuracy: 0.7170 - val_loss: 1.0435 - val_accuracy: 0.6279\n",
      "Epoch 599/1000 - train_loss: 0.7145 - train_accuracy: 0.7183 - val_loss: 1.0436 - val_accuracy: 0.6279\n",
      "Epoch 600/1000 - train_loss: 0.7143 - train_accuracy: 0.7183 - val_loss: 1.0437 - val_accuracy: 0.6279\n",
      "Epoch 601/1000 - train_loss: 0.7140 - train_accuracy: 0.7170 - val_loss: 1.0438 - val_accuracy: 0.6279\n",
      "Epoch 602/1000 - train_loss: 0.7137 - train_accuracy: 0.7170 - val_loss: 1.0439 - val_accuracy: 0.6279\n",
      "Epoch 603/1000 - train_loss: 0.7134 - train_accuracy: 0.7170 - val_loss: 1.0440 - val_accuracy: 0.6337\n",
      "Epoch 604/1000 - train_loss: 0.7131 - train_accuracy: 0.7170 - val_loss: 1.0442 - val_accuracy: 0.6337\n",
      "Epoch 605/1000 - train_loss: 0.7128 - train_accuracy: 0.7156 - val_loss: 1.0443 - val_accuracy: 0.6337\n",
      "Epoch 606/1000 - train_loss: 0.7125 - train_accuracy: 0.7156 - val_loss: 1.0445 - val_accuracy: 0.6337\n",
      "Epoch 607/1000 - train_loss: 0.7122 - train_accuracy: 0.7156 - val_loss: 1.0446 - val_accuracy: 0.6395\n",
      "Epoch 608/1000 - train_loss: 0.7119 - train_accuracy: 0.7156 - val_loss: 1.0448 - val_accuracy: 0.6395\n",
      "Epoch 609/1000 - train_loss: 0.7116 - train_accuracy: 0.7156 - val_loss: 1.0450 - val_accuracy: 0.6395\n",
      "Epoch 610/1000 - train_loss: 0.7113 - train_accuracy: 0.7156 - val_loss: 1.0452 - val_accuracy: 0.6395\n",
      "Epoch 611/1000 - train_loss: 0.7110 - train_accuracy: 0.7156 - val_loss: 1.0454 - val_accuracy: 0.6395\n",
      "Epoch 612/1000 - train_loss: 0.7107 - train_accuracy: 0.7156 - val_loss: 1.0456 - val_accuracy: 0.6395\n",
      "Epoch 613/1000 - train_loss: 0.7104 - train_accuracy: 0.7156 - val_loss: 1.0458 - val_accuracy: 0.6395\n",
      "Epoch 614/1000 - train_loss: 0.7101 - train_accuracy: 0.7156 - val_loss: 1.0460 - val_accuracy: 0.6395\n",
      "Epoch 615/1000 - train_loss: 0.7098 - train_accuracy: 0.7156 - val_loss: 1.0463 - val_accuracy: 0.6395\n",
      "Epoch 616/1000 - train_loss: 0.7094 - train_accuracy: 0.7156 - val_loss: 1.0465 - val_accuracy: 0.6337\n",
      "Epoch 617/1000 - train_loss: 0.7091 - train_accuracy: 0.7170 - val_loss: 1.0467 - val_accuracy: 0.6337\n",
      "Epoch 618/1000 - train_loss: 0.7088 - train_accuracy: 0.7170 - val_loss: 1.0469 - val_accuracy: 0.6337\n",
      "Epoch 619/1000 - train_loss: 0.7085 - train_accuracy: 0.7170 - val_loss: 1.0472 - val_accuracy: 0.6337\n",
      "Epoch 620/1000 - train_loss: 0.7082 - train_accuracy: 0.7183 - val_loss: 1.0474 - val_accuracy: 0.6337\n",
      "Epoch 621/1000 - train_loss: 0.7079 - train_accuracy: 0.7183 - val_loss: 1.0476 - val_accuracy: 0.6337\n",
      "Epoch 622/1000 - train_loss: 0.7076 - train_accuracy: 0.7183 - val_loss: 1.0478 - val_accuracy: 0.6337\n",
      "Epoch 623/1000 - train_loss: 0.7073 - train_accuracy: 0.7183 - val_loss: 1.0480 - val_accuracy: 0.6337\n",
      "Epoch 624/1000 - train_loss: 0.7070 - train_accuracy: 0.7197 - val_loss: 1.0482 - val_accuracy: 0.6337\n",
      "Epoch 625/1000 - train_loss: 0.7067 - train_accuracy: 0.7197 - val_loss: 1.0484 - val_accuracy: 0.6337\n",
      "Epoch 626/1000 - train_loss: 0.7065 - train_accuracy: 0.7183 - val_loss: 1.0486 - val_accuracy: 0.6337\n",
      "Epoch 627/1000 - train_loss: 0.7062 - train_accuracy: 0.7197 - val_loss: 1.0488 - val_accuracy: 0.6337\n",
      "Epoch 628/1000 - train_loss: 0.7059 - train_accuracy: 0.7183 - val_loss: 1.0490 - val_accuracy: 0.6337\n",
      "Epoch 629/1000 - train_loss: 0.7056 - train_accuracy: 0.7183 - val_loss: 1.0492 - val_accuracy: 0.6337\n",
      "Epoch 630/1000 - train_loss: 0.7053 - train_accuracy: 0.7183 - val_loss: 1.0494 - val_accuracy: 0.6337\n",
      "Epoch 631/1000 - train_loss: 0.7050 - train_accuracy: 0.7183 - val_loss: 1.0495 - val_accuracy: 0.6337\n",
      "Epoch 632/1000 - train_loss: 0.7047 - train_accuracy: 0.7197 - val_loss: 1.0497 - val_accuracy: 0.6337\n",
      "Epoch 633/1000 - train_loss: 0.7045 - train_accuracy: 0.7210 - val_loss: 1.0499 - val_accuracy: 0.6279\n",
      "Epoch 634/1000 - train_loss: 0.7042 - train_accuracy: 0.7210 - val_loss: 1.0500 - val_accuracy: 0.6279\n",
      "Epoch 635/1000 - train_loss: 0.7039 - train_accuracy: 0.7210 - val_loss: 1.0502 - val_accuracy: 0.6279\n",
      "Epoch 636/1000 - train_loss: 0.7036 - train_accuracy: 0.7210 - val_loss: 1.0504 - val_accuracy: 0.6279\n",
      "Epoch 637/1000 - train_loss: 0.7033 - train_accuracy: 0.7210 - val_loss: 1.0505 - val_accuracy: 0.6279\n",
      "Epoch 638/1000 - train_loss: 0.7030 - train_accuracy: 0.7210 - val_loss: 1.0507 - val_accuracy: 0.6337\n",
      "Epoch 639/1000 - train_loss: 0.7027 - train_accuracy: 0.7224 - val_loss: 1.0508 - val_accuracy: 0.6337\n",
      "Epoch 640/1000 - train_loss: 0.7025 - train_accuracy: 0.7224 - val_loss: 1.0510 - val_accuracy: 0.6337\n",
      "Epoch 641/1000 - train_loss: 0.7022 - train_accuracy: 0.7224 - val_loss: 1.0511 - val_accuracy: 0.6337\n",
      "Epoch 642/1000 - train_loss: 0.7019 - train_accuracy: 0.7224 - val_loss: 1.0513 - val_accuracy: 0.6279\n",
      "Epoch 643/1000 - train_loss: 0.7016 - train_accuracy: 0.7224 - val_loss: 1.0514 - val_accuracy: 0.6279\n",
      "Epoch 644/1000 - train_loss: 0.7013 - train_accuracy: 0.7224 - val_loss: 1.0515 - val_accuracy: 0.6279\n",
      "Epoch 645/1000 - train_loss: 0.7010 - train_accuracy: 0.7237 - val_loss: 1.0517 - val_accuracy: 0.6279\n",
      "Epoch 646/1000 - train_loss: 0.7008 - train_accuracy: 0.7237 - val_loss: 1.0518 - val_accuracy: 0.6279\n",
      "Epoch 647/1000 - train_loss: 0.7005 - train_accuracy: 0.7224 - val_loss: 1.0519 - val_accuracy: 0.6279\n",
      "Epoch 648/1000 - train_loss: 0.7002 - train_accuracy: 0.7237 - val_loss: 1.0521 - val_accuracy: 0.6279\n",
      "Epoch 649/1000 - train_loss: 0.6999 - train_accuracy: 0.7237 - val_loss: 1.0522 - val_accuracy: 0.6279\n",
      "Epoch 650/1000 - train_loss: 0.6997 - train_accuracy: 0.7237 - val_loss: 1.0523 - val_accuracy: 0.6279\n",
      "Epoch 651/1000 - train_loss: 0.6994 - train_accuracy: 0.7237 - val_loss: 1.0525 - val_accuracy: 0.6279\n",
      "Epoch 652/1000 - train_loss: 0.6991 - train_accuracy: 0.7237 - val_loss: 1.0526 - val_accuracy: 0.6279\n",
      "Epoch 653/1000 - train_loss: 0.6989 - train_accuracy: 0.7224 - val_loss: 1.0527 - val_accuracy: 0.6279\n",
      "Epoch 654/1000 - train_loss: 0.6986 - train_accuracy: 0.7224 - val_loss: 1.0528 - val_accuracy: 0.6221\n",
      "Epoch 655/1000 - train_loss: 0.6983 - train_accuracy: 0.7224 - val_loss: 1.0529 - val_accuracy: 0.6221\n",
      "Epoch 656/1000 - train_loss: 0.6981 - train_accuracy: 0.7237 - val_loss: 1.0531 - val_accuracy: 0.6221\n",
      "Epoch 657/1000 - train_loss: 0.6978 - train_accuracy: 0.7237 - val_loss: 1.0532 - val_accuracy: 0.6221\n",
      "Epoch 658/1000 - train_loss: 0.6976 - train_accuracy: 0.7237 - val_loss: 1.0533 - val_accuracy: 0.6221\n",
      "Epoch 659/1000 - train_loss: 0.6973 - train_accuracy: 0.7237 - val_loss: 1.0534 - val_accuracy: 0.6221\n",
      "Epoch 660/1000 - train_loss: 0.6971 - train_accuracy: 0.7237 - val_loss: 1.0536 - val_accuracy: 0.6221\n",
      "Epoch 661/1000 - train_loss: 0.6968 - train_accuracy: 0.7237 - val_loss: 1.0537 - val_accuracy: 0.6221\n",
      "Epoch 662/1000 - train_loss: 0.6965 - train_accuracy: 0.7237 - val_loss: 1.0538 - val_accuracy: 0.6221\n",
      "Epoch 663/1000 - train_loss: 0.6963 - train_accuracy: 0.7251 - val_loss: 1.0540 - val_accuracy: 0.6221\n",
      "Epoch 664/1000 - train_loss: 0.6960 - train_accuracy: 0.7251 - val_loss: 1.0541 - val_accuracy: 0.6221\n",
      "Epoch 665/1000 - train_loss: 0.6958 - train_accuracy: 0.7251 - val_loss: 1.0542 - val_accuracy: 0.6163\n",
      "Epoch 666/1000 - train_loss: 0.6955 - train_accuracy: 0.7251 - val_loss: 1.0544 - val_accuracy: 0.6163\n",
      "Epoch 667/1000 - train_loss: 0.6953 - train_accuracy: 0.7264 - val_loss: 1.0545 - val_accuracy: 0.6163\n",
      "Epoch 668/1000 - train_loss: 0.6951 - train_accuracy: 0.7264 - val_loss: 1.0546 - val_accuracy: 0.6163\n",
      "Epoch 669/1000 - train_loss: 0.6948 - train_accuracy: 0.7264 - val_loss: 1.0548 - val_accuracy: 0.6163\n",
      "Epoch 670/1000 - train_loss: 0.6946 - train_accuracy: 0.7264 - val_loss: 1.0549 - val_accuracy: 0.6163\n",
      "Epoch 671/1000 - train_loss: 0.6943 - train_accuracy: 0.7264 - val_loss: 1.0550 - val_accuracy: 0.6163\n",
      "Epoch 672/1000 - train_loss: 0.6941 - train_accuracy: 0.7251 - val_loss: 1.0552 - val_accuracy: 0.6163\n",
      "Epoch 673/1000 - train_loss: 0.6938 - train_accuracy: 0.7264 - val_loss: 1.0553 - val_accuracy: 0.6163\n",
      "Epoch 674/1000 - train_loss: 0.6936 - train_accuracy: 0.7264 - val_loss: 1.0555 - val_accuracy: 0.6105\n",
      "Epoch 675/1000 - train_loss: 0.6934 - train_accuracy: 0.7264 - val_loss: 1.0556 - val_accuracy: 0.6105\n",
      "Epoch 676/1000 - train_loss: 0.6931 - train_accuracy: 0.7278 - val_loss: 1.0558 - val_accuracy: 0.6105\n",
      "Epoch 677/1000 - train_loss: 0.6929 - train_accuracy: 0.7278 - val_loss: 1.0559 - val_accuracy: 0.6105\n",
      "Epoch 678/1000 - train_loss: 0.6927 - train_accuracy: 0.7278 - val_loss: 1.0561 - val_accuracy: 0.6105\n",
      "Epoch 679/1000 - train_loss: 0.6924 - train_accuracy: 0.7264 - val_loss: 1.0562 - val_accuracy: 0.6105\n",
      "Epoch 680/1000 - train_loss: 0.6922 - train_accuracy: 0.7264 - val_loss: 1.0564 - val_accuracy: 0.6105\n",
      "Epoch 681/1000 - train_loss: 0.6920 - train_accuracy: 0.7251 - val_loss: 1.0565 - val_accuracy: 0.6105\n",
      "Epoch 682/1000 - train_loss: 0.6917 - train_accuracy: 0.7251 - val_loss: 1.0567 - val_accuracy: 0.6105\n",
      "Epoch 683/1000 - train_loss: 0.6915 - train_accuracy: 0.7251 - val_loss: 1.0568 - val_accuracy: 0.6105\n",
      "Epoch 684/1000 - train_loss: 0.6913 - train_accuracy: 0.7251 - val_loss: 1.0570 - val_accuracy: 0.6105\n",
      "Epoch 685/1000 - train_loss: 0.6911 - train_accuracy: 0.7237 - val_loss: 1.0571 - val_accuracy: 0.6105\n",
      "Epoch 686/1000 - train_loss: 0.6908 - train_accuracy: 0.7237 - val_loss: 1.0573 - val_accuracy: 0.6105\n",
      "Epoch 687/1000 - train_loss: 0.6906 - train_accuracy: 0.7237 - val_loss: 1.0575 - val_accuracy: 0.6105\n",
      "Epoch 688/1000 - train_loss: 0.6904 - train_accuracy: 0.7224 - val_loss: 1.0576 - val_accuracy: 0.6105\n",
      "Epoch 689/1000 - train_loss: 0.6902 - train_accuracy: 0.7224 - val_loss: 1.0578 - val_accuracy: 0.6105\n",
      "Epoch 690/1000 - train_loss: 0.6899 - train_accuracy: 0.7237 - val_loss: 1.0580 - val_accuracy: 0.6105\n",
      "Epoch 691/1000 - train_loss: 0.6897 - train_accuracy: 0.7237 - val_loss: 1.0581 - val_accuracy: 0.6105\n",
      "Epoch 692/1000 - train_loss: 0.6895 - train_accuracy: 0.7224 - val_loss: 1.0583 - val_accuracy: 0.6105\n",
      "Epoch 693/1000 - train_loss: 0.6893 - train_accuracy: 0.7224 - val_loss: 1.0585 - val_accuracy: 0.6105\n",
      "Epoch 694/1000 - train_loss: 0.6890 - train_accuracy: 0.7224 - val_loss: 1.0587 - val_accuracy: 0.6047\n",
      "Epoch 695/1000 - train_loss: 0.6888 - train_accuracy: 0.7224 - val_loss: 1.0588 - val_accuracy: 0.6047\n",
      "Epoch 696/1000 - train_loss: 0.6886 - train_accuracy: 0.7224 - val_loss: 1.0590 - val_accuracy: 0.6047\n",
      "Epoch 697/1000 - train_loss: 0.6884 - train_accuracy: 0.7224 - val_loss: 1.0592 - val_accuracy: 0.6047\n",
      "Epoch 698/1000 - train_loss: 0.6882 - train_accuracy: 0.7224 - val_loss: 1.0594 - val_accuracy: 0.6047\n",
      "Epoch 699/1000 - train_loss: 0.6879 - train_accuracy: 0.7237 - val_loss: 1.0596 - val_accuracy: 0.6047\n",
      "Epoch 700/1000 - train_loss: 0.6877 - train_accuracy: 0.7237 - val_loss: 1.0597 - val_accuracy: 0.6047\n",
      "Epoch 701/1000 - train_loss: 0.6875 - train_accuracy: 0.7237 - val_loss: 1.0599 - val_accuracy: 0.6047\n",
      "Epoch 702/1000 - train_loss: 0.6873 - train_accuracy: 0.7237 - val_loss: 1.0601 - val_accuracy: 0.6047\n",
      "Epoch 703/1000 - train_loss: 0.6871 - train_accuracy: 0.7237 - val_loss: 1.0603 - val_accuracy: 0.6047\n",
      "Epoch 704/1000 - train_loss: 0.6869 - train_accuracy: 0.7237 - val_loss: 1.0605 - val_accuracy: 0.6047\n",
      "Epoch 705/1000 - train_loss: 0.6866 - train_accuracy: 0.7237 - val_loss: 1.0607 - val_accuracy: 0.6047\n",
      "Epoch 706/1000 - train_loss: 0.6864 - train_accuracy: 0.7237 - val_loss: 1.0609 - val_accuracy: 0.6047\n",
      "Epoch 707/1000 - train_loss: 0.6862 - train_accuracy: 0.7237 - val_loss: 1.0611 - val_accuracy: 0.6047\n",
      "Epoch 708/1000 - train_loss: 0.6860 - train_accuracy: 0.7237 - val_loss: 1.0613 - val_accuracy: 0.6047\n",
      "Epoch 709/1000 - train_loss: 0.6858 - train_accuracy: 0.7224 - val_loss: 1.0615 - val_accuracy: 0.6047\n",
      "Epoch 710/1000 - train_loss: 0.6856 - train_accuracy: 0.7224 - val_loss: 1.0617 - val_accuracy: 0.6047\n",
      "Epoch 711/1000 - train_loss: 0.6853 - train_accuracy: 0.7224 - val_loss: 1.0619 - val_accuracy: 0.6047\n",
      "Epoch 712/1000 - train_loss: 0.6851 - train_accuracy: 0.7224 - val_loss: 1.0621 - val_accuracy: 0.6047\n",
      "Epoch 713/1000 - train_loss: 0.6849 - train_accuracy: 0.7224 - val_loss: 1.0623 - val_accuracy: 0.6047\n",
      "Epoch 714/1000 - train_loss: 0.6847 - train_accuracy: 0.7224 - val_loss: 1.0625 - val_accuracy: 0.6047\n",
      "Epoch 715/1000 - train_loss: 0.6845 - train_accuracy: 0.7224 - val_loss: 1.0627 - val_accuracy: 0.6047\n",
      "Epoch 716/1000 - train_loss: 0.6843 - train_accuracy: 0.7224 - val_loss: 1.0629 - val_accuracy: 0.6047\n",
      "Epoch 717/1000 - train_loss: 0.6841 - train_accuracy: 0.7224 - val_loss: 1.0631 - val_accuracy: 0.6047\n",
      "Epoch 718/1000 - train_loss: 0.6838 - train_accuracy: 0.7224 - val_loss: 1.0633 - val_accuracy: 0.6047\n",
      "Epoch 719/1000 - train_loss: 0.6836 - train_accuracy: 0.7224 - val_loss: 1.0636 - val_accuracy: 0.6047\n",
      "Epoch 720/1000 - train_loss: 0.6834 - train_accuracy: 0.7237 - val_loss: 1.0638 - val_accuracy: 0.6047\n",
      "Epoch 721/1000 - train_loss: 0.6832 - train_accuracy: 0.7237 - val_loss: 1.0640 - val_accuracy: 0.6047\n",
      "Epoch 722/1000 - train_loss: 0.6830 - train_accuracy: 0.7237 - val_loss: 1.0642 - val_accuracy: 0.6047\n",
      "Epoch 723/1000 - train_loss: 0.6828 - train_accuracy: 0.7237 - val_loss: 1.0644 - val_accuracy: 0.6047\n",
      "Epoch 724/1000 - train_loss: 0.6825 - train_accuracy: 0.7237 - val_loss: 1.0646 - val_accuracy: 0.6047\n",
      "Epoch 725/1000 - train_loss: 0.6823 - train_accuracy: 0.7237 - val_loss: 1.0649 - val_accuracy: 0.6047\n",
      "Epoch 726/1000 - train_loss: 0.6821 - train_accuracy: 0.7237 - val_loss: 1.0651 - val_accuracy: 0.6047\n",
      "Epoch 727/1000 - train_loss: 0.6819 - train_accuracy: 0.7224 - val_loss: 1.0653 - val_accuracy: 0.6047\n",
      "Epoch 728/1000 - train_loss: 0.6817 - train_accuracy: 0.7224 - val_loss: 1.0655 - val_accuracy: 0.6047\n",
      "Epoch 729/1000 - train_loss: 0.6815 - train_accuracy: 0.7224 - val_loss: 1.0657 - val_accuracy: 0.6047\n",
      "Epoch 730/1000 - train_loss: 0.6812 - train_accuracy: 0.7224 - val_loss: 1.0660 - val_accuracy: 0.6047\n",
      "Epoch 731/1000 - train_loss: 0.6810 - train_accuracy: 0.7224 - val_loss: 1.0662 - val_accuracy: 0.6047\n",
      "Epoch 732/1000 - train_loss: 0.6808 - train_accuracy: 0.7224 - val_loss: 1.0664 - val_accuracy: 0.6047\n",
      "Epoch 733/1000 - train_loss: 0.6806 - train_accuracy: 0.7224 - val_loss: 1.0666 - val_accuracy: 0.6047\n",
      "Epoch 734/1000 - train_loss: 0.6804 - train_accuracy: 0.7224 - val_loss: 1.0669 - val_accuracy: 0.6047\n",
      "Epoch 735/1000 - train_loss: 0.6801 - train_accuracy: 0.7224 - val_loss: 1.0671 - val_accuracy: 0.6047\n",
      "Epoch 736/1000 - train_loss: 0.6799 - train_accuracy: 0.7224 - val_loss: 1.0673 - val_accuracy: 0.6047\n",
      "Epoch 737/1000 - train_loss: 0.6797 - train_accuracy: 0.7224 - val_loss: 1.0675 - val_accuracy: 0.6047\n",
      "Epoch 738/1000 - train_loss: 0.6795 - train_accuracy: 0.7224 - val_loss: 1.0677 - val_accuracy: 0.5988\n",
      "Epoch 739/1000 - train_loss: 0.6792 - train_accuracy: 0.7210 - val_loss: 1.0680 - val_accuracy: 0.5988\n",
      "Epoch 740/1000 - train_loss: 0.6790 - train_accuracy: 0.7224 - val_loss: 1.0682 - val_accuracy: 0.5988\n",
      "Epoch 741/1000 - train_loss: 0.6788 - train_accuracy: 0.7224 - val_loss: 1.0684 - val_accuracy: 0.5988\n",
      "Epoch 742/1000 - train_loss: 0.6786 - train_accuracy: 0.7224 - val_loss: 1.0686 - val_accuracy: 0.6047\n",
      "Epoch 743/1000 - train_loss: 0.6783 - train_accuracy: 0.7224 - val_loss: 1.0688 - val_accuracy: 0.6047\n",
      "Epoch 744/1000 - train_loss: 0.6781 - train_accuracy: 0.7224 - val_loss: 1.0691 - val_accuracy: 0.5988\n",
      "Epoch 745/1000 - train_loss: 0.6779 - train_accuracy: 0.7224 - val_loss: 1.0693 - val_accuracy: 0.5988\n",
      "Epoch 746/1000 - train_loss: 0.6776 - train_accuracy: 0.7224 - val_loss: 1.0695 - val_accuracy: 0.5988\n",
      "Epoch 747/1000 - train_loss: 0.6774 - train_accuracy: 0.7210 - val_loss: 1.0697 - val_accuracy: 0.5988\n",
      "Epoch 748/1000 - train_loss: 0.6772 - train_accuracy: 0.7210 - val_loss: 1.0699 - val_accuracy: 0.5988\n",
      "Epoch 749/1000 - train_loss: 0.6769 - train_accuracy: 0.7210 - val_loss: 1.0702 - val_accuracy: 0.5988\n",
      "Epoch 750/1000 - train_loss: 0.6767 - train_accuracy: 0.7210 - val_loss: 1.0704 - val_accuracy: 0.5988\n",
      "Epoch 751/1000 - train_loss: 0.6765 - train_accuracy: 0.7210 - val_loss: 1.0706 - val_accuracy: 0.5988\n",
      "Epoch 752/1000 - train_loss: 0.6763 - train_accuracy: 0.7210 - val_loss: 1.0708 - val_accuracy: 0.5988\n",
      "Epoch 753/1000 - train_loss: 0.6760 - train_accuracy: 0.7210 - val_loss: 1.0710 - val_accuracy: 0.5988\n",
      "Epoch 754/1000 - train_loss: 0.6758 - train_accuracy: 0.7210 - val_loss: 1.0712 - val_accuracy: 0.5988\n",
      "Epoch 755/1000 - train_loss: 0.6756 - train_accuracy: 0.7197 - val_loss: 1.0714 - val_accuracy: 0.5988\n",
      "Epoch 756/1000 - train_loss: 0.6753 - train_accuracy: 0.7197 - val_loss: 1.0717 - val_accuracy: 0.5988\n",
      "Epoch 757/1000 - train_loss: 0.6751 - train_accuracy: 0.7197 - val_loss: 1.0719 - val_accuracy: 0.5988\n",
      "Epoch 758/1000 - train_loss: 0.6749 - train_accuracy: 0.7197 - val_loss: 1.0721 - val_accuracy: 0.5988\n",
      "Epoch 759/1000 - train_loss: 0.6747 - train_accuracy: 0.7210 - val_loss: 1.0723 - val_accuracy: 0.5988\n",
      "Epoch 760/1000 - train_loss: 0.6744 - train_accuracy: 0.7197 - val_loss: 1.0725 - val_accuracy: 0.5988\n",
      "Epoch 761/1000 - train_loss: 0.6742 - train_accuracy: 0.7197 - val_loss: 1.0727 - val_accuracy: 0.5988\n",
      "Epoch 762/1000 - train_loss: 0.6740 - train_accuracy: 0.7197 - val_loss: 1.0730 - val_accuracy: 0.5988\n",
      "Epoch 763/1000 - train_loss: 0.6737 - train_accuracy: 0.7197 - val_loss: 1.0732 - val_accuracy: 0.5930\n",
      "Epoch 764/1000 - train_loss: 0.6735 - train_accuracy: 0.7197 - val_loss: 1.0734 - val_accuracy: 0.5930\n",
      "Epoch 765/1000 - train_loss: 0.6733 - train_accuracy: 0.7183 - val_loss: 1.0736 - val_accuracy: 0.5930\n",
      "Epoch 766/1000 - train_loss: 0.6731 - train_accuracy: 0.7183 - val_loss: 1.0738 - val_accuracy: 0.5930\n",
      "Epoch 767/1000 - train_loss: 0.6729 - train_accuracy: 0.7183 - val_loss: 1.0740 - val_accuracy: 0.5930\n",
      "Epoch 768/1000 - train_loss: 0.6726 - train_accuracy: 0.7183 - val_loss: 1.0743 - val_accuracy: 0.5930\n",
      "Epoch 769/1000 - train_loss: 0.6724 - train_accuracy: 0.7183 - val_loss: 1.0745 - val_accuracy: 0.5930\n",
      "Epoch 770/1000 - train_loss: 0.6722 - train_accuracy: 0.7183 - val_loss: 1.0747 - val_accuracy: 0.5930\n",
      "Epoch 771/1000 - train_loss: 0.6720 - train_accuracy: 0.7183 - val_loss: 1.0749 - val_accuracy: 0.5930\n",
      "Epoch 772/1000 - train_loss: 0.6717 - train_accuracy: 0.7183 - val_loss: 1.0751 - val_accuracy: 0.5930\n",
      "Epoch 773/1000 - train_loss: 0.6715 - train_accuracy: 0.7170 - val_loss: 1.0753 - val_accuracy: 0.5930\n",
      "Epoch 774/1000 - train_loss: 0.6713 - train_accuracy: 0.7170 - val_loss: 1.0755 - val_accuracy: 0.5930\n",
      "Epoch 775/1000 - train_loss: 0.6711 - train_accuracy: 0.7170 - val_loss: 1.0757 - val_accuracy: 0.5930\n",
      "Epoch 776/1000 - train_loss: 0.6709 - train_accuracy: 0.7170 - val_loss: 1.0759 - val_accuracy: 0.5930\n",
      "Epoch 777/1000 - train_loss: 0.6706 - train_accuracy: 0.7170 - val_loss: 1.0762 - val_accuracy: 0.5930\n",
      "Epoch 778/1000 - train_loss: 0.6704 - train_accuracy: 0.7170 - val_loss: 1.0764 - val_accuracy: 0.5930\n",
      "Epoch 779/1000 - train_loss: 0.6702 - train_accuracy: 0.7170 - val_loss: 1.0766 - val_accuracy: 0.5930\n",
      "Epoch 780/1000 - train_loss: 0.6700 - train_accuracy: 0.7170 - val_loss: 1.0768 - val_accuracy: 0.5930\n",
      "Epoch 781/1000 - train_loss: 0.6698 - train_accuracy: 0.7156 - val_loss: 1.0770 - val_accuracy: 0.5930\n",
      "Epoch 782/1000 - train_loss: 0.6695 - train_accuracy: 0.7156 - val_loss: 1.0772 - val_accuracy: 0.5930\n",
      "Epoch 783/1000 - train_loss: 0.6693 - train_accuracy: 0.7170 - val_loss: 1.0774 - val_accuracy: 0.5930\n",
      "Epoch 784/1000 - train_loss: 0.6691 - train_accuracy: 0.7170 - val_loss: 1.0776 - val_accuracy: 0.5930\n",
      "Epoch 785/1000 - train_loss: 0.6689 - train_accuracy: 0.7170 - val_loss: 1.0778 - val_accuracy: 0.6047\n",
      "Epoch 786/1000 - train_loss: 0.6686 - train_accuracy: 0.7170 - val_loss: 1.0780 - val_accuracy: 0.6047\n",
      "Epoch 787/1000 - train_loss: 0.6684 - train_accuracy: 0.7170 - val_loss: 1.0782 - val_accuracy: 0.6047\n",
      "Epoch 788/1000 - train_loss: 0.6682 - train_accuracy: 0.7170 - val_loss: 1.0784 - val_accuracy: 0.6047\n",
      "Epoch 789/1000 - train_loss: 0.6680 - train_accuracy: 0.7170 - val_loss: 1.0786 - val_accuracy: 0.6047\n",
      "Epoch 790/1000 - train_loss: 0.6678 - train_accuracy: 0.7170 - val_loss: 1.0789 - val_accuracy: 0.6047\n",
      "Epoch 791/1000 - train_loss: 0.6675 - train_accuracy: 0.7183 - val_loss: 1.0791 - val_accuracy: 0.6047\n",
      "Epoch 792/1000 - train_loss: 0.6673 - train_accuracy: 0.7183 - val_loss: 1.0793 - val_accuracy: 0.6047\n",
      "Epoch 793/1000 - train_loss: 0.6671 - train_accuracy: 0.7183 - val_loss: 1.0795 - val_accuracy: 0.6047\n",
      "Epoch 794/1000 - train_loss: 0.6669 - train_accuracy: 0.7183 - val_loss: 1.0797 - val_accuracy: 0.6047\n",
      "Epoch 795/1000 - train_loss: 0.6667 - train_accuracy: 0.7183 - val_loss: 1.0799 - val_accuracy: 0.6105\n",
      "Epoch 796/1000 - train_loss: 0.6664 - train_accuracy: 0.7183 - val_loss: 1.0801 - val_accuracy: 0.6105\n",
      "Epoch 797/1000 - train_loss: 0.6662 - train_accuracy: 0.7170 - val_loss: 1.0803 - val_accuracy: 0.6105\n",
      "Epoch 798/1000 - train_loss: 0.6660 - train_accuracy: 0.7170 - val_loss: 1.0805 - val_accuracy: 0.6105\n",
      "Epoch 799/1000 - train_loss: 0.6658 - train_accuracy: 0.7170 - val_loss: 1.0807 - val_accuracy: 0.6105\n",
      "Epoch 800/1000 - train_loss: 0.6656 - train_accuracy: 0.7170 - val_loss: 1.0809 - val_accuracy: 0.6105\n",
      "Epoch 801/1000 - train_loss: 0.6653 - train_accuracy: 0.7170 - val_loss: 1.0811 - val_accuracy: 0.6105\n",
      "Epoch 802/1000 - train_loss: 0.6651 - train_accuracy: 0.7170 - val_loss: 1.0813 - val_accuracy: 0.6105\n",
      "Epoch 803/1000 - train_loss: 0.6649 - train_accuracy: 0.7170 - val_loss: 1.0815 - val_accuracy: 0.6105\n",
      "Epoch 804/1000 - train_loss: 0.6647 - train_accuracy: 0.7170 - val_loss: 1.0817 - val_accuracy: 0.6105\n",
      "Epoch 805/1000 - train_loss: 0.6645 - train_accuracy: 0.7170 - val_loss: 1.0819 - val_accuracy: 0.6105\n",
      "Epoch 806/1000 - train_loss: 0.6642 - train_accuracy: 0.7143 - val_loss: 1.0821 - val_accuracy: 0.6047\n",
      "Epoch 807/1000 - train_loss: 0.6640 - train_accuracy: 0.7143 - val_loss: 1.0823 - val_accuracy: 0.6047\n",
      "Epoch 808/1000 - train_loss: 0.6638 - train_accuracy: 0.7143 - val_loss: 1.0826 - val_accuracy: 0.6047\n",
      "Epoch 809/1000 - train_loss: 0.6636 - train_accuracy: 0.7129 - val_loss: 1.0828 - val_accuracy: 0.6047\n",
      "Epoch 810/1000 - train_loss: 0.6634 - train_accuracy: 0.7129 - val_loss: 1.0830 - val_accuracy: 0.6047\n",
      "Epoch 811/1000 - train_loss: 0.6631 - train_accuracy: 0.7129 - val_loss: 1.0832 - val_accuracy: 0.6047\n",
      "Epoch 812/1000 - train_loss: 0.6629 - train_accuracy: 0.7129 - val_loss: 1.0834 - val_accuracy: 0.6047\n",
      "Epoch 813/1000 - train_loss: 0.6627 - train_accuracy: 0.7129 - val_loss: 1.0836 - val_accuracy: 0.6047\n",
      "Epoch 814/1000 - train_loss: 0.6625 - train_accuracy: 0.7129 - val_loss: 1.0838 - val_accuracy: 0.6047\n",
      "Epoch 815/1000 - train_loss: 0.6623 - train_accuracy: 0.7143 - val_loss: 1.0840 - val_accuracy: 0.5988\n",
      "Epoch 816/1000 - train_loss: 0.6620 - train_accuracy: 0.7143 - val_loss: 1.0842 - val_accuracy: 0.5930\n",
      "Epoch 817/1000 - train_loss: 0.6618 - train_accuracy: 0.7143 - val_loss: 1.0845 - val_accuracy: 0.5930\n",
      "Epoch 818/1000 - train_loss: 0.6616 - train_accuracy: 0.7143 - val_loss: 1.0847 - val_accuracy: 0.5930\n",
      "Epoch 819/1000 - train_loss: 0.6614 - train_accuracy: 0.7143 - val_loss: 1.0849 - val_accuracy: 0.5930\n",
      "Epoch 820/1000 - train_loss: 0.6612 - train_accuracy: 0.7143 - val_loss: 1.0851 - val_accuracy: 0.5930\n",
      "Epoch 821/1000 - train_loss: 0.6609 - train_accuracy: 0.7143 - val_loss: 1.0853 - val_accuracy: 0.5930\n",
      "Epoch 822/1000 - train_loss: 0.6607 - train_accuracy: 0.7143 - val_loss: 1.0856 - val_accuracy: 0.5930\n",
      "Epoch 823/1000 - train_loss: 0.6605 - train_accuracy: 0.7156 - val_loss: 1.0858 - val_accuracy: 0.5930\n",
      "Epoch 824/1000 - train_loss: 0.6603 - train_accuracy: 0.7170 - val_loss: 1.0860 - val_accuracy: 0.5930\n",
      "Epoch 825/1000 - train_loss: 0.6601 - train_accuracy: 0.7170 - val_loss: 1.0862 - val_accuracy: 0.5930\n",
      "Epoch 826/1000 - train_loss: 0.6599 - train_accuracy: 0.7170 - val_loss: 1.0865 - val_accuracy: 0.5930\n",
      "Epoch 827/1000 - train_loss: 0.6596 - train_accuracy: 0.7170 - val_loss: 1.0867 - val_accuracy: 0.5930\n",
      "Epoch 828/1000 - train_loss: 0.6594 - train_accuracy: 0.7170 - val_loss: 1.0869 - val_accuracy: 0.5930\n",
      "Epoch 829/1000 - train_loss: 0.6592 - train_accuracy: 0.7170 - val_loss: 1.0872 - val_accuracy: 0.5930\n",
      "Epoch 830/1000 - train_loss: 0.6590 - train_accuracy: 0.7183 - val_loss: 1.0874 - val_accuracy: 0.5930\n",
      "Epoch 831/1000 - train_loss: 0.6588 - train_accuracy: 0.7197 - val_loss: 1.0876 - val_accuracy: 0.5930\n",
      "Epoch 832/1000 - train_loss: 0.6585 - train_accuracy: 0.7197 - val_loss: 1.0879 - val_accuracy: 0.5930\n",
      "Epoch 833/1000 - train_loss: 0.6583 - train_accuracy: 0.7197 - val_loss: 1.0881 - val_accuracy: 0.5930\n",
      "Epoch 834/1000 - train_loss: 0.6581 - train_accuracy: 0.7197 - val_loss: 1.0884 - val_accuracy: 0.5930\n",
      "Epoch 835/1000 - train_loss: 0.6579 - train_accuracy: 0.7197 - val_loss: 1.0886 - val_accuracy: 0.5930\n",
      "Epoch 836/1000 - train_loss: 0.6577 - train_accuracy: 0.7197 - val_loss: 1.0889 - val_accuracy: 0.5930\n",
      "Epoch 837/1000 - train_loss: 0.6575 - train_accuracy: 0.7197 - val_loss: 1.0892 - val_accuracy: 0.5930\n",
      "Epoch 838/1000 - train_loss: 0.6572 - train_accuracy: 0.7197 - val_loss: 1.0894 - val_accuracy: 0.5930\n",
      "Epoch 839/1000 - train_loss: 0.6570 - train_accuracy: 0.7210 - val_loss: 1.0897 - val_accuracy: 0.5930\n",
      "Epoch 840/1000 - train_loss: 0.6568 - train_accuracy: 0.7210 - val_loss: 1.0899 - val_accuracy: 0.5930\n",
      "Epoch 841/1000 - train_loss: 0.6566 - train_accuracy: 0.7210 - val_loss: 1.0902 - val_accuracy: 0.5930\n",
      "Epoch 842/1000 - train_loss: 0.6564 - train_accuracy: 0.7210 - val_loss: 1.0905 - val_accuracy: 0.5930\n",
      "Epoch 843/1000 - train_loss: 0.6562 - train_accuracy: 0.7210 - val_loss: 1.0908 - val_accuracy: 0.5930\n",
      "Epoch 844/1000 - train_loss: 0.6560 - train_accuracy: 0.7210 - val_loss: 1.0910 - val_accuracy: 0.5930\n",
      "Epoch 845/1000 - train_loss: 0.6557 - train_accuracy: 0.7197 - val_loss: 1.0913 - val_accuracy: 0.5930\n",
      "Epoch 846/1000 - train_loss: 0.6555 - train_accuracy: 0.7183 - val_loss: 1.0916 - val_accuracy: 0.5930\n",
      "Epoch 847/1000 - train_loss: 0.6553 - train_accuracy: 0.7183 - val_loss: 1.0919 - val_accuracy: 0.5872\n",
      "Epoch 848/1000 - train_loss: 0.6551 - train_accuracy: 0.7183 - val_loss: 1.0922 - val_accuracy: 0.5872\n",
      "Epoch 849/1000 - train_loss: 0.6549 - train_accuracy: 0.7183 - val_loss: 1.0925 - val_accuracy: 0.5872\n",
      "Epoch 850/1000 - train_loss: 0.6547 - train_accuracy: 0.7183 - val_loss: 1.0928 - val_accuracy: 0.5872\n",
      "Epoch 851/1000 - train_loss: 0.6545 - train_accuracy: 0.7183 - val_loss: 1.0931 - val_accuracy: 0.5872\n",
      "Epoch 852/1000 - train_loss: 0.6542 - train_accuracy: 0.7183 - val_loss: 1.0934 - val_accuracy: 0.5872\n",
      "Epoch 853/1000 - train_loss: 0.6540 - train_accuracy: 0.7183 - val_loss: 1.0937 - val_accuracy: 0.5872\n",
      "Epoch 854/1000 - train_loss: 0.6538 - train_accuracy: 0.7183 - val_loss: 1.0941 - val_accuracy: 0.5872\n",
      "Epoch 855/1000 - train_loss: 0.6536 - train_accuracy: 0.7183 - val_loss: 1.0944 - val_accuracy: 0.5872\n",
      "Epoch 856/1000 - train_loss: 0.6534 - train_accuracy: 0.7197 - val_loss: 1.0947 - val_accuracy: 0.5872\n",
      "Epoch 857/1000 - train_loss: 0.6532 - train_accuracy: 0.7210 - val_loss: 1.0950 - val_accuracy: 0.5872\n",
      "Epoch 858/1000 - train_loss: 0.6530 - train_accuracy: 0.7210 - val_loss: 1.0954 - val_accuracy: 0.5872\n",
      "Epoch 859/1000 - train_loss: 0.6528 - train_accuracy: 0.7224 - val_loss: 1.0957 - val_accuracy: 0.5872\n",
      "Epoch 860/1000 - train_loss: 0.6526 - train_accuracy: 0.7210 - val_loss: 1.0961 - val_accuracy: 0.5814\n",
      "Epoch 861/1000 - train_loss: 0.6524 - train_accuracy: 0.7210 - val_loss: 1.0964 - val_accuracy: 0.5814\n",
      "Epoch 862/1000 - train_loss: 0.6522 - train_accuracy: 0.7210 - val_loss: 1.0968 - val_accuracy: 0.5814\n",
      "Epoch 863/1000 - train_loss: 0.6519 - train_accuracy: 0.7210 - val_loss: 1.0971 - val_accuracy: 0.5814\n",
      "Epoch 864/1000 - train_loss: 0.6517 - train_accuracy: 0.7210 - val_loss: 1.0975 - val_accuracy: 0.5814\n",
      "Epoch 865/1000 - train_loss: 0.6515 - train_accuracy: 0.7210 - val_loss: 1.0978 - val_accuracy: 0.5814\n",
      "Epoch 866/1000 - train_loss: 0.6513 - train_accuracy: 0.7197 - val_loss: 1.0982 - val_accuracy: 0.5814\n",
      "Epoch 867/1000 - train_loss: 0.6511 - train_accuracy: 0.7210 - val_loss: 1.0986 - val_accuracy: 0.5756\n",
      "Epoch 868/1000 - train_loss: 0.6509 - train_accuracy: 0.7224 - val_loss: 1.0990 - val_accuracy: 0.5756\n",
      "Epoch 869/1000 - train_loss: 0.6507 - train_accuracy: 0.7224 - val_loss: 1.0993 - val_accuracy: 0.5756\n",
      "Epoch 870/1000 - train_loss: 0.6505 - train_accuracy: 0.7224 - val_loss: 1.0997 - val_accuracy: 0.5756\n",
      "Epoch 871/1000 - train_loss: 0.6503 - train_accuracy: 0.7224 - val_loss: 1.1001 - val_accuracy: 0.5756\n",
      "Epoch 872/1000 - train_loss: 0.6501 - train_accuracy: 0.7210 - val_loss: 1.1005 - val_accuracy: 0.5756\n",
      "Epoch 873/1000 - train_loss: 0.6499 - train_accuracy: 0.7197 - val_loss: 1.1009 - val_accuracy: 0.5756\n",
      "Epoch 874/1000 - train_loss: 0.6497 - train_accuracy: 0.7197 - val_loss: 1.1013 - val_accuracy: 0.5756\n",
      "Epoch 875/1000 - train_loss: 0.6495 - train_accuracy: 0.7197 - val_loss: 1.1017 - val_accuracy: 0.5756\n",
      "Epoch 876/1000 - train_loss: 0.6493 - train_accuracy: 0.7197 - val_loss: 1.1021 - val_accuracy: 0.5756\n",
      "Epoch 877/1000 - train_loss: 0.6491 - train_accuracy: 0.7197 - val_loss: 1.1025 - val_accuracy: 0.5756\n",
      "Epoch 878/1000 - train_loss: 0.6489 - train_accuracy: 0.7197 - val_loss: 1.1029 - val_accuracy: 0.5814\n",
      "Epoch 879/1000 - train_loss: 0.6487 - train_accuracy: 0.7197 - val_loss: 1.1033 - val_accuracy: 0.5814\n",
      "Epoch 880/1000 - train_loss: 0.6485 - train_accuracy: 0.7197 - val_loss: 1.1037 - val_accuracy: 0.5814\n",
      "Epoch 881/1000 - train_loss: 0.6483 - train_accuracy: 0.7197 - val_loss: 1.1042 - val_accuracy: 0.5814\n",
      "Epoch 882/1000 - train_loss: 0.6481 - train_accuracy: 0.7197 - val_loss: 1.1046 - val_accuracy: 0.5814\n",
      "Epoch 883/1000 - train_loss: 0.6479 - train_accuracy: 0.7183 - val_loss: 1.1050 - val_accuracy: 0.5814\n",
      "Epoch 884/1000 - train_loss: 0.6477 - train_accuracy: 0.7183 - val_loss: 1.1054 - val_accuracy: 0.5814\n",
      "Epoch 885/1000 - train_loss: 0.6475 - train_accuracy: 0.7183 - val_loss: 1.1058 - val_accuracy: 0.5756\n",
      "Epoch 886/1000 - train_loss: 0.6473 - train_accuracy: 0.7183 - val_loss: 1.1063 - val_accuracy: 0.5756\n",
      "Epoch 887/1000 - train_loss: 0.6471 - train_accuracy: 0.7183 - val_loss: 1.1067 - val_accuracy: 0.5756\n",
      "Epoch 888/1000 - train_loss: 0.6470 - train_accuracy: 0.7197 - val_loss: 1.1071 - val_accuracy: 0.5698\n",
      "Epoch 889/1000 - train_loss: 0.6468 - train_accuracy: 0.7210 - val_loss: 1.1075 - val_accuracy: 0.5698\n",
      "Epoch 890/1000 - train_loss: 0.6466 - train_accuracy: 0.7210 - val_loss: 1.1080 - val_accuracy: 0.5698\n",
      "Epoch 891/1000 - train_loss: 0.6464 - train_accuracy: 0.7210 - val_loss: 1.1084 - val_accuracy: 0.5698\n",
      "Epoch 892/1000 - train_loss: 0.6462 - train_accuracy: 0.7210 - val_loss: 1.1088 - val_accuracy: 0.5756\n",
      "Epoch 893/1000 - train_loss: 0.6460 - train_accuracy: 0.7210 - val_loss: 1.1092 - val_accuracy: 0.5756\n",
      "Epoch 894/1000 - train_loss: 0.6458 - train_accuracy: 0.7210 - val_loss: 1.1097 - val_accuracy: 0.5756\n",
      "Epoch 895/1000 - train_loss: 0.6456 - train_accuracy: 0.7224 - val_loss: 1.1101 - val_accuracy: 0.5756\n",
      "Epoch 896/1000 - train_loss: 0.6454 - train_accuracy: 0.7224 - val_loss: 1.1105 - val_accuracy: 0.5756\n",
      "Epoch 897/1000 - train_loss: 0.6452 - train_accuracy: 0.7224 - val_loss: 1.1109 - val_accuracy: 0.5756\n",
      "Epoch 898/1000 - train_loss: 0.6450 - train_accuracy: 0.7224 - val_loss: 1.1114 - val_accuracy: 0.5756\n",
      "Epoch 899/1000 - train_loss: 0.6448 - train_accuracy: 0.7224 - val_loss: 1.1118 - val_accuracy: 0.5756\n",
      "Epoch 900/1000 - train_loss: 0.6446 - train_accuracy: 0.7224 - val_loss: 1.1122 - val_accuracy: 0.5756\n",
      "Epoch 901/1000 - train_loss: 0.6445 - train_accuracy: 0.7224 - val_loss: 1.1126 - val_accuracy: 0.5756\n",
      "Epoch 902/1000 - train_loss: 0.6443 - train_accuracy: 0.7237 - val_loss: 1.1130 - val_accuracy: 0.5756\n",
      "Epoch 903/1000 - train_loss: 0.6441 - train_accuracy: 0.7237 - val_loss: 1.1134 - val_accuracy: 0.5756\n",
      "Epoch 904/1000 - train_loss: 0.6439 - train_accuracy: 0.7237 - val_loss: 1.1138 - val_accuracy: 0.5756\n",
      "Epoch 905/1000 - train_loss: 0.6437 - train_accuracy: 0.7237 - val_loss: 1.1142 - val_accuracy: 0.5756\n",
      "Epoch 906/1000 - train_loss: 0.6435 - train_accuracy: 0.7237 - val_loss: 1.1146 - val_accuracy: 0.5756\n",
      "Epoch 907/1000 - train_loss: 0.6433 - train_accuracy: 0.7237 - val_loss: 1.1150 - val_accuracy: 0.5756\n",
      "Epoch 908/1000 - train_loss: 0.6431 - train_accuracy: 0.7237 - val_loss: 1.1154 - val_accuracy: 0.5756\n",
      "Epoch 909/1000 - train_loss: 0.6429 - train_accuracy: 0.7251 - val_loss: 1.1157 - val_accuracy: 0.5756\n",
      "Epoch 910/1000 - train_loss: 0.6427 - train_accuracy: 0.7264 - val_loss: 1.1161 - val_accuracy: 0.5756\n",
      "Epoch 911/1000 - train_loss: 0.6425 - train_accuracy: 0.7264 - val_loss: 1.1165 - val_accuracy: 0.5756\n",
      "Epoch 912/1000 - train_loss: 0.6424 - train_accuracy: 0.7278 - val_loss: 1.1168 - val_accuracy: 0.5756\n",
      "Epoch 913/1000 - train_loss: 0.6422 - train_accuracy: 0.7278 - val_loss: 1.1172 - val_accuracy: 0.5756\n",
      "Epoch 914/1000 - train_loss: 0.6420 - train_accuracy: 0.7278 - val_loss: 1.1176 - val_accuracy: 0.5756\n",
      "Epoch 915/1000 - train_loss: 0.6418 - train_accuracy: 0.7264 - val_loss: 1.1179 - val_accuracy: 0.5756\n",
      "Epoch 916/1000 - train_loss: 0.6416 - train_accuracy: 0.7264 - val_loss: 1.1183 - val_accuracy: 0.5756\n",
      "Epoch 917/1000 - train_loss: 0.6414 - train_accuracy: 0.7264 - val_loss: 1.1186 - val_accuracy: 0.5756\n",
      "Epoch 918/1000 - train_loss: 0.6412 - train_accuracy: 0.7278 - val_loss: 1.1189 - val_accuracy: 0.5756\n",
      "Epoch 919/1000 - train_loss: 0.6410 - train_accuracy: 0.7278 - val_loss: 1.1193 - val_accuracy: 0.5756\n",
      "Epoch 920/1000 - train_loss: 0.6408 - train_accuracy: 0.7278 - val_loss: 1.1196 - val_accuracy: 0.5756\n",
      "Epoch 921/1000 - train_loss: 0.6406 - train_accuracy: 0.7278 - val_loss: 1.1199 - val_accuracy: 0.5756\n",
      "Epoch 922/1000 - train_loss: 0.6405 - train_accuracy: 0.7278 - val_loss: 1.1203 - val_accuracy: 0.5756\n",
      "Epoch 923/1000 - train_loss: 0.6403 - train_accuracy: 0.7278 - val_loss: 1.1206 - val_accuracy: 0.5756\n",
      "Epoch 924/1000 - train_loss: 0.6401 - train_accuracy: 0.7278 - val_loss: 1.1209 - val_accuracy: 0.5756\n",
      "Epoch 925/1000 - train_loss: 0.6399 - train_accuracy: 0.7264 - val_loss: 1.1212 - val_accuracy: 0.5756\n",
      "Epoch 926/1000 - train_loss: 0.6397 - train_accuracy: 0.7264 - val_loss: 1.1215 - val_accuracy: 0.5756\n",
      "Epoch 927/1000 - train_loss: 0.6395 - train_accuracy: 0.7264 - val_loss: 1.1219 - val_accuracy: 0.5756\n",
      "Epoch 928/1000 - train_loss: 0.6393 - train_accuracy: 0.7264 - val_loss: 1.1222 - val_accuracy: 0.5756\n",
      "Epoch 929/1000 - train_loss: 0.6392 - train_accuracy: 0.7264 - val_loss: 1.1225 - val_accuracy: 0.5756\n",
      "Epoch 930/1000 - train_loss: 0.6390 - train_accuracy: 0.7278 - val_loss: 1.1228 - val_accuracy: 0.5756\n",
      "Epoch 931/1000 - train_loss: 0.6388 - train_accuracy: 0.7291 - val_loss: 1.1232 - val_accuracy: 0.5698\n",
      "Epoch 932/1000 - train_loss: 0.6386 - train_accuracy: 0.7291 - val_loss: 1.1235 - val_accuracy: 0.5698\n",
      "Epoch 933/1000 - train_loss: 0.6384 - train_accuracy: 0.7291 - val_loss: 1.1238 - val_accuracy: 0.5698\n",
      "Epoch 934/1000 - train_loss: 0.6382 - train_accuracy: 0.7291 - val_loss: 1.1241 - val_accuracy: 0.5640\n",
      "Epoch 935/1000 - train_loss: 0.6381 - train_accuracy: 0.7291 - val_loss: 1.1244 - val_accuracy: 0.5640\n",
      "Epoch 936/1000 - train_loss: 0.6379 - train_accuracy: 0.7291 - val_loss: 1.1248 - val_accuracy: 0.5640\n",
      "Epoch 937/1000 - train_loss: 0.6377 - train_accuracy: 0.7278 - val_loss: 1.1251 - val_accuracy: 0.5640\n",
      "Epoch 938/1000 - train_loss: 0.6375 - train_accuracy: 0.7291 - val_loss: 1.1254 - val_accuracy: 0.5640\n",
      "Epoch 939/1000 - train_loss: 0.6374 - train_accuracy: 0.7291 - val_loss: 1.1257 - val_accuracy: 0.5640\n",
      "Epoch 940/1000 - train_loss: 0.6372 - train_accuracy: 0.7291 - val_loss: 1.1261 - val_accuracy: 0.5640\n",
      "Epoch 941/1000 - train_loss: 0.6370 - train_accuracy: 0.7291 - val_loss: 1.1264 - val_accuracy: 0.5640\n",
      "Epoch 942/1000 - train_loss: 0.6368 - train_accuracy: 0.7278 - val_loss: 1.1267 - val_accuracy: 0.5640\n",
      "Epoch 943/1000 - train_loss: 0.6367 - train_accuracy: 0.7278 - val_loss: 1.1271 - val_accuracy: 0.5640\n",
      "Epoch 944/1000 - train_loss: 0.6365 - train_accuracy: 0.7278 - val_loss: 1.1274 - val_accuracy: 0.5640\n",
      "Epoch 945/1000 - train_loss: 0.6363 - train_accuracy: 0.7278 - val_loss: 1.1277 - val_accuracy: 0.5640\n",
      "Epoch 946/1000 - train_loss: 0.6361 - train_accuracy: 0.7278 - val_loss: 1.1281 - val_accuracy: 0.5640\n",
      "Epoch 947/1000 - train_loss: 0.6360 - train_accuracy: 0.7278 - val_loss: 1.1284 - val_accuracy: 0.5640\n",
      "Epoch 948/1000 - train_loss: 0.6358 - train_accuracy: 0.7278 - val_loss: 1.1288 - val_accuracy: 0.5581\n",
      "Epoch 949/1000 - train_loss: 0.6356 - train_accuracy: 0.7278 - val_loss: 1.1291 - val_accuracy: 0.5465\n",
      "Epoch 950/1000 - train_loss: 0.6355 - train_accuracy: 0.7278 - val_loss: 1.1294 - val_accuracy: 0.5407\n",
      "Epoch 951/1000 - train_loss: 0.6353 - train_accuracy: 0.7278 - val_loss: 1.1298 - val_accuracy: 0.5407\n",
      "Epoch 952/1000 - train_loss: 0.6351 - train_accuracy: 0.7278 - val_loss: 1.1301 - val_accuracy: 0.5407\n",
      "Epoch 953/1000 - train_loss: 0.6350 - train_accuracy: 0.7278 - val_loss: 1.1305 - val_accuracy: 0.5407\n",
      "Epoch 954/1000 - train_loss: 0.6348 - train_accuracy: 0.7278 - val_loss: 1.1308 - val_accuracy: 0.5407\n",
      "Epoch 955/1000 - train_loss: 0.6346 - train_accuracy: 0.7278 - val_loss: 1.1312 - val_accuracy: 0.5407\n",
      "Epoch 956/1000 - train_loss: 0.6345 - train_accuracy: 0.7278 - val_loss: 1.1315 - val_accuracy: 0.5465\n",
      "Epoch 957/1000 - train_loss: 0.6343 - train_accuracy: 0.7278 - val_loss: 1.1319 - val_accuracy: 0.5465\n",
      "Epoch 958/1000 - train_loss: 0.6341 - train_accuracy: 0.7278 - val_loss: 1.1322 - val_accuracy: 0.5465\n",
      "Epoch 959/1000 - train_loss: 0.6340 - train_accuracy: 0.7278 - val_loss: 1.1326 - val_accuracy: 0.5465\n",
      "Epoch 960/1000 - train_loss: 0.6338 - train_accuracy: 0.7291 - val_loss: 1.1329 - val_accuracy: 0.5465\n",
      "Epoch 961/1000 - train_loss: 0.6337 - train_accuracy: 0.7291 - val_loss: 1.1333 - val_accuracy: 0.5465\n",
      "Epoch 962/1000 - train_loss: 0.6335 - train_accuracy: 0.7291 - val_loss: 1.1336 - val_accuracy: 0.5465\n",
      "Epoch 963/1000 - train_loss: 0.6333 - train_accuracy: 0.7291 - val_loss: 1.1340 - val_accuracy: 0.5465\n",
      "Epoch 964/1000 - train_loss: 0.6332 - train_accuracy: 0.7305 - val_loss: 1.1343 - val_accuracy: 0.5465\n",
      "Epoch 965/1000 - train_loss: 0.6330 - train_accuracy: 0.7318 - val_loss: 1.1347 - val_accuracy: 0.5465\n",
      "Epoch 966/1000 - train_loss: 0.6328 - train_accuracy: 0.7318 - val_loss: 1.1351 - val_accuracy: 0.5465\n",
      "Epoch 967/1000 - train_loss: 0.6327 - train_accuracy: 0.7318 - val_loss: 1.1354 - val_accuracy: 0.5465\n",
      "Epoch 968/1000 - train_loss: 0.6325 - train_accuracy: 0.7318 - val_loss: 1.1358 - val_accuracy: 0.5465\n",
      "Epoch 969/1000 - train_loss: 0.6324 - train_accuracy: 0.7318 - val_loss: 1.1361 - val_accuracy: 0.5407\n",
      "Epoch 970/1000 - train_loss: 0.6322 - train_accuracy: 0.7345 - val_loss: 1.1365 - val_accuracy: 0.5465\n",
      "Epoch 971/1000 - train_loss: 0.6321 - train_accuracy: 0.7345 - val_loss: 1.1369 - val_accuracy: 0.5465\n",
      "Epoch 972/1000 - train_loss: 0.6319 - train_accuracy: 0.7332 - val_loss: 1.1372 - val_accuracy: 0.5465\n",
      "Epoch 973/1000 - train_loss: 0.6317 - train_accuracy: 0.7332 - val_loss: 1.1376 - val_accuracy: 0.5465\n",
      "Epoch 974/1000 - train_loss: 0.6316 - train_accuracy: 0.7332 - val_loss: 1.1380 - val_accuracy: 0.5465\n",
      "Epoch 975/1000 - train_loss: 0.6314 - train_accuracy: 0.7332 - val_loss: 1.1383 - val_accuracy: 0.5465\n",
      "Epoch 976/1000 - train_loss: 0.6313 - train_accuracy: 0.7332 - val_loss: 1.1387 - val_accuracy: 0.5465\n",
      "Epoch 977/1000 - train_loss: 0.6311 - train_accuracy: 0.7332 - val_loss: 1.1391 - val_accuracy: 0.5465\n",
      "Epoch 978/1000 - train_loss: 0.6310 - train_accuracy: 0.7332 - val_loss: 1.1394 - val_accuracy: 0.5465\n",
      "Epoch 979/1000 - train_loss: 0.6308 - train_accuracy: 0.7332 - val_loss: 1.1398 - val_accuracy: 0.5465\n",
      "Epoch 980/1000 - train_loss: 0.6306 - train_accuracy: 0.7332 - val_loss: 1.1402 - val_accuracy: 0.5465\n",
      "Epoch 981/1000 - train_loss: 0.6305 - train_accuracy: 0.7332 - val_loss: 1.1405 - val_accuracy: 0.5465\n",
      "Epoch 982/1000 - train_loss: 0.6303 - train_accuracy: 0.7332 - val_loss: 1.1409 - val_accuracy: 0.5523\n",
      "Epoch 983/1000 - train_loss: 0.6302 - train_accuracy: 0.7345 - val_loss: 1.1413 - val_accuracy: 0.5523\n",
      "Epoch 984/1000 - train_loss: 0.6300 - train_accuracy: 0.7358 - val_loss: 1.1416 - val_accuracy: 0.5523\n",
      "Epoch 985/1000 - train_loss: 0.6299 - train_accuracy: 0.7358 - val_loss: 1.1420 - val_accuracy: 0.5523\n",
      "Epoch 986/1000 - train_loss: 0.6297 - train_accuracy: 0.7372 - val_loss: 1.1424 - val_accuracy: 0.5581\n",
      "Epoch 987/1000 - train_loss: 0.6296 - train_accuracy: 0.7372 - val_loss: 1.1428 - val_accuracy: 0.5581\n",
      "Epoch 988/1000 - train_loss: 0.6294 - train_accuracy: 0.7372 - val_loss: 1.1431 - val_accuracy: 0.5523\n",
      "Epoch 989/1000 - train_loss: 0.6293 - train_accuracy: 0.7372 - val_loss: 1.1435 - val_accuracy: 0.5523\n",
      "Epoch 990/1000 - train_loss: 0.6291 - train_accuracy: 0.7372 - val_loss: 1.1439 - val_accuracy: 0.5523\n",
      "Epoch 991/1000 - train_loss: 0.6289 - train_accuracy: 0.7372 - val_loss: 1.1442 - val_accuracy: 0.5523\n",
      "Epoch 992/1000 - train_loss: 0.6288 - train_accuracy: 0.7372 - val_loss: 1.1446 - val_accuracy: 0.5523\n",
      "Epoch 993/1000 - train_loss: 0.6286 - train_accuracy: 0.7358 - val_loss: 1.1450 - val_accuracy: 0.5523\n",
      "Epoch 994/1000 - train_loss: 0.6285 - train_accuracy: 0.7358 - val_loss: 1.1454 - val_accuracy: 0.5523\n",
      "Epoch 995/1000 - train_loss: 0.6283 - train_accuracy: 0.7358 - val_loss: 1.1458 - val_accuracy: 0.5523\n",
      "Epoch 996/1000 - train_loss: 0.6282 - train_accuracy: 0.7358 - val_loss: 1.1461 - val_accuracy: 0.5523\n",
      "Epoch 997/1000 - train_loss: 0.6280 - train_accuracy: 0.7358 - val_loss: 1.1465 - val_accuracy: 0.5523\n",
      "Epoch 998/1000 - train_loss: 0.6279 - train_accuracy: 0.7358 - val_loss: 1.1469 - val_accuracy: 0.5523\n",
      "Epoch 999/1000 - train_loss: 0.6277 - train_accuracy: 0.7358 - val_loss: 1.1473 - val_accuracy: 0.5523\n",
      "Epoch 1000/1000 - train_loss: 0.6276 - train_accuracy: 0.7358 - val_loss: 1.1476 - val_accuracy: 0.5523\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(learning_rate=0.01, activation='sigmoid', optimizer='sgd', hidden_layers=[8, 6], epochs=1000)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(train_X,y_true,val_X,y_true_val)\n",
    "\n",
    "# Make predictions\n",
    "# y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1118bbb3",
   "metadata": {},
   "source": [
    "<h3>Classification report and Accuracy on Test Set</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1271e715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.33      0.17      0.22         6\n",
      "           2       0.69      0.71      0.70       102\n",
      "           3       0.49      0.59      0.53        85\n",
      "           4       0.53      0.32      0.40        31\n",
      "           5       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.58       229\n",
      "   macro avg       0.34      0.30      0.31       229\n",
      "weighted avg       0.57      0.58      0.57       229\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/rohan/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test_y = test_y.reshape((-1,1))\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_true_test = one_hot_encoder.fit_transform(test_y)\n",
    "\n",
    "print(mlp.inference(test_X,y_true_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebf9342e",
   "metadata": {},
   "source": [
    "### WandB "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f96aaedc",
   "metadata": {},
   "source": [
    "### Combinations of activation function and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8553822f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrohan-victorious108\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e547c5729ce4395a94f2cecf1c779e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124533-hw45sv9i</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/mlp-new-2/runs/hw45sv9i' target=\"_blank\">stoic-resonance-2</a></strong> to <a href='https://wandb.ai/rohan-victorious108/mlp-new-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/mlp-new-2' target=\"_blank\">https://wandb.ai/rohan-victorious108/mlp-new-2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/mlp-new-2/runs/hw45sv9i' target=\"_blank\">https://wandb.ai/rohan-victorious108/mlp-new-2/runs/hw45sv9i</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hw45sv9i) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421e1cc2139345e3b49a2aa13f6f450f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stoic-resonance-2</strong> at: <a href='https://wandb.ai/rohan-victorious108/mlp-new-2/runs/hw45sv9i' target=\"_blank\">https://wandb.ai/rohan-victorious108/mlp-new-2/runs/hw45sv9i</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124533-hw45sv9i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hw45sv9i). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5deacf1cdf0d4a37b240552834cc5dfd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124537-7qh1bszf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/7qh1bszf' target=\"_blank\">lr=0.001_act=sigmoid_optim=sgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/7qh1bszf' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/7qh1bszf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.5358 - train_accuracy: 0.4111 - val_loss: 1.5242 - val_accuracy: 0.4419\n",
      "Epoch 2/100 - train_loss: 1.3962 - train_accuracy: 0.4111 - val_loss: 1.3767 - val_accuracy: 0.4419\n",
      "Epoch 3/100 - train_loss: 1.3215 - train_accuracy: 0.4111 - val_loss: 1.2966 - val_accuracy: 0.4419\n",
      "Epoch 4/100 - train_loss: 1.2793 - train_accuracy: 0.4111 - val_loss: 1.2508 - val_accuracy: 0.4419\n",
      "Epoch 5/100 - train_loss: 1.2538 - train_accuracy: 0.4137 - val_loss: 1.2228 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.2373 - train_accuracy: 0.4137 - val_loss: 1.2048 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.2259 - train_accuracy: 0.4137 - val_loss: 1.1926 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.2178 - train_accuracy: 0.4137 - val_loss: 1.1839 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.2116 - train_accuracy: 0.4137 - val_loss: 1.1775 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.2068 - train_accuracy: 0.4137 - val_loss: 1.1728 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.2030 - train_accuracy: 0.4137 - val_loss: 1.1691 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.2000 - train_accuracy: 0.4137 - val_loss: 1.1663 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.1974 - train_accuracy: 0.4137 - val_loss: 1.1641 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.1953 - train_accuracy: 0.4137 - val_loss: 1.1624 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.1936 - train_accuracy: 0.4137 - val_loss: 1.1610 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.1920 - train_accuracy: 0.4137 - val_loss: 1.1599 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.1908 - train_accuracy: 0.4137 - val_loss: 1.1591 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.1896 - train_accuracy: 0.4137 - val_loss: 1.1584 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.1887 - train_accuracy: 0.4137 - val_loss: 1.1579 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.1878 - train_accuracy: 0.4137 - val_loss: 1.1575 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.1871 - train_accuracy: 0.4137 - val_loss: 1.1572 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.1864 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.1858 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.1853 - train_accuracy: 0.4137 - val_loss: 1.1567 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.1848 - train_accuracy: 0.4137 - val_loss: 1.1567 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.1844 - train_accuracy: 0.4137 - val_loss: 1.1567 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.1840 - train_accuracy: 0.4137 - val_loss: 1.1567 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.1836 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.1832 - train_accuracy: 0.4137 - val_loss: 1.1569 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.1829 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.1826 - train_accuracy: 0.4137 - val_loss: 1.1571 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.1824 - train_accuracy: 0.4137 - val_loss: 1.1572 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.1821 - train_accuracy: 0.4137 - val_loss: 1.1574 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.1819 - train_accuracy: 0.4137 - val_loss: 1.1575 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.1817 - train_accuracy: 0.4137 - val_loss: 1.1577 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.1815 - train_accuracy: 0.4137 - val_loss: 1.1579 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.1813 - train_accuracy: 0.4137 - val_loss: 1.1580 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.1811 - train_accuracy: 0.4137 - val_loss: 1.1582 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.1809 - train_accuracy: 0.4137 - val_loss: 1.1584 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.1807 - train_accuracy: 0.4137 - val_loss: 1.1586 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.1806 - train_accuracy: 0.4137 - val_loss: 1.1588 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.1804 - train_accuracy: 0.4137 - val_loss: 1.1590 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.1803 - train_accuracy: 0.4137 - val_loss: 1.1592 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.1802 - train_accuracy: 0.4137 - val_loss: 1.1594 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.1800 - train_accuracy: 0.4137 - val_loss: 1.1596 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.1799 - train_accuracy: 0.4137 - val_loss: 1.1598 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.1798 - train_accuracy: 0.4137 - val_loss: 1.1599 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.1797 - train_accuracy: 0.4137 - val_loss: 1.1601 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.1796 - train_accuracy: 0.4137 - val_loss: 1.1603 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.1795 - train_accuracy: 0.4137 - val_loss: 1.1605 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.1794 - train_accuracy: 0.4137 - val_loss: 1.1607 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.1793 - train_accuracy: 0.4137 - val_loss: 1.1609 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.1792 - train_accuracy: 0.4137 - val_loss: 1.1611 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.1791 - train_accuracy: 0.4137 - val_loss: 1.1613 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.1790 - train_accuracy: 0.4137 - val_loss: 1.1615 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.1789 - train_accuracy: 0.4137 - val_loss: 1.1616 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.1788 - train_accuracy: 0.4137 - val_loss: 1.1618 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.1788 - train_accuracy: 0.4137 - val_loss: 1.1620 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.1787 - train_accuracy: 0.4137 - val_loss: 1.1622 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.1786 - train_accuracy: 0.4137 - val_loss: 1.1623 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.1785 - train_accuracy: 0.4205 - val_loss: 1.1625 - val_accuracy: 0.4128\n",
      "Epoch 62/100 - train_loss: 1.1785 - train_accuracy: 0.4730 - val_loss: 1.1627 - val_accuracy: 0.4709\n",
      "Epoch 63/100 - train_loss: 1.1784 - train_accuracy: 0.5512 - val_loss: 1.1629 - val_accuracy: 0.5349\n",
      "Epoch 64/100 - train_loss: 1.1783 - train_accuracy: 0.5687 - val_loss: 1.1630 - val_accuracy: 0.5291\n",
      "Epoch 65/100 - train_loss: 1.1783 - train_accuracy: 0.5580 - val_loss: 1.1632 - val_accuracy: 0.5407\n",
      "Epoch 66/100 - train_loss: 1.1782 - train_accuracy: 0.5202 - val_loss: 1.1633 - val_accuracy: 0.5581\n",
      "Epoch 67/100 - train_loss: 1.1781 - train_accuracy: 0.4838 - val_loss: 1.1635 - val_accuracy: 0.5233\n",
      "Epoch 68/100 - train_loss: 1.1781 - train_accuracy: 0.4596 - val_loss: 1.1637 - val_accuracy: 0.4767\n",
      "Epoch 69/100 - train_loss: 1.1780 - train_accuracy: 0.4407 - val_loss: 1.1638 - val_accuracy: 0.4709\n",
      "Epoch 70/100 - train_loss: 1.1780 - train_accuracy: 0.4259 - val_loss: 1.1640 - val_accuracy: 0.4593\n",
      "Epoch 71/100 - train_loss: 1.1779 - train_accuracy: 0.4137 - val_loss: 1.1641 - val_accuracy: 0.4477\n",
      "Epoch 72/100 - train_loss: 1.1779 - train_accuracy: 0.4124 - val_loss: 1.1643 - val_accuracy: 0.4419\n",
      "Epoch 73/100 - train_loss: 1.1778 - train_accuracy: 0.4111 - val_loss: 1.1644 - val_accuracy: 0.4419\n",
      "Epoch 74/100 - train_loss: 1.1777 - train_accuracy: 0.4097 - val_loss: 1.1646 - val_accuracy: 0.4419\n",
      "Epoch 75/100 - train_loss: 1.1777 - train_accuracy: 0.4097 - val_loss: 1.1647 - val_accuracy: 0.4419\n",
      "Epoch 76/100 - train_loss: 1.1776 - train_accuracy: 0.4097 - val_loss: 1.1648 - val_accuracy: 0.4419\n",
      "Epoch 77/100 - train_loss: 1.1776 - train_accuracy: 0.4097 - val_loss: 1.1650 - val_accuracy: 0.4419\n",
      "Epoch 78/100 - train_loss: 1.1775 - train_accuracy: 0.4111 - val_loss: 1.1651 - val_accuracy: 0.4419\n",
      "Epoch 79/100 - train_loss: 1.1775 - train_accuracy: 0.4111 - val_loss: 1.1653 - val_accuracy: 0.4419\n",
      "Epoch 80/100 - train_loss: 1.1774 - train_accuracy: 0.4111 - val_loss: 1.1654 - val_accuracy: 0.4419\n",
      "Epoch 81/100 - train_loss: 1.1774 - train_accuracy: 0.4111 - val_loss: 1.1655 - val_accuracy: 0.4419\n",
      "Epoch 82/100 - train_loss: 1.1774 - train_accuracy: 0.4111 - val_loss: 1.1656 - val_accuracy: 0.4419\n",
      "Epoch 83/100 - train_loss: 1.1773 - train_accuracy: 0.4111 - val_loss: 1.1658 - val_accuracy: 0.4419\n",
      "Epoch 84/100 - train_loss: 1.1773 - train_accuracy: 0.4111 - val_loss: 1.1659 - val_accuracy: 0.4419\n",
      "Epoch 85/100 - train_loss: 1.1772 - train_accuracy: 0.4111 - val_loss: 1.1660 - val_accuracy: 0.4419\n",
      "Epoch 86/100 - train_loss: 1.1772 - train_accuracy: 0.4111 - val_loss: 1.1662 - val_accuracy: 0.4419\n",
      "Epoch 87/100 - train_loss: 1.1771 - train_accuracy: 0.4111 - val_loss: 1.1663 - val_accuracy: 0.4419\n",
      "Epoch 88/100 - train_loss: 1.1771 - train_accuracy: 0.4111 - val_loss: 1.1664 - val_accuracy: 0.4419\n",
      "Epoch 89/100 - train_loss: 1.1771 - train_accuracy: 0.4111 - val_loss: 1.1665 - val_accuracy: 0.4419\n",
      "Epoch 90/100 - train_loss: 1.1770 - train_accuracy: 0.4111 - val_loss: 1.1666 - val_accuracy: 0.4419\n",
      "Epoch 91/100 - train_loss: 1.1770 - train_accuracy: 0.4111 - val_loss: 1.1668 - val_accuracy: 0.4419\n",
      "Epoch 92/100 - train_loss: 1.1769 - train_accuracy: 0.4111 - val_loss: 1.1669 - val_accuracy: 0.4419\n",
      "Epoch 93/100 - train_loss: 1.1769 - train_accuracy: 0.4111 - val_loss: 1.1670 - val_accuracy: 0.4419\n",
      "Epoch 94/100 - train_loss: 1.1769 - train_accuracy: 0.4111 - val_loss: 1.1671 - val_accuracy: 0.4419\n",
      "Epoch 95/100 - train_loss: 1.1768 - train_accuracy: 0.4111 - val_loss: 1.1672 - val_accuracy: 0.4419\n",
      "Epoch 96/100 - train_loss: 1.1768 - train_accuracy: 0.4111 - val_loss: 1.1673 - val_accuracy: 0.4419\n",
      "Epoch 97/100 - train_loss: 1.1767 - train_accuracy: 0.4111 - val_loss: 1.1674 - val_accuracy: 0.4419\n",
      "Epoch 98/100 - train_loss: 1.1767 - train_accuracy: 0.4111 - val_loss: 1.1675 - val_accuracy: 0.4419\n",
      "Epoch 99/100 - train_loss: 1.1767 - train_accuracy: 0.4111 - val_loss: 1.1676 - val_accuracy: 0.4419\n",
      "Epoch 100/100 - train_loss: 1.1766 - train_accuracy: 0.4111 - val_loss: 1.1677 - val_accuracy: 0.4419\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d86d79729454cde88a814f1667da331"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.001_act=sigmoid_optim=sgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/7qh1bszf' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/7qh1bszf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124537-7qh1bszf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b473d43c65704283b176fbe2c2fbe073"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124600-js7mrqgu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/js7mrqgu' target=\"_blank\">lr=0.001_act=sigmoid_optim=bgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/js7mrqgu' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/js7mrqgu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7873 - train_accuracy: 0.0013 - val_loss: 1.7869 - val_accuracy: 0.0174\n",
      "Epoch 2/100 - train_loss: 1.7868 - train_accuracy: 0.4137 - val_loss: 1.7864 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.7863 - train_accuracy: 0.4137 - val_loss: 1.7859 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.7859 - train_accuracy: 0.4137 - val_loss: 1.7854 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.7854 - train_accuracy: 0.4137 - val_loss: 1.7850 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.7849 - train_accuracy: 0.4137 - val_loss: 1.7845 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.7844 - train_accuracy: 0.4137 - val_loss: 1.7840 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.7840 - train_accuracy: 0.4137 - val_loss: 1.7835 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.7835 - train_accuracy: 0.4137 - val_loss: 1.7830 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.7830 - train_accuracy: 0.4137 - val_loss: 1.7825 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.7826 - train_accuracy: 0.4137 - val_loss: 1.7820 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.7821 - train_accuracy: 0.4137 - val_loss: 1.7815 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.7816 - train_accuracy: 0.4137 - val_loss: 1.7810 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.7811 - train_accuracy: 0.4137 - val_loss: 1.7805 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.7807 - train_accuracy: 0.4137 - val_loss: 1.7801 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.7802 - train_accuracy: 0.4137 - val_loss: 1.7796 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.7797 - train_accuracy: 0.4137 - val_loss: 1.7791 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.7793 - train_accuracy: 0.4137 - val_loss: 1.7786 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.7788 - train_accuracy: 0.4137 - val_loss: 1.7781 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.7783 - train_accuracy: 0.4137 - val_loss: 1.7776 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.7779 - train_accuracy: 0.4137 - val_loss: 1.7771 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.7774 - train_accuracy: 0.4137 - val_loss: 1.7766 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.7769 - train_accuracy: 0.4137 - val_loss: 1.7762 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.7765 - train_accuracy: 0.4137 - val_loss: 1.7757 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.7760 - train_accuracy: 0.4137 - val_loss: 1.7752 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.7755 - train_accuracy: 0.4137 - val_loss: 1.7747 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.7751 - train_accuracy: 0.4137 - val_loss: 1.7742 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.7746 - train_accuracy: 0.4137 - val_loss: 1.7737 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.7741 - train_accuracy: 0.4137 - val_loss: 1.7733 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.7737 - train_accuracy: 0.4137 - val_loss: 1.7728 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.7732 - train_accuracy: 0.4137 - val_loss: 1.7723 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.7727 - train_accuracy: 0.4137 - val_loss: 1.7718 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.7723 - train_accuracy: 0.4137 - val_loss: 1.7713 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.7718 - train_accuracy: 0.4137 - val_loss: 1.7708 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.7713 - train_accuracy: 0.4137 - val_loss: 1.7704 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.7709 - train_accuracy: 0.4137 - val_loss: 1.7699 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.7704 - train_accuracy: 0.4137 - val_loss: 1.7694 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.7700 - train_accuracy: 0.4137 - val_loss: 1.7689 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.7695 - train_accuracy: 0.4137 - val_loss: 1.7685 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.7690 - train_accuracy: 0.4137 - val_loss: 1.7680 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.7686 - train_accuracy: 0.4137 - val_loss: 1.7675 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.7681 - train_accuracy: 0.4137 - val_loss: 1.7670 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.7677 - train_accuracy: 0.4137 - val_loss: 1.7665 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.7672 - train_accuracy: 0.4137 - val_loss: 1.7661 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.7667 - train_accuracy: 0.4137 - val_loss: 1.7656 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.7663 - train_accuracy: 0.4137 - val_loss: 1.7651 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.7658 - train_accuracy: 0.4137 - val_loss: 1.7646 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.7654 - train_accuracy: 0.4137 - val_loss: 1.7642 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.7649 - train_accuracy: 0.4137 - val_loss: 1.7637 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.7645 - train_accuracy: 0.4137 - val_loss: 1.7632 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.7640 - train_accuracy: 0.4137 - val_loss: 1.7627 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.7635 - train_accuracy: 0.4137 - val_loss: 1.7623 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.7631 - train_accuracy: 0.4137 - val_loss: 1.7618 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.7626 - train_accuracy: 0.4137 - val_loss: 1.7613 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.7622 - train_accuracy: 0.4137 - val_loss: 1.7608 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.7617 - train_accuracy: 0.4137 - val_loss: 1.7604 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.7613 - train_accuracy: 0.4137 - val_loss: 1.7599 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.7608 - train_accuracy: 0.4137 - val_loss: 1.7594 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.7604 - train_accuracy: 0.4137 - val_loss: 1.7590 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.7599 - train_accuracy: 0.4137 - val_loss: 1.7585 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.7595 - train_accuracy: 0.4137 - val_loss: 1.7580 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.7590 - train_accuracy: 0.4137 - val_loss: 1.7576 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.7586 - train_accuracy: 0.4137 - val_loss: 1.7571 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.7581 - train_accuracy: 0.4137 - val_loss: 1.7566 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.7577 - train_accuracy: 0.4137 - val_loss: 1.7561 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.7572 - train_accuracy: 0.4137 - val_loss: 1.7557 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.7567 - train_accuracy: 0.4137 - val_loss: 1.7552 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.7563 - train_accuracy: 0.4137 - val_loss: 1.7547 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.7559 - train_accuracy: 0.4137 - val_loss: 1.7543 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.7554 - train_accuracy: 0.4137 - val_loss: 1.7538 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.7550 - train_accuracy: 0.4137 - val_loss: 1.7533 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.7545 - train_accuracy: 0.4137 - val_loss: 1.7529 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.7541 - train_accuracy: 0.4137 - val_loss: 1.7524 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.7536 - train_accuracy: 0.4137 - val_loss: 1.7519 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.7532 - train_accuracy: 0.4137 - val_loss: 1.7515 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.7527 - train_accuracy: 0.4137 - val_loss: 1.7510 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.7523 - train_accuracy: 0.4137 - val_loss: 1.7506 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.7518 - train_accuracy: 0.4137 - val_loss: 1.7501 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.7514 - train_accuracy: 0.4137 - val_loss: 1.7496 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.7509 - train_accuracy: 0.4137 - val_loss: 1.7492 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.7505 - train_accuracy: 0.4137 - val_loss: 1.7487 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.7500 - train_accuracy: 0.4137 - val_loss: 1.7482 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.7496 - train_accuracy: 0.4137 - val_loss: 1.7478 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.7492 - train_accuracy: 0.4137 - val_loss: 1.7473 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.7487 - train_accuracy: 0.4137 - val_loss: 1.7469 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.7483 - train_accuracy: 0.4137 - val_loss: 1.7464 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.7478 - train_accuracy: 0.4137 - val_loss: 1.7459 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.7474 - train_accuracy: 0.4137 - val_loss: 1.7455 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.7469 - train_accuracy: 0.4137 - val_loss: 1.7450 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.7465 - train_accuracy: 0.4137 - val_loss: 1.7446 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.7461 - train_accuracy: 0.4137 - val_loss: 1.7441 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.7456 - train_accuracy: 0.4137 - val_loss: 1.7436 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.7452 - train_accuracy: 0.4137 - val_loss: 1.7432 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.7447 - train_accuracy: 0.4137 - val_loss: 1.7427 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.7443 - train_accuracy: 0.4137 - val_loss: 1.7423 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.7439 - train_accuracy: 0.4137 - val_loss: 1.7418 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.7434 - train_accuracy: 0.4137 - val_loss: 1.7414 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.7430 - train_accuracy: 0.4137 - val_loss: 1.7409 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.7425 - train_accuracy: 0.4137 - val_loss: 1.7404 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.7421 - train_accuracy: 0.4137 - val_loss: 1.7400 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f4e98d9ef04daa85c69ade5d342e75"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.001_act=sigmoid_optim=bgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/js7mrqgu' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/js7mrqgu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124600-js7mrqgu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0906a5d8ebda43cb9b12fe567507c834"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124613-i7pydxu3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/i7pydxu3' target=\"_blank\">lr=0.001_act=sigmoid_optim=mbgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/i7pydxu3' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/i7pydxu3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7805 - train_accuracy: 0.4137 - val_loss: 1.7805 - val_accuracy: 0.4070\n",
      "Epoch 2/100 - train_loss: 1.7693 - train_accuracy: 0.4137 - val_loss: 1.7688 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.7583 - train_accuracy: 0.4137 - val_loss: 1.7574 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.7476 - train_accuracy: 0.4137 - val_loss: 1.7462 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.7370 - train_accuracy: 0.4137 - val_loss: 1.7352 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.7267 - train_accuracy: 0.4137 - val_loss: 1.7244 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.7165 - train_accuracy: 0.4137 - val_loss: 1.7139 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.7066 - train_accuracy: 0.4137 - val_loss: 1.7035 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.6969 - train_accuracy: 0.4137 - val_loss: 1.6934 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.6873 - train_accuracy: 0.4137 - val_loss: 1.6835 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.6780 - train_accuracy: 0.4137 - val_loss: 1.6737 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.6689 - train_accuracy: 0.4137 - val_loss: 1.6642 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.6599 - train_accuracy: 0.4137 - val_loss: 1.6549 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.6512 - train_accuracy: 0.4137 - val_loss: 1.6457 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.6426 - train_accuracy: 0.4137 - val_loss: 1.6367 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.6342 - train_accuracy: 0.4137 - val_loss: 1.6280 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.6259 - train_accuracy: 0.4137 - val_loss: 1.6194 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.6179 - train_accuracy: 0.4137 - val_loss: 1.6109 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.6100 - train_accuracy: 0.4137 - val_loss: 1.6027 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.6023 - train_accuracy: 0.4137 - val_loss: 1.5946 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.5947 - train_accuracy: 0.4137 - val_loss: 1.5867 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.5873 - train_accuracy: 0.4137 - val_loss: 1.5789 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.5801 - train_accuracy: 0.4137 - val_loss: 1.5713 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.5730 - train_accuracy: 0.4137 - val_loss: 1.5638 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.5660 - train_accuracy: 0.4137 - val_loss: 1.5565 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.5592 - train_accuracy: 0.4137 - val_loss: 1.5494 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.5525 - train_accuracy: 0.4137 - val_loss: 1.5424 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.5460 - train_accuracy: 0.4137 - val_loss: 1.5356 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.5396 - train_accuracy: 0.4137 - val_loss: 1.5288 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.5334 - train_accuracy: 0.4137 - val_loss: 1.5223 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.5273 - train_accuracy: 0.4137 - val_loss: 1.5158 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.5213 - train_accuracy: 0.4137 - val_loss: 1.5095 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.5154 - train_accuracy: 0.4137 - val_loss: 1.5033 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.5097 - train_accuracy: 0.4137 - val_loss: 1.4973 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.5041 - train_accuracy: 0.4137 - val_loss: 1.4914 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.4986 - train_accuracy: 0.4137 - val_loss: 1.4856 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.4932 - train_accuracy: 0.4137 - val_loss: 1.4799 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.4879 - train_accuracy: 0.4137 - val_loss: 1.4743 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.4828 - train_accuracy: 0.4137 - val_loss: 1.4689 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.4777 - train_accuracy: 0.4137 - val_loss: 1.4635 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.4728 - train_accuracy: 0.4137 - val_loss: 1.4583 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.4679 - train_accuracy: 0.4137 - val_loss: 1.4532 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.4632 - train_accuracy: 0.4137 - val_loss: 1.4481 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.4586 - train_accuracy: 0.4137 - val_loss: 1.4432 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.4540 - train_accuracy: 0.4137 - val_loss: 1.4384 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.4496 - train_accuracy: 0.4137 - val_loss: 1.4337 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.4452 - train_accuracy: 0.4137 - val_loss: 1.4291 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.4409 - train_accuracy: 0.4137 - val_loss: 1.4245 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.4368 - train_accuracy: 0.4137 - val_loss: 1.4201 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.4327 - train_accuracy: 0.4137 - val_loss: 1.4157 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.4287 - train_accuracy: 0.4137 - val_loss: 1.4115 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.4247 - train_accuracy: 0.4137 - val_loss: 1.4073 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.4209 - train_accuracy: 0.4137 - val_loss: 1.4032 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.4171 - train_accuracy: 0.4137 - val_loss: 1.3992 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.4135 - train_accuracy: 0.4137 - val_loss: 1.3953 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.4099 - train_accuracy: 0.4137 - val_loss: 1.3914 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.4063 - train_accuracy: 0.4137 - val_loss: 1.3877 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.4029 - train_accuracy: 0.4137 - val_loss: 1.3840 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.3995 - train_accuracy: 0.4137 - val_loss: 1.3803 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.3961 - train_accuracy: 0.4137 - val_loss: 1.3768 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.3929 - train_accuracy: 0.4137 - val_loss: 1.3733 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.3897 - train_accuracy: 0.4137 - val_loss: 1.3699 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.3866 - train_accuracy: 0.4137 - val_loss: 1.3666 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.3835 - train_accuracy: 0.4137 - val_loss: 1.3633 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.3805 - train_accuracy: 0.4137 - val_loss: 1.3601 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.3776 - train_accuracy: 0.4137 - val_loss: 1.3569 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.3747 - train_accuracy: 0.4137 - val_loss: 1.3538 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.3719 - train_accuracy: 0.4137 - val_loss: 1.3508 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.3691 - train_accuracy: 0.4137 - val_loss: 1.3478 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.3664 - train_accuracy: 0.4137 - val_loss: 1.3449 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.3637 - train_accuracy: 0.4137 - val_loss: 1.3420 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.3611 - train_accuracy: 0.4137 - val_loss: 1.3392 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.3586 - train_accuracy: 0.4137 - val_loss: 1.3365 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.3561 - train_accuracy: 0.4137 - val_loss: 1.3338 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.3536 - train_accuracy: 0.4137 - val_loss: 1.3312 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.3512 - train_accuracy: 0.4137 - val_loss: 1.3286 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.3489 - train_accuracy: 0.4137 - val_loss: 1.3260 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.3465 - train_accuracy: 0.4137 - val_loss: 1.3235 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.3443 - train_accuracy: 0.4137 - val_loss: 1.3211 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.3421 - train_accuracy: 0.4137 - val_loss: 1.3187 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.3399 - train_accuracy: 0.4137 - val_loss: 1.3163 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.3377 - train_accuracy: 0.4137 - val_loss: 1.3140 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.3356 - train_accuracy: 0.4137 - val_loss: 1.3117 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.3336 - train_accuracy: 0.4137 - val_loss: 1.3095 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.3316 - train_accuracy: 0.4137 - val_loss: 1.3073 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.3296 - train_accuracy: 0.4137 - val_loss: 1.3052 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.3276 - train_accuracy: 0.4137 - val_loss: 1.3031 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.3257 - train_accuracy: 0.4137 - val_loss: 1.3010 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.3238 - train_accuracy: 0.4137 - val_loss: 1.2990 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.3220 - train_accuracy: 0.4137 - val_loss: 1.2970 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.3202 - train_accuracy: 0.4137 - val_loss: 1.2950 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.3184 - train_accuracy: 0.4137 - val_loss: 1.2931 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.3167 - train_accuracy: 0.4137 - val_loss: 1.2912 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.3150 - train_accuracy: 0.4137 - val_loss: 1.2893 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.3133 - train_accuracy: 0.4137 - val_loss: 1.2875 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.3117 - train_accuracy: 0.4137 - val_loss: 1.2857 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.3101 - train_accuracy: 0.4137 - val_loss: 1.2840 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.3085 - train_accuracy: 0.4137 - val_loss: 1.2823 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.3069 - train_accuracy: 0.4137 - val_loss: 1.2806 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.3054 - train_accuracy: 0.4137 - val_loss: 1.2789 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681e26576f884adf999fa9a01cb16df7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.001_act=sigmoid_optim=mbgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/i7pydxu3' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/i7pydxu3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124613-i7pydxu3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1146728e1cf043eaa80dc4cf3d46aa72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124624-eu7g627t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/eu7g627t' target=\"_blank\">lr=0.001_act=tanh_optim=sgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/eu7g627t' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/eu7g627t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.6667 - train_accuracy: 0.4137 - val_loss: 1.6617 - val_accuracy: 0.4070\n",
      "Epoch 2/100 - train_loss: 1.5703 - train_accuracy: 0.4137 - val_loss: 1.5609 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.4965 - train_accuracy: 0.4137 - val_loss: 1.4833 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.4399 - train_accuracy: 0.4137 - val_loss: 1.4235 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.3963 - train_accuracy: 0.4137 - val_loss: 1.3771 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.3624 - train_accuracy: 0.4137 - val_loss: 1.3408 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.3357 - train_accuracy: 0.4137 - val_loss: 1.3121 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.3146 - train_accuracy: 0.4137 - val_loss: 1.2892 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.2975 - train_accuracy: 0.4137 - val_loss: 1.2707 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.2836 - train_accuracy: 0.4137 - val_loss: 1.2556 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.2722 - train_accuracy: 0.4137 - val_loss: 1.2431 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.2627 - train_accuracy: 0.4137 - val_loss: 1.2327 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.2547 - train_accuracy: 0.4137 - val_loss: 1.2239 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.2478 - train_accuracy: 0.4137 - val_loss: 1.2164 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.2420 - train_accuracy: 0.4137 - val_loss: 1.2100 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.2369 - train_accuracy: 0.4137 - val_loss: 1.2045 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.2325 - train_accuracy: 0.4137 - val_loss: 1.1997 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.2285 - train_accuracy: 0.4137 - val_loss: 1.1955 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.2251 - train_accuracy: 0.4137 - val_loss: 1.1918 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.2220 - train_accuracy: 0.4137 - val_loss: 1.1885 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.2192 - train_accuracy: 0.4137 - val_loss: 1.1856 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.2167 - train_accuracy: 0.4137 - val_loss: 1.1829 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.2145 - train_accuracy: 0.4137 - val_loss: 1.1806 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.2124 - train_accuracy: 0.4137 - val_loss: 1.1785 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.2105 - train_accuracy: 0.4137 - val_loss: 1.1766 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.2088 - train_accuracy: 0.4137 - val_loss: 1.1749 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.2072 - train_accuracy: 0.4137 - val_loss: 1.1733 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.2058 - train_accuracy: 0.4137 - val_loss: 1.1719 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.2044 - train_accuracy: 0.4137 - val_loss: 1.1706 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.2031 - train_accuracy: 0.4137 - val_loss: 1.1694 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.2020 - train_accuracy: 0.4137 - val_loss: 1.1684 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.2009 - train_accuracy: 0.4137 - val_loss: 1.1674 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.1999 - train_accuracy: 0.4137 - val_loss: 1.1665 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.1989 - train_accuracy: 0.4137 - val_loss: 1.1656 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.1981 - train_accuracy: 0.4137 - val_loss: 1.1649 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.1972 - train_accuracy: 0.4137 - val_loss: 1.1642 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.1964 - train_accuracy: 0.4137 - val_loss: 1.1635 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.1957 - train_accuracy: 0.4137 - val_loss: 1.1629 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.1950 - train_accuracy: 0.4137 - val_loss: 1.1624 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.1944 - train_accuracy: 0.4137 - val_loss: 1.1619 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.1938 - train_accuracy: 0.4137 - val_loss: 1.1614 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.1932 - train_accuracy: 0.4137 - val_loss: 1.1610 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.1926 - train_accuracy: 0.4137 - val_loss: 1.1606 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.1921 - train_accuracy: 0.4137 - val_loss: 1.1602 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.1916 - train_accuracy: 0.4137 - val_loss: 1.1599 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.1912 - train_accuracy: 0.4137 - val_loss: 1.1596 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.1907 - train_accuracy: 0.4137 - val_loss: 1.1593 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.1903 - train_accuracy: 0.4137 - val_loss: 1.1590 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.1899 - train_accuracy: 0.4137 - val_loss: 1.1588 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.1895 - train_accuracy: 0.4137 - val_loss: 1.1586 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.1891 - train_accuracy: 0.4137 - val_loss: 1.1584 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.1888 - train_accuracy: 0.4137 - val_loss: 1.1582 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.1885 - train_accuracy: 0.4137 - val_loss: 1.1580 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.1881 - train_accuracy: 0.4137 - val_loss: 1.1579 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.1878 - train_accuracy: 0.4137 - val_loss: 1.1577 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.1875 - train_accuracy: 0.4137 - val_loss: 1.1576 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.1873 - train_accuracy: 0.4137 - val_loss: 1.1575 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.1870 - train_accuracy: 0.4137 - val_loss: 1.1574 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.1867 - train_accuracy: 0.4137 - val_loss: 1.1573 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.1865 - train_accuracy: 0.4137 - val_loss: 1.1572 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.1862 - train_accuracy: 0.4137 - val_loss: 1.1571 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.1860 - train_accuracy: 0.4137 - val_loss: 1.1571 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.1858 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.1856 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.1854 - train_accuracy: 0.4137 - val_loss: 1.1569 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.1852 - train_accuracy: 0.4137 - val_loss: 1.1569 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.1850 - train_accuracy: 0.4137 - val_loss: 1.1569 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.1848 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.1846 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.1844 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.1842 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.1840 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.1839 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.1837 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.1835 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.1834 - train_accuracy: 0.4164 - val_loss: 1.1567 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.1832 - train_accuracy: 0.4164 - val_loss: 1.1567 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.1830 - train_accuracy: 0.4259 - val_loss: 1.1567 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.1829 - train_accuracy: 0.4313 - val_loss: 1.1567 - val_accuracy: 0.4244\n",
      "Epoch 80/100 - train_loss: 1.1827 - train_accuracy: 0.4420 - val_loss: 1.1567 - val_accuracy: 0.4419\n",
      "Epoch 81/100 - train_loss: 1.1825 - train_accuracy: 0.4609 - val_loss: 1.1567 - val_accuracy: 0.4593\n",
      "Epoch 82/100 - train_loss: 1.1823 - train_accuracy: 0.4946 - val_loss: 1.1567 - val_accuracy: 0.5058\n",
      "Epoch 83/100 - train_loss: 1.1822 - train_accuracy: 0.5310 - val_loss: 1.1567 - val_accuracy: 0.5523\n",
      "Epoch 84/100 - train_loss: 1.1820 - train_accuracy: 0.5499 - val_loss: 1.1567 - val_accuracy: 0.5465\n",
      "Epoch 85/100 - train_loss: 1.1818 - train_accuracy: 0.5633 - val_loss: 1.1566 - val_accuracy: 0.5233\n",
      "Epoch 86/100 - train_loss: 1.1816 - train_accuracy: 0.5701 - val_loss: 1.1566 - val_accuracy: 0.5174\n",
      "Epoch 87/100 - train_loss: 1.1813 - train_accuracy: 0.5728 - val_loss: 1.1566 - val_accuracy: 0.5233\n",
      "Epoch 88/100 - train_loss: 1.1811 - train_accuracy: 0.5849 - val_loss: 1.1565 - val_accuracy: 0.5465\n",
      "Epoch 89/100 - train_loss: 1.1808 - train_accuracy: 0.5741 - val_loss: 1.1564 - val_accuracy: 0.5407\n",
      "Epoch 90/100 - train_loss: 1.1805 - train_accuracy: 0.5593 - val_loss: 1.1563 - val_accuracy: 0.5349\n",
      "Epoch 91/100 - train_loss: 1.1802 - train_accuracy: 0.5580 - val_loss: 1.1561 - val_accuracy: 0.5233\n",
      "Epoch 92/100 - train_loss: 1.1798 - train_accuracy: 0.5539 - val_loss: 1.1559 - val_accuracy: 0.5174\n",
      "Epoch 93/100 - train_loss: 1.1794 - train_accuracy: 0.5539 - val_loss: 1.1557 - val_accuracy: 0.5291\n",
      "Epoch 94/100 - train_loss: 1.1788 - train_accuracy: 0.5526 - val_loss: 1.1553 - val_accuracy: 0.5407\n",
      "Epoch 95/100 - train_loss: 1.1782 - train_accuracy: 0.5539 - val_loss: 1.1549 - val_accuracy: 0.5465\n",
      "Epoch 96/100 - train_loss: 1.1774 - train_accuracy: 0.5485 - val_loss: 1.1544 - val_accuracy: 0.5407\n",
      "Epoch 97/100 - train_loss: 1.1765 - train_accuracy: 0.5472 - val_loss: 1.1537 - val_accuracy: 0.5465\n",
      "Epoch 98/100 - train_loss: 1.1753 - train_accuracy: 0.5458 - val_loss: 1.1528 - val_accuracy: 0.5407\n",
      "Epoch 99/100 - train_loss: 1.1738 - train_accuracy: 0.5485 - val_loss: 1.1516 - val_accuracy: 0.5465\n",
      "Epoch 100/100 - train_loss: 1.1718 - train_accuracy: 0.5539 - val_loss: 1.1500 - val_accuracy: 0.5465\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a65cf47ea5848f8b9383835afe76e71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.001_act=tanh_optim=sgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/eu7g627t' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/eu7g627t</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124624-eu7g627t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6118ea51c2504636a327854814b60ddd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124637-k473irvx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/k473irvx' target=\"_blank\">lr=0.001_act=tanh_optim=bgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/k473irvx' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/k473irvx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7916 - train_accuracy: 0.3962 - val_loss: 1.7916 - val_accuracy: 0.3081\n",
      "Epoch 2/100 - train_loss: 1.7914 - train_accuracy: 0.4003 - val_loss: 1.7914 - val_accuracy: 0.3547\n",
      "Epoch 3/100 - train_loss: 1.7912 - train_accuracy: 0.4003 - val_loss: 1.7912 - val_accuracy: 0.3663\n",
      "Epoch 4/100 - train_loss: 1.7910 - train_accuracy: 0.4030 - val_loss: 1.7910 - val_accuracy: 0.3837\n",
      "Epoch 5/100 - train_loss: 1.7908 - train_accuracy: 0.3976 - val_loss: 1.7908 - val_accuracy: 0.3605\n",
      "Epoch 6/100 - train_loss: 1.7906 - train_accuracy: 0.3935 - val_loss: 1.7906 - val_accuracy: 0.3779\n",
      "Epoch 7/100 - train_loss: 1.7904 - train_accuracy: 0.3989 - val_loss: 1.7904 - val_accuracy: 0.3895\n",
      "Epoch 8/100 - train_loss: 1.7902 - train_accuracy: 0.3962 - val_loss: 1.7902 - val_accuracy: 0.3953\n",
      "Epoch 9/100 - train_loss: 1.7901 - train_accuracy: 0.4030 - val_loss: 1.7900 - val_accuracy: 0.3953\n",
      "Epoch 10/100 - train_loss: 1.7899 - train_accuracy: 0.4070 - val_loss: 1.7898 - val_accuracy: 0.4012\n",
      "Epoch 11/100 - train_loss: 1.7897 - train_accuracy: 0.4111 - val_loss: 1.7896 - val_accuracy: 0.3953\n",
      "Epoch 12/100 - train_loss: 1.7895 - train_accuracy: 0.4151 - val_loss: 1.7894 - val_accuracy: 0.4012\n",
      "Epoch 13/100 - train_loss: 1.7893 - train_accuracy: 0.4124 - val_loss: 1.7892 - val_accuracy: 0.4012\n",
      "Epoch 14/100 - train_loss: 1.7891 - train_accuracy: 0.4097 - val_loss: 1.7890 - val_accuracy: 0.4012\n",
      "Epoch 15/100 - train_loss: 1.7889 - train_accuracy: 0.4111 - val_loss: 1.7888 - val_accuracy: 0.4012\n",
      "Epoch 16/100 - train_loss: 1.7887 - train_accuracy: 0.4111 - val_loss: 1.7886 - val_accuracy: 0.4012\n",
      "Epoch 17/100 - train_loss: 1.7885 - train_accuracy: 0.4111 - val_loss: 1.7884 - val_accuracy: 0.4012\n",
      "Epoch 18/100 - train_loss: 1.7883 - train_accuracy: 0.4137 - val_loss: 1.7882 - val_accuracy: 0.4012\n",
      "Epoch 19/100 - train_loss: 1.7882 - train_accuracy: 0.4137 - val_loss: 1.7880 - val_accuracy: 0.4012\n",
      "Epoch 20/100 - train_loss: 1.7880 - train_accuracy: 0.4111 - val_loss: 1.7878 - val_accuracy: 0.4012\n",
      "Epoch 21/100 - train_loss: 1.7878 - train_accuracy: 0.4151 - val_loss: 1.7876 - val_accuracy: 0.4012\n",
      "Epoch 22/100 - train_loss: 1.7876 - train_accuracy: 0.4151 - val_loss: 1.7874 - val_accuracy: 0.4012\n",
      "Epoch 23/100 - train_loss: 1.7874 - train_accuracy: 0.4151 - val_loss: 1.7872 - val_accuracy: 0.4012\n",
      "Epoch 24/100 - train_loss: 1.7872 - train_accuracy: 0.4124 - val_loss: 1.7870 - val_accuracy: 0.4012\n",
      "Epoch 25/100 - train_loss: 1.7870 - train_accuracy: 0.4124 - val_loss: 1.7868 - val_accuracy: 0.4012\n",
      "Epoch 26/100 - train_loss: 1.7868 - train_accuracy: 0.4137 - val_loss: 1.7866 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.7866 - train_accuracy: 0.4137 - val_loss: 1.7864 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.7865 - train_accuracy: 0.4137 - val_loss: 1.7862 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.7863 - train_accuracy: 0.4137 - val_loss: 1.7861 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.7861 - train_accuracy: 0.4137 - val_loss: 1.7859 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.7859 - train_accuracy: 0.4137 - val_loss: 1.7857 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.7857 - train_accuracy: 0.4137 - val_loss: 1.7855 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.7855 - train_accuracy: 0.4137 - val_loss: 1.7853 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.7853 - train_accuracy: 0.4137 - val_loss: 1.7851 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.7851 - train_accuracy: 0.4137 - val_loss: 1.7849 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.7849 - train_accuracy: 0.4137 - val_loss: 1.7847 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.7848 - train_accuracy: 0.4137 - val_loss: 1.7845 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.7846 - train_accuracy: 0.4137 - val_loss: 1.7843 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.7844 - train_accuracy: 0.4137 - val_loss: 1.7841 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.7842 - train_accuracy: 0.4137 - val_loss: 1.7839 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.7840 - train_accuracy: 0.4137 - val_loss: 1.7837 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.7838 - train_accuracy: 0.4137 - val_loss: 1.7835 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.7836 - train_accuracy: 0.4137 - val_loss: 1.7833 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.7834 - train_accuracy: 0.4137 - val_loss: 1.7831 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.7832 - train_accuracy: 0.4137 - val_loss: 1.7829 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.7831 - train_accuracy: 0.4137 - val_loss: 1.7827 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.7829 - train_accuracy: 0.4137 - val_loss: 1.7825 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.7827 - train_accuracy: 0.4137 - val_loss: 1.7823 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.7825 - train_accuracy: 0.4137 - val_loss: 1.7821 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.7823 - train_accuracy: 0.4137 - val_loss: 1.7820 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.7821 - train_accuracy: 0.4137 - val_loss: 1.7818 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.7819 - train_accuracy: 0.4137 - val_loss: 1.7816 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.7817 - train_accuracy: 0.4137 - val_loss: 1.7814 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.7816 - train_accuracy: 0.4137 - val_loss: 1.7812 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.7814 - train_accuracy: 0.4137 - val_loss: 1.7810 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.7812 - train_accuracy: 0.4137 - val_loss: 1.7808 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.7810 - train_accuracy: 0.4137 - val_loss: 1.7806 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.7808 - train_accuracy: 0.4137 - val_loss: 1.7804 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.7806 - train_accuracy: 0.4137 - val_loss: 1.7802 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.7804 - train_accuracy: 0.4137 - val_loss: 1.7800 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.7803 - train_accuracy: 0.4137 - val_loss: 1.7798 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.7801 - train_accuracy: 0.4137 - val_loss: 1.7796 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.7799 - train_accuracy: 0.4137 - val_loss: 1.7794 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.7797 - train_accuracy: 0.4137 - val_loss: 1.7792 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.7795 - train_accuracy: 0.4137 - val_loss: 1.7790 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.7793 - train_accuracy: 0.4137 - val_loss: 1.7788 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.7791 - train_accuracy: 0.4137 - val_loss: 1.7787 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.7789 - train_accuracy: 0.4137 - val_loss: 1.7785 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.7788 - train_accuracy: 0.4137 - val_loss: 1.7783 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.7786 - train_accuracy: 0.4137 - val_loss: 1.7781 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.7784 - train_accuracy: 0.4137 - val_loss: 1.7779 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.7782 - train_accuracy: 0.4137 - val_loss: 1.7777 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.7780 - train_accuracy: 0.4137 - val_loss: 1.7775 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.7778 - train_accuracy: 0.4137 - val_loss: 1.7773 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.7776 - train_accuracy: 0.4137 - val_loss: 1.7771 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.7775 - train_accuracy: 0.4137 - val_loss: 1.7769 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.7773 - train_accuracy: 0.4137 - val_loss: 1.7767 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.7771 - train_accuracy: 0.4137 - val_loss: 1.7765 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.7769 - train_accuracy: 0.4137 - val_loss: 1.7763 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.7767 - train_accuracy: 0.4137 - val_loss: 1.7761 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.7765 - train_accuracy: 0.4137 - val_loss: 1.7759 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.7763 - train_accuracy: 0.4137 - val_loss: 1.7758 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.7762 - train_accuracy: 0.4137 - val_loss: 1.7756 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.7760 - train_accuracy: 0.4137 - val_loss: 1.7754 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.7758 - train_accuracy: 0.4137 - val_loss: 1.7752 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.7756 - train_accuracy: 0.4137 - val_loss: 1.7750 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.7754 - train_accuracy: 0.4137 - val_loss: 1.7748 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.7752 - train_accuracy: 0.4137 - val_loss: 1.7746 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.7750 - train_accuracy: 0.4137 - val_loss: 1.7744 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.7749 - train_accuracy: 0.4137 - val_loss: 1.7742 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.7747 - train_accuracy: 0.4137 - val_loss: 1.7740 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.7745 - train_accuracy: 0.4137 - val_loss: 1.7738 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.7743 - train_accuracy: 0.4137 - val_loss: 1.7736 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.7741 - train_accuracy: 0.4137 - val_loss: 1.7734 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.7739 - train_accuracy: 0.4137 - val_loss: 1.7733 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.7738 - train_accuracy: 0.4137 - val_loss: 1.7731 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.7736 - train_accuracy: 0.4137 - val_loss: 1.7729 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.7734 - train_accuracy: 0.4137 - val_loss: 1.7727 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.7732 - train_accuracy: 0.4137 - val_loss: 1.7725 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.7730 - train_accuracy: 0.4137 - val_loss: 1.7723 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d75b08f1694a0489d96a94a1beb6e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.001_act=tanh_optim=bgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/k473irvx' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/k473irvx</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124637-k473irvx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac097e2e603942ac897eeb120ebc2f9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124649-f0zyoyi8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/f0zyoyi8' target=\"_blank\">lr=0.001_act=tanh_optim=mbgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/f0zyoyi8' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/f0zyoyi8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7872 - train_accuracy: 0.4137 - val_loss: 1.7870 - val_accuracy: 0.4070\n",
      "Epoch 2/100 - train_loss: 1.7826 - train_accuracy: 0.4137 - val_loss: 1.7822 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.7781 - train_accuracy: 0.4137 - val_loss: 1.7775 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.7736 - train_accuracy: 0.4137 - val_loss: 1.7729 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.7691 - train_accuracy: 0.4137 - val_loss: 1.7682 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.7647 - train_accuracy: 0.4137 - val_loss: 1.7636 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.7603 - train_accuracy: 0.4137 - val_loss: 1.7591 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.7560 - train_accuracy: 0.4137 - val_loss: 1.7546 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.7516 - train_accuracy: 0.4137 - val_loss: 1.7501 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.7474 - train_accuracy: 0.4137 - val_loss: 1.7456 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.7431 - train_accuracy: 0.4137 - val_loss: 1.7412 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.7389 - train_accuracy: 0.4137 - val_loss: 1.7368 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.7347 - train_accuracy: 0.4137 - val_loss: 1.7325 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.7306 - train_accuracy: 0.4137 - val_loss: 1.7282 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.7265 - train_accuracy: 0.4137 - val_loss: 1.7239 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.7224 - train_accuracy: 0.4137 - val_loss: 1.7197 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.7184 - train_accuracy: 0.4137 - val_loss: 1.7155 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.7144 - train_accuracy: 0.4137 - val_loss: 1.7113 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.7104 - train_accuracy: 0.4137 - val_loss: 1.7072 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.7065 - train_accuracy: 0.4137 - val_loss: 1.7031 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.7026 - train_accuracy: 0.4137 - val_loss: 1.6990 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.6987 - train_accuracy: 0.4137 - val_loss: 1.6950 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.6949 - train_accuracy: 0.4137 - val_loss: 1.6910 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.6911 - train_accuracy: 0.4137 - val_loss: 1.6870 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.6873 - train_accuracy: 0.4137 - val_loss: 1.6831 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.6835 - train_accuracy: 0.4137 - val_loss: 1.6792 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.6798 - train_accuracy: 0.4137 - val_loss: 1.6753 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.6761 - train_accuracy: 0.4137 - val_loss: 1.6715 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.6725 - train_accuracy: 0.4137 - val_loss: 1.6676 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.6689 - train_accuracy: 0.4137 - val_loss: 1.6639 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.6653 - train_accuracy: 0.4137 - val_loss: 1.6601 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.6617 - train_accuracy: 0.4137 - val_loss: 1.6564 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.6582 - train_accuracy: 0.4137 - val_loss: 1.6527 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.6547 - train_accuracy: 0.4137 - val_loss: 1.6491 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.6512 - train_accuracy: 0.4137 - val_loss: 1.6455 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.6478 - train_accuracy: 0.4137 - val_loss: 1.6419 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.6444 - train_accuracy: 0.4137 - val_loss: 1.6383 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.6410 - train_accuracy: 0.4137 - val_loss: 1.6348 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.6377 - train_accuracy: 0.4137 - val_loss: 1.6313 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.6343 - train_accuracy: 0.4137 - val_loss: 1.6278 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.6311 - train_accuracy: 0.4137 - val_loss: 1.6244 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.6278 - train_accuracy: 0.4137 - val_loss: 1.6210 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.6245 - train_accuracy: 0.4137 - val_loss: 1.6176 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.6213 - train_accuracy: 0.4137 - val_loss: 1.6142 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.6182 - train_accuracy: 0.4137 - val_loss: 1.6109 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.6150 - train_accuracy: 0.4137 - val_loss: 1.6076 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.6119 - train_accuracy: 0.4137 - val_loss: 1.6043 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.6088 - train_accuracy: 0.4137 - val_loss: 1.6011 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.6057 - train_accuracy: 0.4137 - val_loss: 1.5978 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.6027 - train_accuracy: 0.4137 - val_loss: 1.5947 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.5996 - train_accuracy: 0.4137 - val_loss: 1.5915 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.5966 - train_accuracy: 0.4137 - val_loss: 1.5884 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.5937 - train_accuracy: 0.4137 - val_loss: 1.5852 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.5907 - train_accuracy: 0.4137 - val_loss: 1.5822 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.5878 - train_accuracy: 0.4137 - val_loss: 1.5791 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.5849 - train_accuracy: 0.4137 - val_loss: 1.5761 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.5820 - train_accuracy: 0.4137 - val_loss: 1.5731 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.5792 - train_accuracy: 0.4137 - val_loss: 1.5701 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.5764 - train_accuracy: 0.4137 - val_loss: 1.5671 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.5736 - train_accuracy: 0.4137 - val_loss: 1.5642 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.5708 - train_accuracy: 0.4137 - val_loss: 1.5613 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.5681 - train_accuracy: 0.4137 - val_loss: 1.5584 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.5654 - train_accuracy: 0.4137 - val_loss: 1.5556 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.5627 - train_accuracy: 0.4137 - val_loss: 1.5527 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.5600 - train_accuracy: 0.4137 - val_loss: 1.5499 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.5573 - train_accuracy: 0.4137 - val_loss: 1.5471 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.5547 - train_accuracy: 0.4137 - val_loss: 1.5444 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.5521 - train_accuracy: 0.4137 - val_loss: 1.5416 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.5495 - train_accuracy: 0.4137 - val_loss: 1.5389 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.5470 - train_accuracy: 0.4137 - val_loss: 1.5362 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.5444 - train_accuracy: 0.4137 - val_loss: 1.5336 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.5419 - train_accuracy: 0.4137 - val_loss: 1.5309 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.5394 - train_accuracy: 0.4137 - val_loss: 1.5283 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.5369 - train_accuracy: 0.4137 - val_loss: 1.5257 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.5345 - train_accuracy: 0.4137 - val_loss: 1.5231 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.5321 - train_accuracy: 0.4137 - val_loss: 1.5206 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.5296 - train_accuracy: 0.4137 - val_loss: 1.5180 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.5273 - train_accuracy: 0.4137 - val_loss: 1.5155 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.5249 - train_accuracy: 0.4137 - val_loss: 1.5130 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.5225 - train_accuracy: 0.4137 - val_loss: 1.5106 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.5202 - train_accuracy: 0.4137 - val_loss: 1.5081 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.5179 - train_accuracy: 0.4137 - val_loss: 1.5057 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.5156 - train_accuracy: 0.4137 - val_loss: 1.5033 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.5134 - train_accuracy: 0.4137 - val_loss: 1.5009 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.5111 - train_accuracy: 0.4137 - val_loss: 1.4985 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.5089 - train_accuracy: 0.4137 - val_loss: 1.4962 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.5067 - train_accuracy: 0.4137 - val_loss: 1.4938 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.5045 - train_accuracy: 0.4137 - val_loss: 1.4915 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.5023 - train_accuracy: 0.4137 - val_loss: 1.4892 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.5002 - train_accuracy: 0.4137 - val_loss: 1.4870 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.4980 - train_accuracy: 0.4137 - val_loss: 1.4847 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.4959 - train_accuracy: 0.4137 - val_loss: 1.4825 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.4938 - train_accuracy: 0.4137 - val_loss: 1.4803 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.4917 - train_accuracy: 0.4137 - val_loss: 1.4781 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.4897 - train_accuracy: 0.4137 - val_loss: 1.4759 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.4876 - train_accuracy: 0.4137 - val_loss: 1.4738 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.4856 - train_accuracy: 0.4137 - val_loss: 1.4716 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.4836 - train_accuracy: 0.4137 - val_loss: 1.4695 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.4816 - train_accuracy: 0.4137 - val_loss: 1.4674 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.4797 - train_accuracy: 0.4137 - val_loss: 1.4653 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e57a414f63476b9f2ef2a46b50dda5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.001_act=tanh_optim=mbgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/f0zyoyi8' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/f0zyoyi8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124649-f0zyoyi8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb20c5c84854017a36831fc71d57dfa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124700-u40zm7eb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/u40zm7eb' target=\"_blank\">lr=0.001_act=relu_optim=sgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/u40zm7eb' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/u40zm7eb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.6667 - train_accuracy: 0.4137 - val_loss: 1.6617 - val_accuracy: 0.4070\n",
      "Epoch 2/100 - train_loss: 1.5704 - train_accuracy: 0.4137 - val_loss: 1.5609 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.4965 - train_accuracy: 0.4137 - val_loss: 1.4833 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.4400 - train_accuracy: 0.4137 - val_loss: 1.4235 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.3964 - train_accuracy: 0.4137 - val_loss: 1.3771 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.3624 - train_accuracy: 0.4137 - val_loss: 1.3409 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.3358 - train_accuracy: 0.4137 - val_loss: 1.3122 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.3146 - train_accuracy: 0.4137 - val_loss: 1.2893 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.2976 - train_accuracy: 0.4137 - val_loss: 1.2708 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.2837 - train_accuracy: 0.4137 - val_loss: 1.2557 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.2723 - train_accuracy: 0.4137 - val_loss: 1.2432 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.2628 - train_accuracy: 0.4137 - val_loss: 1.2327 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.2547 - train_accuracy: 0.4137 - val_loss: 1.2240 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.2479 - train_accuracy: 0.4137 - val_loss: 1.2165 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.2421 - train_accuracy: 0.4137 - val_loss: 1.2101 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.2370 - train_accuracy: 0.4137 - val_loss: 1.2046 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.2325 - train_accuracy: 0.4137 - val_loss: 1.1998 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.2286 - train_accuracy: 0.4137 - val_loss: 1.1956 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.2252 - train_accuracy: 0.4137 - val_loss: 1.1919 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.2221 - train_accuracy: 0.4137 - val_loss: 1.1886 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.2193 - train_accuracy: 0.4137 - val_loss: 1.1857 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.2168 - train_accuracy: 0.4137 - val_loss: 1.1831 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.2146 - train_accuracy: 0.4137 - val_loss: 1.1807 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.2125 - train_accuracy: 0.4137 - val_loss: 1.1786 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.2106 - train_accuracy: 0.4137 - val_loss: 1.1767 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.2089 - train_accuracy: 0.4137 - val_loss: 1.1750 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.2073 - train_accuracy: 0.4137 - val_loss: 1.1734 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.2059 - train_accuracy: 0.4137 - val_loss: 1.1720 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.2045 - train_accuracy: 0.4137 - val_loss: 1.1707 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.2033 - train_accuracy: 0.4137 - val_loss: 1.1695 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.2021 - train_accuracy: 0.4137 - val_loss: 1.1685 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.2010 - train_accuracy: 0.4137 - val_loss: 1.1675 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.2000 - train_accuracy: 0.4137 - val_loss: 1.1666 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.1991 - train_accuracy: 0.4137 - val_loss: 1.1657 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.1982 - train_accuracy: 0.4137 - val_loss: 1.1650 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.1973 - train_accuracy: 0.4137 - val_loss: 1.1643 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.1966 - train_accuracy: 0.4137 - val_loss: 1.1636 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.1958 - train_accuracy: 0.4137 - val_loss: 1.1630 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.1951 - train_accuracy: 0.4137 - val_loss: 1.1625 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.1945 - train_accuracy: 0.4137 - val_loss: 1.1620 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.1939 - train_accuracy: 0.4137 - val_loss: 1.1615 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.1933 - train_accuracy: 0.4137 - val_loss: 1.1611 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.1928 - train_accuracy: 0.4137 - val_loss: 1.1607 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.1922 - train_accuracy: 0.4137 - val_loss: 1.1603 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.1917 - train_accuracy: 0.4137 - val_loss: 1.1600 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.1913 - train_accuracy: 0.4137 - val_loss: 1.1597 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.1908 - train_accuracy: 0.4137 - val_loss: 1.1594 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.1904 - train_accuracy: 0.4137 - val_loss: 1.1591 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.1900 - train_accuracy: 0.4137 - val_loss: 1.1589 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.1896 - train_accuracy: 0.4137 - val_loss: 1.1587 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.1893 - train_accuracy: 0.4137 - val_loss: 1.1585 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.1889 - train_accuracy: 0.4137 - val_loss: 1.1583 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.1886 - train_accuracy: 0.4137 - val_loss: 1.1581 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.1883 - train_accuracy: 0.4137 - val_loss: 1.1580 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.1880 - train_accuracy: 0.4137 - val_loss: 1.1579 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.1877 - train_accuracy: 0.4137 - val_loss: 1.1577 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.1874 - train_accuracy: 0.4137 - val_loss: 1.1576 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.1872 - train_accuracy: 0.4137 - val_loss: 1.1575 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.1869 - train_accuracy: 0.4137 - val_loss: 1.1574 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.1867 - train_accuracy: 0.4137 - val_loss: 1.1574 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.1864 - train_accuracy: 0.4137 - val_loss: 1.1573 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.1862 - train_accuracy: 0.4137 - val_loss: 1.1572 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.1860 - train_accuracy: 0.4137 - val_loss: 1.1572 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.1858 - train_accuracy: 0.4137 - val_loss: 1.1571 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.1856 - train_accuracy: 0.4137 - val_loss: 1.1571 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.1854 - train_accuracy: 0.4137 - val_loss: 1.1571 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.1852 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.1850 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.1848 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.1847 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.1845 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.1844 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.1842 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.1841 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.1839 - train_accuracy: 0.4137 - val_loss: 1.1571 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.1838 - train_accuracy: 0.4137 - val_loss: 1.1571 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.1836 - train_accuracy: 0.4137 - val_loss: 1.1571 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.1835 - train_accuracy: 0.4137 - val_loss: 1.1571 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.1834 - train_accuracy: 0.4137 - val_loss: 1.1572 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.1833 - train_accuracy: 0.4137 - val_loss: 1.1572 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.1831 - train_accuracy: 0.4137 - val_loss: 1.1573 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.1830 - train_accuracy: 0.4137 - val_loss: 1.1573 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.1829 - train_accuracy: 0.4137 - val_loss: 1.1573 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.1828 - train_accuracy: 0.4137 - val_loss: 1.1574 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.1827 - train_accuracy: 0.4137 - val_loss: 1.1574 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.1826 - train_accuracy: 0.4137 - val_loss: 1.1575 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.1825 - train_accuracy: 0.4137 - val_loss: 1.1575 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.1824 - train_accuracy: 0.4137 - val_loss: 1.1576 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.1823 - train_accuracy: 0.4137 - val_loss: 1.1577 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.1822 - train_accuracy: 0.4137 - val_loss: 1.1577 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.1821 - train_accuracy: 0.4137 - val_loss: 1.1578 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.1820 - train_accuracy: 0.4137 - val_loss: 1.1578 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.1819 - train_accuracy: 0.4137 - val_loss: 1.1579 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.1819 - train_accuracy: 0.4137 - val_loss: 1.1580 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.1818 - train_accuracy: 0.4137 - val_loss: 1.1580 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.1817 - train_accuracy: 0.4137 - val_loss: 1.1581 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.1816 - train_accuracy: 0.4137 - val_loss: 1.1582 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.1815 - train_accuracy: 0.4137 - val_loss: 1.1583 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.1815 - train_accuracy: 0.4137 - val_loss: 1.1583 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.1814 - train_accuracy: 0.4137 - val_loss: 1.1584 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600a18b995d74ae397bac43b9f0da917"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.001_act=relu_optim=sgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/u40zm7eb' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/u40zm7eb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124700-u40zm7eb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0faf0e4fe09640f5b7d11fc17e494437"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124713-tps9f848</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/tps9f848' target=\"_blank\">lr=0.001_act=relu_optim=bgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/tps9f848' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/tps9f848</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7916 - train_accuracy: 0.4124 - val_loss: 1.7916 - val_accuracy: 0.4535\n",
      "Epoch 2/100 - train_loss: 1.7914 - train_accuracy: 0.3693 - val_loss: 1.7914 - val_accuracy: 0.4302\n",
      "Epoch 3/100 - train_loss: 1.7912 - train_accuracy: 0.3774 - val_loss: 1.7912 - val_accuracy: 0.3895\n",
      "Epoch 4/100 - train_loss: 1.7910 - train_accuracy: 0.3760 - val_loss: 1.7910 - val_accuracy: 0.4012\n",
      "Epoch 5/100 - train_loss: 1.7908 - train_accuracy: 0.3827 - val_loss: 1.7908 - val_accuracy: 0.3895\n",
      "Epoch 6/100 - train_loss: 1.7906 - train_accuracy: 0.3881 - val_loss: 1.7906 - val_accuracy: 0.4012\n",
      "Epoch 7/100 - train_loss: 1.7904 - train_accuracy: 0.3854 - val_loss: 1.7904 - val_accuracy: 0.4128\n",
      "Epoch 8/100 - train_loss: 1.7902 - train_accuracy: 0.3962 - val_loss: 1.7902 - val_accuracy: 0.3953\n",
      "Epoch 9/100 - train_loss: 1.7900 - train_accuracy: 0.4016 - val_loss: 1.7900 - val_accuracy: 0.4128\n",
      "Epoch 10/100 - train_loss: 1.7899 - train_accuracy: 0.3989 - val_loss: 1.7898 - val_accuracy: 0.4128\n",
      "Epoch 11/100 - train_loss: 1.7897 - train_accuracy: 0.4070 - val_loss: 1.7896 - val_accuracy: 0.4186\n",
      "Epoch 12/100 - train_loss: 1.7895 - train_accuracy: 0.4137 - val_loss: 1.7894 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.7893 - train_accuracy: 0.4178 - val_loss: 1.7892 - val_accuracy: 0.3895\n",
      "Epoch 14/100 - train_loss: 1.7891 - train_accuracy: 0.4164 - val_loss: 1.7890 - val_accuracy: 0.3953\n",
      "Epoch 15/100 - train_loss: 1.7889 - train_accuracy: 0.4151 - val_loss: 1.7888 - val_accuracy: 0.3953\n",
      "Epoch 16/100 - train_loss: 1.7887 - train_accuracy: 0.4137 - val_loss: 1.7886 - val_accuracy: 0.3895\n",
      "Epoch 17/100 - train_loss: 1.7885 - train_accuracy: 0.4164 - val_loss: 1.7884 - val_accuracy: 0.3895\n",
      "Epoch 18/100 - train_loss: 1.7883 - train_accuracy: 0.4137 - val_loss: 1.7882 - val_accuracy: 0.3895\n",
      "Epoch 19/100 - train_loss: 1.7882 - train_accuracy: 0.4151 - val_loss: 1.7880 - val_accuracy: 0.3953\n",
      "Epoch 20/100 - train_loss: 1.7880 - train_accuracy: 0.4137 - val_loss: 1.7878 - val_accuracy: 0.3953\n",
      "Epoch 21/100 - train_loss: 1.7878 - train_accuracy: 0.4151 - val_loss: 1.7876 - val_accuracy: 0.4012\n",
      "Epoch 22/100 - train_loss: 1.7876 - train_accuracy: 0.4124 - val_loss: 1.7874 - val_accuracy: 0.4012\n",
      "Epoch 23/100 - train_loss: 1.7874 - train_accuracy: 0.4137 - val_loss: 1.7872 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.7872 - train_accuracy: 0.4137 - val_loss: 1.7870 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.7870 - train_accuracy: 0.4137 - val_loss: 1.7868 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.7868 - train_accuracy: 0.4137 - val_loss: 1.7866 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.7866 - train_accuracy: 0.4137 - val_loss: 1.7864 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.7865 - train_accuracy: 0.4137 - val_loss: 1.7862 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.7863 - train_accuracy: 0.4137 - val_loss: 1.7861 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.7861 - train_accuracy: 0.4137 - val_loss: 1.7859 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.7859 - train_accuracy: 0.4137 - val_loss: 1.7857 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.7857 - train_accuracy: 0.4137 - val_loss: 1.7855 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.7855 - train_accuracy: 0.4137 - val_loss: 1.7853 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.7853 - train_accuracy: 0.4137 - val_loss: 1.7851 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.7851 - train_accuracy: 0.4137 - val_loss: 1.7849 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.7849 - train_accuracy: 0.4137 - val_loss: 1.7847 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.7848 - train_accuracy: 0.4137 - val_loss: 1.7845 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.7846 - train_accuracy: 0.4137 - val_loss: 1.7843 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.7844 - train_accuracy: 0.4137 - val_loss: 1.7841 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.7842 - train_accuracy: 0.4137 - val_loss: 1.7839 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.7840 - train_accuracy: 0.4137 - val_loss: 1.7837 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.7838 - train_accuracy: 0.4137 - val_loss: 1.7835 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.7836 - train_accuracy: 0.4137 - val_loss: 1.7833 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.7834 - train_accuracy: 0.4137 - val_loss: 1.7831 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.7832 - train_accuracy: 0.4137 - val_loss: 1.7829 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.7831 - train_accuracy: 0.4137 - val_loss: 1.7827 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.7829 - train_accuracy: 0.4137 - val_loss: 1.7825 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.7827 - train_accuracy: 0.4137 - val_loss: 1.7823 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.7825 - train_accuracy: 0.4137 - val_loss: 1.7821 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.7823 - train_accuracy: 0.4137 - val_loss: 1.7820 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.7821 - train_accuracy: 0.4137 - val_loss: 1.7818 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.7819 - train_accuracy: 0.4137 - val_loss: 1.7816 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.7817 - train_accuracy: 0.4137 - val_loss: 1.7814 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.7816 - train_accuracy: 0.4137 - val_loss: 1.7812 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.7814 - train_accuracy: 0.4137 - val_loss: 1.7810 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.7812 - train_accuracy: 0.4137 - val_loss: 1.7808 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.7810 - train_accuracy: 0.4137 - val_loss: 1.7806 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.7808 - train_accuracy: 0.4137 - val_loss: 1.7804 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.7806 - train_accuracy: 0.4137 - val_loss: 1.7802 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.7804 - train_accuracy: 0.4137 - val_loss: 1.7800 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.7803 - train_accuracy: 0.4137 - val_loss: 1.7798 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.7801 - train_accuracy: 0.4137 - val_loss: 1.7796 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.7799 - train_accuracy: 0.4137 - val_loss: 1.7794 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.7797 - train_accuracy: 0.4137 - val_loss: 1.7792 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.7795 - train_accuracy: 0.4137 - val_loss: 1.7790 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.7793 - train_accuracy: 0.4137 - val_loss: 1.7788 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.7791 - train_accuracy: 0.4137 - val_loss: 1.7787 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.7789 - train_accuracy: 0.4137 - val_loss: 1.7785 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.7788 - train_accuracy: 0.4137 - val_loss: 1.7783 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.7786 - train_accuracy: 0.4137 - val_loss: 1.7781 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.7784 - train_accuracy: 0.4137 - val_loss: 1.7779 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.7782 - train_accuracy: 0.4137 - val_loss: 1.7777 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.7780 - train_accuracy: 0.4137 - val_loss: 1.7775 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.7778 - train_accuracy: 0.4137 - val_loss: 1.7773 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.7776 - train_accuracy: 0.4137 - val_loss: 1.7771 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.7775 - train_accuracy: 0.4137 - val_loss: 1.7769 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.7773 - train_accuracy: 0.4137 - val_loss: 1.7767 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.7771 - train_accuracy: 0.4137 - val_loss: 1.7765 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.7769 - train_accuracy: 0.4137 - val_loss: 1.7763 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.7767 - train_accuracy: 0.4137 - val_loss: 1.7761 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.7765 - train_accuracy: 0.4137 - val_loss: 1.7759 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.7763 - train_accuracy: 0.4137 - val_loss: 1.7758 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.7762 - train_accuracy: 0.4137 - val_loss: 1.7756 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.7760 - train_accuracy: 0.4137 - val_loss: 1.7754 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.7758 - train_accuracy: 0.4137 - val_loss: 1.7752 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.7756 - train_accuracy: 0.4137 - val_loss: 1.7750 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.7754 - train_accuracy: 0.4137 - val_loss: 1.7748 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.7752 - train_accuracy: 0.4137 - val_loss: 1.7746 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.7750 - train_accuracy: 0.4137 - val_loss: 1.7744 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.7749 - train_accuracy: 0.4137 - val_loss: 1.7742 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.7747 - train_accuracy: 0.4137 - val_loss: 1.7740 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.7745 - train_accuracy: 0.4137 - val_loss: 1.7738 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.7743 - train_accuracy: 0.4137 - val_loss: 1.7736 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.7741 - train_accuracy: 0.4137 - val_loss: 1.7735 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.7739 - train_accuracy: 0.4137 - val_loss: 1.7733 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.7738 - train_accuracy: 0.4137 - val_loss: 1.7731 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.7736 - train_accuracy: 0.4137 - val_loss: 1.7729 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.7734 - train_accuracy: 0.4137 - val_loss: 1.7727 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.7732 - train_accuracy: 0.4137 - val_loss: 1.7725 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.7730 - train_accuracy: 0.4137 - val_loss: 1.7723 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74efd247c11f4a9e93913e014eb48b7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.001_act=relu_optim=bgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/tps9f848' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/tps9f848</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124713-tps9f848/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af71adc0012d4553a9857954ce3d22bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124726-i29zqzw0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/i29zqzw0' target=\"_blank\">lr=0.001_act=relu_optim=mbgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/i29zqzw0' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/i29zqzw0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7872 - train_accuracy: 0.4151 - val_loss: 1.7870 - val_accuracy: 0.4070\n",
      "Epoch 2/100 - train_loss: 1.7826 - train_accuracy: 0.4137 - val_loss: 1.7822 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.7781 - train_accuracy: 0.4137 - val_loss: 1.7775 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.7736 - train_accuracy: 0.4137 - val_loss: 1.7729 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.7691 - train_accuracy: 0.4137 - val_loss: 1.7682 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.7647 - train_accuracy: 0.4137 - val_loss: 1.7637 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.7603 - train_accuracy: 0.4137 - val_loss: 1.7591 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.7560 - train_accuracy: 0.4137 - val_loss: 1.7546 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.7517 - train_accuracy: 0.4137 - val_loss: 1.7501 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.7474 - train_accuracy: 0.4137 - val_loss: 1.7457 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.7432 - train_accuracy: 0.4137 - val_loss: 1.7412 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.7389 - train_accuracy: 0.4137 - val_loss: 1.7369 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.7348 - train_accuracy: 0.4137 - val_loss: 1.7325 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.7306 - train_accuracy: 0.4137 - val_loss: 1.7282 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.7265 - train_accuracy: 0.4137 - val_loss: 1.7240 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.7225 - train_accuracy: 0.4137 - val_loss: 1.7197 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.7184 - train_accuracy: 0.4137 - val_loss: 1.7155 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.7144 - train_accuracy: 0.4137 - val_loss: 1.7114 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.7105 - train_accuracy: 0.4137 - val_loss: 1.7072 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.7065 - train_accuracy: 0.4137 - val_loss: 1.7031 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.7026 - train_accuracy: 0.4137 - val_loss: 1.6991 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.6988 - train_accuracy: 0.4137 - val_loss: 1.6950 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.6949 - train_accuracy: 0.4137 - val_loss: 1.6910 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.6911 - train_accuracy: 0.4137 - val_loss: 1.6871 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.6873 - train_accuracy: 0.4137 - val_loss: 1.6831 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.6836 - train_accuracy: 0.4137 - val_loss: 1.6792 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.6799 - train_accuracy: 0.4137 - val_loss: 1.6754 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.6762 - train_accuracy: 0.4137 - val_loss: 1.6715 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.6726 - train_accuracy: 0.4137 - val_loss: 1.6677 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.6689 - train_accuracy: 0.4137 - val_loss: 1.6639 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.6654 - train_accuracy: 0.4137 - val_loss: 1.6602 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.6618 - train_accuracy: 0.4137 - val_loss: 1.6565 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.6583 - train_accuracy: 0.4137 - val_loss: 1.6528 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.6548 - train_accuracy: 0.4137 - val_loss: 1.6492 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.6513 - train_accuracy: 0.4137 - val_loss: 1.6455 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.6479 - train_accuracy: 0.4137 - val_loss: 1.6420 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.6445 - train_accuracy: 0.4137 - val_loss: 1.6384 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.6411 - train_accuracy: 0.4137 - val_loss: 1.6349 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.6377 - train_accuracy: 0.4137 - val_loss: 1.6314 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.6344 - train_accuracy: 0.4137 - val_loss: 1.6279 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.6311 - train_accuracy: 0.4137 - val_loss: 1.6245 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.6279 - train_accuracy: 0.4137 - val_loss: 1.6210 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.6246 - train_accuracy: 0.4137 - val_loss: 1.6177 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.6214 - train_accuracy: 0.4137 - val_loss: 1.6143 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.6183 - train_accuracy: 0.4137 - val_loss: 1.6110 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.6151 - train_accuracy: 0.4137 - val_loss: 1.6077 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.6120 - train_accuracy: 0.4137 - val_loss: 1.6044 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.6089 - train_accuracy: 0.4137 - val_loss: 1.6012 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.6058 - train_accuracy: 0.4137 - val_loss: 1.5980 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.6028 - train_accuracy: 0.4137 - val_loss: 1.5948 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.5997 - train_accuracy: 0.4137 - val_loss: 1.5916 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.5967 - train_accuracy: 0.4137 - val_loss: 1.5885 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.5938 - train_accuracy: 0.4137 - val_loss: 1.5854 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.5908 - train_accuracy: 0.4137 - val_loss: 1.5823 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.5879 - train_accuracy: 0.4137 - val_loss: 1.5792 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.5850 - train_accuracy: 0.4137 - val_loss: 1.5762 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.5822 - train_accuracy: 0.4137 - val_loss: 1.5732 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.5793 - train_accuracy: 0.4137 - val_loss: 1.5702 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.5765 - train_accuracy: 0.4137 - val_loss: 1.5673 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.5737 - train_accuracy: 0.4137 - val_loss: 1.5643 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.5709 - train_accuracy: 0.4137 - val_loss: 1.5614 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.5682 - train_accuracy: 0.4137 - val_loss: 1.5585 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.5655 - train_accuracy: 0.4137 - val_loss: 1.5557 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.5628 - train_accuracy: 0.4137 - val_loss: 1.5529 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.5601 - train_accuracy: 0.4137 - val_loss: 1.5501 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.5575 - train_accuracy: 0.4137 - val_loss: 1.5473 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.5548 - train_accuracy: 0.4137 - val_loss: 1.5445 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.5522 - train_accuracy: 0.4137 - val_loss: 1.5418 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.5496 - train_accuracy: 0.4137 - val_loss: 1.5391 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.5471 - train_accuracy: 0.4137 - val_loss: 1.5364 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.5446 - train_accuracy: 0.4137 - val_loss: 1.5337 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.5420 - train_accuracy: 0.4137 - val_loss: 1.5311 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.5395 - train_accuracy: 0.4137 - val_loss: 1.5284 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.5371 - train_accuracy: 0.4137 - val_loss: 1.5258 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.5346 - train_accuracy: 0.4137 - val_loss: 1.5233 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.5322 - train_accuracy: 0.4137 - val_loss: 1.5207 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.5298 - train_accuracy: 0.4137 - val_loss: 1.5182 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.5274 - train_accuracy: 0.4137 - val_loss: 1.5157 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.5250 - train_accuracy: 0.4137 - val_loss: 1.5132 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.5227 - train_accuracy: 0.4137 - val_loss: 1.5107 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.5204 - train_accuracy: 0.4137 - val_loss: 1.5083 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.5181 - train_accuracy: 0.4137 - val_loss: 1.5058 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.5158 - train_accuracy: 0.4137 - val_loss: 1.5034 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.5135 - train_accuracy: 0.4137 - val_loss: 1.5010 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.5113 - train_accuracy: 0.4137 - val_loss: 1.4987 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.5090 - train_accuracy: 0.4137 - val_loss: 1.4963 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.5068 - train_accuracy: 0.4137 - val_loss: 1.4940 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.5046 - train_accuracy: 0.4137 - val_loss: 1.4917 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.5025 - train_accuracy: 0.4137 - val_loss: 1.4894 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.5003 - train_accuracy: 0.4137 - val_loss: 1.4872 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.4982 - train_accuracy: 0.4137 - val_loss: 1.4849 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.4961 - train_accuracy: 0.4137 - val_loss: 1.4827 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.4940 - train_accuracy: 0.4137 - val_loss: 1.4805 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.4919 - train_accuracy: 0.4137 - val_loss: 1.4783 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.4899 - train_accuracy: 0.4137 - val_loss: 1.4761 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.4878 - train_accuracy: 0.4137 - val_loss: 1.4739 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.4858 - train_accuracy: 0.4137 - val_loss: 1.4718 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.4838 - train_accuracy: 0.4137 - val_loss: 1.4697 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.4818 - train_accuracy: 0.4137 - val_loss: 1.4676 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.4798 - train_accuracy: 0.4137 - val_loss: 1.4655 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.001_act=relu_optim=mbgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/i29zqzw0' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/i29zqzw0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124726-i29zqzw0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b5fbac0fc04cd780932cf2c2d6176d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124735-sv512ab6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/sv512ab6' target=\"_blank\">lr=0.01_act=sigmoid_optim=sgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/sv512ab6' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/sv512ab6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.2082 - train_accuracy: 0.4111 - val_loss: 1.1715 - val_accuracy: 0.4419\n",
      "Epoch 2/100 - train_loss: 1.1894 - train_accuracy: 0.4111 - val_loss: 1.1565 - val_accuracy: 0.4419\n",
      "Epoch 3/100 - train_loss: 1.1845 - train_accuracy: 0.4111 - val_loss: 1.1560 - val_accuracy: 0.4419\n",
      "Epoch 4/100 - train_loss: 1.1824 - train_accuracy: 0.4111 - val_loss: 1.1576 - val_accuracy: 0.4419\n",
      "Epoch 5/100 - train_loss: 1.1812 - train_accuracy: 0.4111 - val_loss: 1.1595 - val_accuracy: 0.4419\n",
      "Epoch 6/100 - train_loss: 1.1804 - train_accuracy: 0.4111 - val_loss: 1.1613 - val_accuracy: 0.4419\n",
      "Epoch 7/100 - train_loss: 1.1798 - train_accuracy: 0.4111 - val_loss: 1.1629 - val_accuracy: 0.4419\n",
      "Epoch 8/100 - train_loss: 1.1793 - train_accuracy: 0.4111 - val_loss: 1.1643 - val_accuracy: 0.4419\n",
      "Epoch 9/100 - train_loss: 1.1790 - train_accuracy: 0.4111 - val_loss: 1.1655 - val_accuracy: 0.4419\n",
      "Epoch 10/100 - train_loss: 1.1787 - train_accuracy: 0.4111 - val_loss: 1.1666 - val_accuracy: 0.4419\n",
      "Epoch 11/100 - train_loss: 1.1784 - train_accuracy: 0.4111 - val_loss: 1.1676 - val_accuracy: 0.4419\n",
      "Epoch 12/100 - train_loss: 1.1781 - train_accuracy: 0.4111 - val_loss: 1.1684 - val_accuracy: 0.4419\n",
      "Epoch 13/100 - train_loss: 1.1779 - train_accuracy: 0.4111 - val_loss: 1.1692 - val_accuracy: 0.4419\n",
      "Epoch 14/100 - train_loss: 1.1777 - train_accuracy: 0.4111 - val_loss: 1.1699 - val_accuracy: 0.4419\n",
      "Epoch 15/100 - train_loss: 1.1775 - train_accuracy: 0.4111 - val_loss: 1.1706 - val_accuracy: 0.4419\n",
      "Epoch 16/100 - train_loss: 1.1772 - train_accuracy: 0.4111 - val_loss: 1.1712 - val_accuracy: 0.4419\n",
      "Epoch 17/100 - train_loss: 1.1770 - train_accuracy: 0.4111 - val_loss: 1.1717 - val_accuracy: 0.4419\n",
      "Epoch 18/100 - train_loss: 1.1767 - train_accuracy: 0.4111 - val_loss: 1.1722 - val_accuracy: 0.4419\n",
      "Epoch 19/100 - train_loss: 1.1764 - train_accuracy: 0.4111 - val_loss: 1.1726 - val_accuracy: 0.4419\n",
      "Epoch 20/100 - train_loss: 1.1761 - train_accuracy: 0.4111 - val_loss: 1.1729 - val_accuracy: 0.4419\n",
      "Epoch 21/100 - train_loss: 1.1757 - train_accuracy: 0.4111 - val_loss: 1.1732 - val_accuracy: 0.4419\n",
      "Epoch 22/100 - train_loss: 1.1753 - train_accuracy: 0.4111 - val_loss: 1.1735 - val_accuracy: 0.4419\n",
      "Epoch 23/100 - train_loss: 1.1748 - train_accuracy: 0.4111 - val_loss: 1.1736 - val_accuracy: 0.4419\n",
      "Epoch 24/100 - train_loss: 1.1741 - train_accuracy: 0.4111 - val_loss: 1.1737 - val_accuracy: 0.4419\n",
      "Epoch 25/100 - train_loss: 1.1734 - train_accuracy: 0.4111 - val_loss: 1.1736 - val_accuracy: 0.4419\n",
      "Epoch 26/100 - train_loss: 1.1724 - train_accuracy: 0.4111 - val_loss: 1.1734 - val_accuracy: 0.4419\n",
      "Epoch 27/100 - train_loss: 1.1713 - train_accuracy: 0.4111 - val_loss: 1.1730 - val_accuracy: 0.4419\n",
      "Epoch 28/100 - train_loss: 1.1699 - train_accuracy: 0.4111 - val_loss: 1.1724 - val_accuracy: 0.4419\n",
      "Epoch 29/100 - train_loss: 1.1681 - train_accuracy: 0.4111 - val_loss: 1.1716 - val_accuracy: 0.4419\n",
      "Epoch 30/100 - train_loss: 1.1659 - train_accuracy: 0.4111 - val_loss: 1.1703 - val_accuracy: 0.4419\n",
      "Epoch 31/100 - train_loss: 1.1630 - train_accuracy: 0.4111 - val_loss: 1.1686 - val_accuracy: 0.4419\n",
      "Epoch 32/100 - train_loss: 1.1595 - train_accuracy: 0.4111 - val_loss: 1.1664 - val_accuracy: 0.4419\n",
      "Epoch 33/100 - train_loss: 1.1551 - train_accuracy: 0.4097 - val_loss: 1.1634 - val_accuracy: 0.4419\n",
      "Epoch 34/100 - train_loss: 1.1496 - train_accuracy: 0.4137 - val_loss: 1.1595 - val_accuracy: 0.4419\n",
      "Epoch 35/100 - train_loss: 1.1429 - train_accuracy: 0.4407 - val_loss: 1.1547 - val_accuracy: 0.4593\n",
      "Epoch 36/100 - train_loss: 1.1348 - train_accuracy: 0.4609 - val_loss: 1.1487 - val_accuracy: 0.4826\n",
      "Epoch 37/100 - train_loss: 1.1252 - train_accuracy: 0.4852 - val_loss: 1.1415 - val_accuracy: 0.5058\n",
      "Epoch 38/100 - train_loss: 1.1142 - train_accuracy: 0.4973 - val_loss: 1.1331 - val_accuracy: 0.5349\n",
      "Epoch 39/100 - train_loss: 1.1019 - train_accuracy: 0.5094 - val_loss: 1.1236 - val_accuracy: 0.5407\n",
      "Epoch 40/100 - train_loss: 1.0887 - train_accuracy: 0.5350 - val_loss: 1.1133 - val_accuracy: 0.5407\n",
      "Epoch 41/100 - train_loss: 1.0748 - train_accuracy: 0.5431 - val_loss: 1.1025 - val_accuracy: 0.5523\n",
      "Epoch 42/100 - train_loss: 1.0608 - train_accuracy: 0.5539 - val_loss: 1.0916 - val_accuracy: 0.5465\n",
      "Epoch 43/100 - train_loss: 1.0472 - train_accuracy: 0.5647 - val_loss: 1.0810 - val_accuracy: 0.5640\n",
      "Epoch 44/100 - train_loss: 1.0344 - train_accuracy: 0.5701 - val_loss: 1.0712 - val_accuracy: 0.5581\n",
      "Epoch 45/100 - train_loss: 1.0228 - train_accuracy: 0.5660 - val_loss: 1.0623 - val_accuracy: 0.5640\n",
      "Epoch 46/100 - train_loss: 1.0124 - train_accuracy: 0.5674 - val_loss: 1.0545 - val_accuracy: 0.5640\n",
      "Epoch 47/100 - train_loss: 1.0033 - train_accuracy: 0.5714 - val_loss: 1.0478 - val_accuracy: 0.5640\n",
      "Epoch 48/100 - train_loss: 0.9955 - train_accuracy: 0.5755 - val_loss: 1.0422 - val_accuracy: 0.5581\n",
      "Epoch 49/100 - train_loss: 0.9888 - train_accuracy: 0.5795 - val_loss: 1.0375 - val_accuracy: 0.5581\n",
      "Epoch 50/100 - train_loss: 0.9831 - train_accuracy: 0.5849 - val_loss: 1.0335 - val_accuracy: 0.5698\n",
      "Epoch 51/100 - train_loss: 0.9783 - train_accuracy: 0.5889 - val_loss: 1.0303 - val_accuracy: 0.5814\n",
      "Epoch 52/100 - train_loss: 0.9741 - train_accuracy: 0.5930 - val_loss: 1.0275 - val_accuracy: 0.5814\n",
      "Epoch 53/100 - train_loss: 0.9705 - train_accuracy: 0.5930 - val_loss: 1.0252 - val_accuracy: 0.5930\n",
      "Epoch 54/100 - train_loss: 0.9674 - train_accuracy: 0.5957 - val_loss: 1.0231 - val_accuracy: 0.5930\n",
      "Epoch 55/100 - train_loss: 0.9647 - train_accuracy: 0.5943 - val_loss: 1.0214 - val_accuracy: 0.5930\n",
      "Epoch 56/100 - train_loss: 0.9622 - train_accuracy: 0.5930 - val_loss: 1.0198 - val_accuracy: 0.5930\n",
      "Epoch 57/100 - train_loss: 0.9600 - train_accuracy: 0.5984 - val_loss: 1.0184 - val_accuracy: 0.5930\n",
      "Epoch 58/100 - train_loss: 0.9581 - train_accuracy: 0.5984 - val_loss: 1.0171 - val_accuracy: 0.5872\n",
      "Epoch 59/100 - train_loss: 0.9563 - train_accuracy: 0.5997 - val_loss: 1.0159 - val_accuracy: 0.5988\n",
      "Epoch 60/100 - train_loss: 0.9546 - train_accuracy: 0.5984 - val_loss: 1.0148 - val_accuracy: 0.5930\n",
      "Epoch 61/100 - train_loss: 0.9531 - train_accuracy: 0.5984 - val_loss: 1.0137 - val_accuracy: 0.5930\n",
      "Epoch 62/100 - train_loss: 0.9516 - train_accuracy: 0.6011 - val_loss: 1.0128 - val_accuracy: 0.5930\n",
      "Epoch 63/100 - train_loss: 0.9503 - train_accuracy: 0.6011 - val_loss: 1.0119 - val_accuracy: 0.5930\n",
      "Epoch 64/100 - train_loss: 0.9491 - train_accuracy: 0.5984 - val_loss: 1.0111 - val_accuracy: 0.5930\n",
      "Epoch 65/100 - train_loss: 0.9479 - train_accuracy: 0.5970 - val_loss: 1.0104 - val_accuracy: 0.5988\n",
      "Epoch 66/100 - train_loss: 0.9468 - train_accuracy: 0.5997 - val_loss: 1.0097 - val_accuracy: 0.5988\n",
      "Epoch 67/100 - train_loss: 0.9457 - train_accuracy: 0.5997 - val_loss: 1.0092 - val_accuracy: 0.5988\n",
      "Epoch 68/100 - train_loss: 0.9448 - train_accuracy: 0.5997 - val_loss: 1.0087 - val_accuracy: 0.5988\n",
      "Epoch 69/100 - train_loss: 0.9439 - train_accuracy: 0.6011 - val_loss: 1.0083 - val_accuracy: 0.5988\n",
      "Epoch 70/100 - train_loss: 0.9430 - train_accuracy: 0.6011 - val_loss: 1.0080 - val_accuracy: 0.6047\n",
      "Epoch 71/100 - train_loss: 0.9422 - train_accuracy: 0.6011 - val_loss: 1.0077 - val_accuracy: 0.6047\n",
      "Epoch 72/100 - train_loss: 0.9415 - train_accuracy: 0.6024 - val_loss: 1.0075 - val_accuracy: 0.6047\n",
      "Epoch 73/100 - train_loss: 0.9408 - train_accuracy: 0.6038 - val_loss: 1.0073 - val_accuracy: 0.6047\n",
      "Epoch 74/100 - train_loss: 0.9401 - train_accuracy: 0.6038 - val_loss: 1.0072 - val_accuracy: 0.6047\n",
      "Epoch 75/100 - train_loss: 0.9395 - train_accuracy: 0.6038 - val_loss: 1.0071 - val_accuracy: 0.6047\n",
      "Epoch 76/100 - train_loss: 0.9389 - train_accuracy: 0.6038 - val_loss: 1.0070 - val_accuracy: 0.6105\n",
      "Epoch 77/100 - train_loss: 0.9383 - train_accuracy: 0.6038 - val_loss: 1.0070 - val_accuracy: 0.6105\n",
      "Epoch 78/100 - train_loss: 0.9378 - train_accuracy: 0.6038 - val_loss: 1.0070 - val_accuracy: 0.6105\n",
      "Epoch 79/100 - train_loss: 0.9373 - train_accuracy: 0.6038 - val_loss: 1.0070 - val_accuracy: 0.6105\n",
      "Epoch 80/100 - train_loss: 0.9368 - train_accuracy: 0.6038 - val_loss: 1.0070 - val_accuracy: 0.6105\n",
      "Epoch 81/100 - train_loss: 0.9363 - train_accuracy: 0.6051 - val_loss: 1.0070 - val_accuracy: 0.6163\n",
      "Epoch 82/100 - train_loss: 0.9359 - train_accuracy: 0.6105 - val_loss: 1.0070 - val_accuracy: 0.6163\n",
      "Epoch 83/100 - train_loss: 0.9354 - train_accuracy: 0.6119 - val_loss: 1.0070 - val_accuracy: 0.6163\n",
      "Epoch 84/100 - train_loss: 0.9350 - train_accuracy: 0.6119 - val_loss: 1.0070 - val_accuracy: 0.6105\n",
      "Epoch 85/100 - train_loss: 0.9346 - train_accuracy: 0.6105 - val_loss: 1.0071 - val_accuracy: 0.6047\n",
      "Epoch 86/100 - train_loss: 0.9342 - train_accuracy: 0.6105 - val_loss: 1.0071 - val_accuracy: 0.6047\n",
      "Epoch 87/100 - train_loss: 0.9338 - train_accuracy: 0.6092 - val_loss: 1.0071 - val_accuracy: 0.6105\n",
      "Epoch 88/100 - train_loss: 0.9334 - train_accuracy: 0.6105 - val_loss: 1.0071 - val_accuracy: 0.6163\n",
      "Epoch 89/100 - train_loss: 0.9330 - train_accuracy: 0.6119 - val_loss: 1.0071 - val_accuracy: 0.6163\n",
      "Epoch 90/100 - train_loss: 0.9326 - train_accuracy: 0.6119 - val_loss: 1.0071 - val_accuracy: 0.6163\n",
      "Epoch 91/100 - train_loss: 0.9322 - train_accuracy: 0.6119 - val_loss: 1.0071 - val_accuracy: 0.6163\n",
      "Epoch 92/100 - train_loss: 0.9318 - train_accuracy: 0.6119 - val_loss: 1.0071 - val_accuracy: 0.6163\n",
      "Epoch 93/100 - train_loss: 0.9314 - train_accuracy: 0.6105 - val_loss: 1.0071 - val_accuracy: 0.6163\n",
      "Epoch 94/100 - train_loss: 0.9310 - train_accuracy: 0.6092 - val_loss: 1.0071 - val_accuracy: 0.6163\n",
      "Epoch 95/100 - train_loss: 0.9306 - train_accuracy: 0.6105 - val_loss: 1.0072 - val_accuracy: 0.6163\n",
      "Epoch 96/100 - train_loss: 0.9302 - train_accuracy: 0.6105 - val_loss: 1.0072 - val_accuracy: 0.6163\n",
      "Epoch 97/100 - train_loss: 0.9298 - train_accuracy: 0.6119 - val_loss: 1.0072 - val_accuracy: 0.6163\n",
      "Epoch 98/100 - train_loss: 0.9294 - train_accuracy: 0.6119 - val_loss: 1.0073 - val_accuracy: 0.6163\n",
      "Epoch 99/100 - train_loss: 0.9290 - train_accuracy: 0.6119 - val_loss: 1.0073 - val_accuracy: 0.6163\n",
      "Epoch 100/100 - train_loss: 0.9286 - train_accuracy: 0.6119 - val_loss: 1.0074 - val_accuracy: 0.6163\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488107f0aeec4ec0a52612be11410861"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.01_act=sigmoid_optim=sgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/sv512ab6' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/sv512ab6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124735-sv512ab6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138ee38868ea487db11a79019d59631c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124748-22lc1rk5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/22lc1rk5' target=\"_blank\">lr=0.01_act=sigmoid_optim=bgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/22lc1rk5' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/22lc1rk5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7898 - train_accuracy: 0.4137 - val_loss: 1.7899 - val_accuracy: 0.4070\n",
      "Epoch 2/100 - train_loss: 1.7850 - train_accuracy: 0.4137 - val_loss: 1.7849 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.7803 - train_accuracy: 0.4137 - val_loss: 1.7801 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.7757 - train_accuracy: 0.4137 - val_loss: 1.7752 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.7711 - train_accuracy: 0.4137 - val_loss: 1.7705 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.7665 - train_accuracy: 0.4137 - val_loss: 1.7657 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.7620 - train_accuracy: 0.4137 - val_loss: 1.7610 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.7575 - train_accuracy: 0.4137 - val_loss: 1.7563 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.7530 - train_accuracy: 0.4137 - val_loss: 1.7517 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.7486 - train_accuracy: 0.4137 - val_loss: 1.7471 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.7442 - train_accuracy: 0.4137 - val_loss: 1.7425 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.7398 - train_accuracy: 0.4137 - val_loss: 1.7380 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.7355 - train_accuracy: 0.4137 - val_loss: 1.7335 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.7313 - train_accuracy: 0.4137 - val_loss: 1.7291 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.7270 - train_accuracy: 0.4137 - val_loss: 1.7246 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.7228 - train_accuracy: 0.4137 - val_loss: 1.7203 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.7186 - train_accuracy: 0.4137 - val_loss: 1.7159 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.7145 - train_accuracy: 0.4137 - val_loss: 1.7116 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.7104 - train_accuracy: 0.4137 - val_loss: 1.7074 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.7064 - train_accuracy: 0.4137 - val_loss: 1.7031 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.7023 - train_accuracy: 0.4137 - val_loss: 1.6989 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.6983 - train_accuracy: 0.4137 - val_loss: 1.6948 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.6944 - train_accuracy: 0.4137 - val_loss: 1.6907 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.6905 - train_accuracy: 0.4137 - val_loss: 1.6866 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.6866 - train_accuracy: 0.4137 - val_loss: 1.6825 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.6827 - train_accuracy: 0.4137 - val_loss: 1.6785 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.6789 - train_accuracy: 0.4137 - val_loss: 1.6745 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.6751 - train_accuracy: 0.4137 - val_loss: 1.6706 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.6713 - train_accuracy: 0.4137 - val_loss: 1.6666 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.6676 - train_accuracy: 0.4137 - val_loss: 1.6628 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.6639 - train_accuracy: 0.4137 - val_loss: 1.6589 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.6602 - train_accuracy: 0.4137 - val_loss: 1.6551 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.6566 - train_accuracy: 0.4137 - val_loss: 1.6513 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.6530 - train_accuracy: 0.4137 - val_loss: 1.6475 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.6494 - train_accuracy: 0.4137 - val_loss: 1.6438 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.6459 - train_accuracy: 0.4137 - val_loss: 1.6401 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.6424 - train_accuracy: 0.4137 - val_loss: 1.6364 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.6389 - train_accuracy: 0.4137 - val_loss: 1.6328 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.6355 - train_accuracy: 0.4137 - val_loss: 1.6292 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.6321 - train_accuracy: 0.4137 - val_loss: 1.6256 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.6287 - train_accuracy: 0.4137 - val_loss: 1.6221 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.6253 - train_accuracy: 0.4137 - val_loss: 1.6186 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.6220 - train_accuracy: 0.4137 - val_loss: 1.6151 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.6187 - train_accuracy: 0.4137 - val_loss: 1.6117 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.6154 - train_accuracy: 0.4137 - val_loss: 1.6082 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.6122 - train_accuracy: 0.4137 - val_loss: 1.6048 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.6090 - train_accuracy: 0.4137 - val_loss: 1.6015 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.6058 - train_accuracy: 0.4137 - val_loss: 1.5982 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.6026 - train_accuracy: 0.4137 - val_loss: 1.5948 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.5995 - train_accuracy: 0.4137 - val_loss: 1.5916 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.5964 - train_accuracy: 0.4137 - val_loss: 1.5883 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.5933 - train_accuracy: 0.4137 - val_loss: 1.5851 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.5903 - train_accuracy: 0.4137 - val_loss: 1.5819 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.5872 - train_accuracy: 0.4137 - val_loss: 1.5787 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.5842 - train_accuracy: 0.4137 - val_loss: 1.5756 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.5813 - train_accuracy: 0.4137 - val_loss: 1.5725 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.5783 - train_accuracy: 0.4137 - val_loss: 1.5694 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.5754 - train_accuracy: 0.4137 - val_loss: 1.5663 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.5725 - train_accuracy: 0.4137 - val_loss: 1.5633 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.5696 - train_accuracy: 0.4137 - val_loss: 1.5603 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.5668 - train_accuracy: 0.4137 - val_loss: 1.5573 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.5640 - train_accuracy: 0.4137 - val_loss: 1.5544 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.5612 - train_accuracy: 0.4137 - val_loss: 1.5514 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.5584 - train_accuracy: 0.4137 - val_loss: 1.5485 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.5557 - train_accuracy: 0.4137 - val_loss: 1.5456 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.5530 - train_accuracy: 0.4137 - val_loss: 1.5428 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.5503 - train_accuracy: 0.4137 - val_loss: 1.5399 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.5476 - train_accuracy: 0.4137 - val_loss: 1.5371 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.5449 - train_accuracy: 0.4137 - val_loss: 1.5344 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.5423 - train_accuracy: 0.4137 - val_loss: 1.5316 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.5397 - train_accuracy: 0.4137 - val_loss: 1.5289 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.5371 - train_accuracy: 0.4137 - val_loss: 1.5261 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.5346 - train_accuracy: 0.4137 - val_loss: 1.5235 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.5320 - train_accuracy: 0.4137 - val_loss: 1.5208 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.5295 - train_accuracy: 0.4137 - val_loss: 1.5181 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.5270 - train_accuracy: 0.4137 - val_loss: 1.5155 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.5246 - train_accuracy: 0.4137 - val_loss: 1.5129 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.5221 - train_accuracy: 0.4137 - val_loss: 1.5103 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.5197 - train_accuracy: 0.4137 - val_loss: 1.5078 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.5173 - train_accuracy: 0.4137 - val_loss: 1.5053 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.5149 - train_accuracy: 0.4137 - val_loss: 1.5027 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.5125 - train_accuracy: 0.4137 - val_loss: 1.5003 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.5102 - train_accuracy: 0.4137 - val_loss: 1.4978 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.5079 - train_accuracy: 0.4137 - val_loss: 1.4953 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.5056 - train_accuracy: 0.4137 - val_loss: 1.4929 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.5033 - train_accuracy: 0.4137 - val_loss: 1.4905 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.5010 - train_accuracy: 0.4137 - val_loss: 1.4881 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.4988 - train_accuracy: 0.4137 - val_loss: 1.4858 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.4965 - train_accuracy: 0.4137 - val_loss: 1.4834 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.4943 - train_accuracy: 0.4137 - val_loss: 1.4811 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.4922 - train_accuracy: 0.4137 - val_loss: 1.4788 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.4900 - train_accuracy: 0.4137 - val_loss: 1.4765 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.4878 - train_accuracy: 0.4137 - val_loss: 1.4742 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.4857 - train_accuracy: 0.4137 - val_loss: 1.4720 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.4836 - train_accuracy: 0.4137 - val_loss: 1.4698 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.4815 - train_accuracy: 0.4137 - val_loss: 1.4676 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.4794 - train_accuracy: 0.4137 - val_loss: 1.4654 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.4774 - train_accuracy: 0.4137 - val_loss: 1.4632 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.4754 - train_accuracy: 0.4137 - val_loss: 1.4610 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.4733 - train_accuracy: 0.4137 - val_loss: 1.4589 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.01_act=sigmoid_optim=bgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/22lc1rk5' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/22lc1rk5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124748-22lc1rk5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3e42cda17194eb9a89c201d0a76beda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124758-fr7r0eez</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/fr7r0eez' target=\"_blank\">lr=0.01_act=sigmoid_optim=mbgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/fr7r0eez' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/fr7r0eez</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.6838 - train_accuracy: 0.4111 - val_loss: 1.6793 - val_accuracy: 0.4419\n",
      "Epoch 2/100 - train_loss: 1.5995 - train_accuracy: 0.4111 - val_loss: 1.5912 - val_accuracy: 0.4419\n",
      "Epoch 3/100 - train_loss: 1.5312 - train_accuracy: 0.4137 - val_loss: 1.5196 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.4760 - train_accuracy: 0.4137 - val_loss: 1.4614 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.4313 - train_accuracy: 0.4137 - val_loss: 1.4140 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.3951 - train_accuracy: 0.4137 - val_loss: 1.3754 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.3656 - train_accuracy: 0.4137 - val_loss: 1.3437 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.3414 - train_accuracy: 0.4137 - val_loss: 1.3177 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.3215 - train_accuracy: 0.4137 - val_loss: 1.2962 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.3050 - train_accuracy: 0.4137 - val_loss: 1.2782 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.2912 - train_accuracy: 0.4137 - val_loss: 1.2632 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.2796 - train_accuracy: 0.4137 - val_loss: 1.2505 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.2699 - train_accuracy: 0.4137 - val_loss: 1.2397 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.2615 - train_accuracy: 0.4137 - val_loss: 1.2305 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.2543 - train_accuracy: 0.4137 - val_loss: 1.2226 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.2481 - train_accuracy: 0.4137 - val_loss: 1.2158 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.2427 - train_accuracy: 0.4137 - val_loss: 1.2098 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.2380 - train_accuracy: 0.4137 - val_loss: 1.2046 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.2338 - train_accuracy: 0.4137 - val_loss: 1.2000 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.2301 - train_accuracy: 0.4137 - val_loss: 1.1959 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.2268 - train_accuracy: 0.4137 - val_loss: 1.1923 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.2238 - train_accuracy: 0.4137 - val_loss: 1.1891 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.2211 - train_accuracy: 0.4137 - val_loss: 1.1862 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.2187 - train_accuracy: 0.4137 - val_loss: 1.1836 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.2165 - train_accuracy: 0.4137 - val_loss: 1.1813 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.2144 - train_accuracy: 0.4137 - val_loss: 1.1792 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.2126 - train_accuracy: 0.4137 - val_loss: 1.1772 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.2109 - train_accuracy: 0.4137 - val_loss: 1.1755 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.2093 - train_accuracy: 0.4137 - val_loss: 1.1739 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.2078 - train_accuracy: 0.4137 - val_loss: 1.1724 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.2065 - train_accuracy: 0.4137 - val_loss: 1.1711 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.2052 - train_accuracy: 0.4137 - val_loss: 1.1698 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.2040 - train_accuracy: 0.4137 - val_loss: 1.1687 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.2029 - train_accuracy: 0.4137 - val_loss: 1.1677 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.2019 - train_accuracy: 0.4137 - val_loss: 1.1667 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.2009 - train_accuracy: 0.4137 - val_loss: 1.1658 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.2000 - train_accuracy: 0.4137 - val_loss: 1.1650 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.1992 - train_accuracy: 0.4137 - val_loss: 1.1642 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.1984 - train_accuracy: 0.4137 - val_loss: 1.1635 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.1976 - train_accuracy: 0.4137 - val_loss: 1.1629 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.1969 - train_accuracy: 0.4137 - val_loss: 1.1623 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.1962 - train_accuracy: 0.4137 - val_loss: 1.1617 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.1956 - train_accuracy: 0.4137 - val_loss: 1.1612 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.1950 - train_accuracy: 0.4137 - val_loss: 1.1607 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.1944 - train_accuracy: 0.4137 - val_loss: 1.1603 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.1939 - train_accuracy: 0.4137 - val_loss: 1.1599 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.1934 - train_accuracy: 0.4137 - val_loss: 1.1595 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.1929 - train_accuracy: 0.4137 - val_loss: 1.1591 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.1924 - train_accuracy: 0.4137 - val_loss: 1.1588 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.1920 - train_accuracy: 0.4137 - val_loss: 1.1585 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.1915 - train_accuracy: 0.4137 - val_loss: 1.1582 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.1911 - train_accuracy: 0.4137 - val_loss: 1.1580 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.1908 - train_accuracy: 0.4137 - val_loss: 1.1577 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.1904 - train_accuracy: 0.4137 - val_loss: 1.1575 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.1900 - train_accuracy: 0.4137 - val_loss: 1.1573 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.1897 - train_accuracy: 0.4137 - val_loss: 1.1571 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.1894 - train_accuracy: 0.4137 - val_loss: 1.1569 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.1891 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.1888 - train_accuracy: 0.4137 - val_loss: 1.1566 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.1885 - train_accuracy: 0.4137 - val_loss: 1.1565 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.1882 - train_accuracy: 0.4137 - val_loss: 1.1563 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.1880 - train_accuracy: 0.4137 - val_loss: 1.1562 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.1877 - train_accuracy: 0.4137 - val_loss: 1.1561 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.1875 - train_accuracy: 0.4137 - val_loss: 1.1560 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.1873 - train_accuracy: 0.4137 - val_loss: 1.1559 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.1870 - train_accuracy: 0.4137 - val_loss: 1.1559 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.1868 - train_accuracy: 0.4137 - val_loss: 1.1558 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.1866 - train_accuracy: 0.4137 - val_loss: 1.1557 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.1864 - train_accuracy: 0.4137 - val_loss: 1.1557 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.1862 - train_accuracy: 0.4137 - val_loss: 1.1556 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.1860 - train_accuracy: 0.4137 - val_loss: 1.1556 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.1859 - train_accuracy: 0.4137 - val_loss: 1.1556 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.1857 - train_accuracy: 0.4137 - val_loss: 1.1555 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.1855 - train_accuracy: 0.4137 - val_loss: 1.1555 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.1854 - train_accuracy: 0.4137 - val_loss: 1.1555 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.1852 - train_accuracy: 0.4137 - val_loss: 1.1555 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.1851 - train_accuracy: 0.4137 - val_loss: 1.1554 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.1849 - train_accuracy: 0.4137 - val_loss: 1.1554 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.1848 - train_accuracy: 0.4137 - val_loss: 1.1554 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.1846 - train_accuracy: 0.4137 - val_loss: 1.1554 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.1845 - train_accuracy: 0.4137 - val_loss: 1.1554 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.1844 - train_accuracy: 0.4137 - val_loss: 1.1555 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.1842 - train_accuracy: 0.4137 - val_loss: 1.1555 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.1841 - train_accuracy: 0.4137 - val_loss: 1.1555 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.1840 - train_accuracy: 0.4137 - val_loss: 1.1555 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.1839 - train_accuracy: 0.4137 - val_loss: 1.1555 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.1838 - train_accuracy: 0.4137 - val_loss: 1.1555 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.1836 - train_accuracy: 0.4137 - val_loss: 1.1556 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.1835 - train_accuracy: 0.4137 - val_loss: 1.1556 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.1834 - train_accuracy: 0.4137 - val_loss: 1.1556 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.1833 - train_accuracy: 0.4137 - val_loss: 1.1557 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.1832 - train_accuracy: 0.4137 - val_loss: 1.1557 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.1831 - train_accuracy: 0.4137 - val_loss: 1.1557 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.1830 - train_accuracy: 0.4137 - val_loss: 1.1558 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.1830 - train_accuracy: 0.4137 - val_loss: 1.1558 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.1829 - train_accuracy: 0.4137 - val_loss: 1.1558 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.1828 - train_accuracy: 0.4137 - val_loss: 1.1559 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.1827 - train_accuracy: 0.4137 - val_loss: 1.1559 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.1826 - train_accuracy: 0.4137 - val_loss: 1.1560 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.1825 - train_accuracy: 0.4137 - val_loss: 1.1560 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207b94d73b444e778475283ee8badb92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.01_act=sigmoid_optim=mbgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/fr7r0eez' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/fr7r0eez</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124758-fr7r0eez/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5183b06f2606448a91d235ba56f89992"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124808-u84tpbwg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/u84tpbwg' target=\"_blank\">lr=0.01_act=tanh_optim=sgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/u84tpbwg' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/u84tpbwg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.2834 - train_accuracy: 0.4111 - val_loss: 1.2549 - val_accuracy: 0.4419\n",
      "Epoch 2/100 - train_loss: 1.2223 - train_accuracy: 0.4111 - val_loss: 1.1881 - val_accuracy: 0.4419\n",
      "Epoch 3/100 - train_loss: 1.2036 - train_accuracy: 0.4111 - val_loss: 1.1692 - val_accuracy: 0.4419\n",
      "Epoch 4/100 - train_loss: 1.1949 - train_accuracy: 0.4111 - val_loss: 1.1617 - val_accuracy: 0.4419\n",
      "Epoch 5/100 - train_loss: 1.1901 - train_accuracy: 0.4111 - val_loss: 1.1584 - val_accuracy: 0.4419\n",
      "Epoch 6/100 - train_loss: 1.1871 - train_accuracy: 0.4111 - val_loss: 1.1569 - val_accuracy: 0.4419\n",
      "Epoch 7/100 - train_loss: 1.1851 - train_accuracy: 0.4111 - val_loss: 1.1565 - val_accuracy: 0.4419\n",
      "Epoch 8/100 - train_loss: 1.1836 - train_accuracy: 0.4111 - val_loss: 1.1565 - val_accuracy: 0.4419\n",
      "Epoch 9/100 - train_loss: 1.1825 - train_accuracy: 0.4111 - val_loss: 1.1568 - val_accuracy: 0.4419\n",
      "Epoch 10/100 - train_loss: 1.1813 - train_accuracy: 0.4111 - val_loss: 1.1570 - val_accuracy: 0.4419\n",
      "Epoch 11/100 - train_loss: 1.1793 - train_accuracy: 0.4111 - val_loss: 1.1564 - val_accuracy: 0.4419\n",
      "Epoch 12/100 - train_loss: 1.1679 - train_accuracy: 0.4879 - val_loss: 1.1474 - val_accuracy: 0.5174\n",
      "Epoch 13/100 - train_loss: 1.0503 - train_accuracy: 0.5674 - val_loss: 1.0463 - val_accuracy: 0.5523\n",
      "Epoch 14/100 - train_loss: 0.9754 - train_accuracy: 0.5930 - val_loss: 0.9780 - val_accuracy: 0.5930\n",
      "Epoch 15/100 - train_loss: 0.9581 - train_accuracy: 0.5930 - val_loss: 0.9592 - val_accuracy: 0.6105\n",
      "Epoch 16/100 - train_loss: 0.9496 - train_accuracy: 0.5984 - val_loss: 0.9521 - val_accuracy: 0.6221\n",
      "Epoch 17/100 - train_loss: 0.9443 - train_accuracy: 0.6011 - val_loss: 0.9498 - val_accuracy: 0.6163\n",
      "Epoch 18/100 - train_loss: 0.9406 - train_accuracy: 0.6078 - val_loss: 0.9494 - val_accuracy: 0.6163\n",
      "Epoch 19/100 - train_loss: 0.9377 - train_accuracy: 0.6078 - val_loss: 0.9498 - val_accuracy: 0.5988\n",
      "Epoch 20/100 - train_loss: 0.9352 - train_accuracy: 0.6051 - val_loss: 0.9505 - val_accuracy: 0.6047\n",
      "Epoch 21/100 - train_loss: 0.9326 - train_accuracy: 0.6051 - val_loss: 0.9512 - val_accuracy: 0.6047\n",
      "Epoch 22/100 - train_loss: 0.9295 - train_accuracy: 0.6038 - val_loss: 0.9520 - val_accuracy: 0.6105\n",
      "Epoch 23/100 - train_loss: 0.9258 - train_accuracy: 0.6024 - val_loss: 0.9527 - val_accuracy: 0.5988\n",
      "Epoch 24/100 - train_loss: 0.9215 - train_accuracy: 0.6065 - val_loss: 0.9534 - val_accuracy: 0.6163\n",
      "Epoch 25/100 - train_loss: 0.9166 - train_accuracy: 0.6173 - val_loss: 0.9541 - val_accuracy: 0.6221\n",
      "Epoch 26/100 - train_loss: 0.9114 - train_accuracy: 0.6280 - val_loss: 0.9548 - val_accuracy: 0.6163\n",
      "Epoch 27/100 - train_loss: 0.9061 - train_accuracy: 0.6388 - val_loss: 0.9554 - val_accuracy: 0.6163\n",
      "Epoch 28/100 - train_loss: 0.9006 - train_accuracy: 0.6388 - val_loss: 0.9558 - val_accuracy: 0.6221\n",
      "Epoch 29/100 - train_loss: 0.8951 - train_accuracy: 0.6402 - val_loss: 0.9562 - val_accuracy: 0.6337\n",
      "Epoch 30/100 - train_loss: 0.8896 - train_accuracy: 0.6402 - val_loss: 0.9564 - val_accuracy: 0.6395\n",
      "Epoch 31/100 - train_loss: 0.8842 - train_accuracy: 0.6388 - val_loss: 0.9567 - val_accuracy: 0.6279\n",
      "Epoch 32/100 - train_loss: 0.8790 - train_accuracy: 0.6415 - val_loss: 0.9570 - val_accuracy: 0.6163\n",
      "Epoch 33/100 - train_loss: 0.8741 - train_accuracy: 0.6482 - val_loss: 0.9574 - val_accuracy: 0.6163\n",
      "Epoch 34/100 - train_loss: 0.8694 - train_accuracy: 0.6496 - val_loss: 0.9580 - val_accuracy: 0.6105\n",
      "Epoch 35/100 - train_loss: 0.8650 - train_accuracy: 0.6509 - val_loss: 0.9587 - val_accuracy: 0.6047\n",
      "Epoch 36/100 - train_loss: 0.8608 - train_accuracy: 0.6496 - val_loss: 0.9596 - val_accuracy: 0.5988\n",
      "Epoch 37/100 - train_loss: 0.8570 - train_accuracy: 0.6536 - val_loss: 0.9606 - val_accuracy: 0.6047\n",
      "Epoch 38/100 - train_loss: 0.8536 - train_accuracy: 0.6509 - val_loss: 0.9619 - val_accuracy: 0.6163\n",
      "Epoch 39/100 - train_loss: 0.8504 - train_accuracy: 0.6509 - val_loss: 0.9631 - val_accuracy: 0.6105\n",
      "Epoch 40/100 - train_loss: 0.8475 - train_accuracy: 0.6590 - val_loss: 0.9643 - val_accuracy: 0.6105\n",
      "Epoch 41/100 - train_loss: 0.8449 - train_accuracy: 0.6590 - val_loss: 0.9654 - val_accuracy: 0.6105\n",
      "Epoch 42/100 - train_loss: 0.8424 - train_accuracy: 0.6577 - val_loss: 0.9663 - val_accuracy: 0.6105\n",
      "Epoch 43/100 - train_loss: 0.8401 - train_accuracy: 0.6577 - val_loss: 0.9671 - val_accuracy: 0.6105\n",
      "Epoch 44/100 - train_loss: 0.8378 - train_accuracy: 0.6590 - val_loss: 0.9678 - val_accuracy: 0.6163\n",
      "Epoch 45/100 - train_loss: 0.8356 - train_accuracy: 0.6604 - val_loss: 0.9683 - val_accuracy: 0.6163\n",
      "Epoch 46/100 - train_loss: 0.8335 - train_accuracy: 0.6617 - val_loss: 0.9686 - val_accuracy: 0.6105\n",
      "Epoch 47/100 - train_loss: 0.8313 - train_accuracy: 0.6698 - val_loss: 0.9688 - val_accuracy: 0.6163\n",
      "Epoch 48/100 - train_loss: 0.8291 - train_accuracy: 0.6685 - val_loss: 0.9689 - val_accuracy: 0.6163\n",
      "Epoch 49/100 - train_loss: 0.8269 - train_accuracy: 0.6644 - val_loss: 0.9690 - val_accuracy: 0.6221\n",
      "Epoch 50/100 - train_loss: 0.8246 - train_accuracy: 0.6617 - val_loss: 0.9692 - val_accuracy: 0.6105\n",
      "Epoch 51/100 - train_loss: 0.8222 - train_accuracy: 0.6658 - val_loss: 0.9694 - val_accuracy: 0.6105\n",
      "Epoch 52/100 - train_loss: 0.8198 - train_accuracy: 0.6617 - val_loss: 0.9698 - val_accuracy: 0.6105\n",
      "Epoch 53/100 - train_loss: 0.8174 - train_accuracy: 0.6631 - val_loss: 0.9702 - val_accuracy: 0.6105\n",
      "Epoch 54/100 - train_loss: 0.8149 - train_accuracy: 0.6658 - val_loss: 0.9707 - val_accuracy: 0.6047\n",
      "Epoch 55/100 - train_loss: 0.8124 - train_accuracy: 0.6671 - val_loss: 0.9713 - val_accuracy: 0.6105\n",
      "Epoch 56/100 - train_loss: 0.8098 - train_accuracy: 0.6698 - val_loss: 0.9719 - val_accuracy: 0.6105\n",
      "Epoch 57/100 - train_loss: 0.8073 - train_accuracy: 0.6725 - val_loss: 0.9727 - val_accuracy: 0.6163\n",
      "Epoch 58/100 - train_loss: 0.8048 - train_accuracy: 0.6712 - val_loss: 0.9737 - val_accuracy: 0.6163\n",
      "Epoch 59/100 - train_loss: 0.8024 - train_accuracy: 0.6752 - val_loss: 0.9748 - val_accuracy: 0.6163\n",
      "Epoch 60/100 - train_loss: 0.8000 - train_accuracy: 0.6779 - val_loss: 0.9760 - val_accuracy: 0.6163\n",
      "Epoch 61/100 - train_loss: 0.7976 - train_accuracy: 0.6806 - val_loss: 0.9773 - val_accuracy: 0.6105\n",
      "Epoch 62/100 - train_loss: 0.7954 - train_accuracy: 0.6806 - val_loss: 0.9785 - val_accuracy: 0.6163\n",
      "Epoch 63/100 - train_loss: 0.7932 - train_accuracy: 0.6833 - val_loss: 0.9798 - val_accuracy: 0.6279\n",
      "Epoch 64/100 - train_loss: 0.7912 - train_accuracy: 0.6860 - val_loss: 0.9811 - val_accuracy: 0.6279\n",
      "Epoch 65/100 - train_loss: 0.7892 - train_accuracy: 0.6887 - val_loss: 0.9824 - val_accuracy: 0.6279\n",
      "Epoch 66/100 - train_loss: 0.7872 - train_accuracy: 0.6927 - val_loss: 0.9837 - val_accuracy: 0.6279\n",
      "Epoch 67/100 - train_loss: 0.7854 - train_accuracy: 0.6968 - val_loss: 0.9850 - val_accuracy: 0.6279\n",
      "Epoch 68/100 - train_loss: 0.7836 - train_accuracy: 0.6941 - val_loss: 0.9863 - val_accuracy: 0.6279\n",
      "Epoch 69/100 - train_loss: 0.7818 - train_accuracy: 0.6941 - val_loss: 0.9876 - val_accuracy: 0.6279\n",
      "Epoch 70/100 - train_loss: 0.7801 - train_accuracy: 0.6941 - val_loss: 0.9889 - val_accuracy: 0.6221\n",
      "Epoch 71/100 - train_loss: 0.7785 - train_accuracy: 0.6927 - val_loss: 0.9903 - val_accuracy: 0.6221\n",
      "Epoch 72/100 - train_loss: 0.7769 - train_accuracy: 0.6927 - val_loss: 0.9918 - val_accuracy: 0.6221\n",
      "Epoch 73/100 - train_loss: 0.7754 - train_accuracy: 0.6927 - val_loss: 0.9933 - val_accuracy: 0.6221\n",
      "Epoch 74/100 - train_loss: 0.7739 - train_accuracy: 0.6927 - val_loss: 0.9948 - val_accuracy: 0.6221\n",
      "Epoch 75/100 - train_loss: 0.7724 - train_accuracy: 0.6914 - val_loss: 0.9964 - val_accuracy: 0.6279\n",
      "Epoch 76/100 - train_loss: 0.7710 - train_accuracy: 0.6914 - val_loss: 0.9981 - val_accuracy: 0.6221\n",
      "Epoch 77/100 - train_loss: 0.7695 - train_accuracy: 0.6927 - val_loss: 0.9999 - val_accuracy: 0.6221\n",
      "Epoch 78/100 - train_loss: 0.7682 - train_accuracy: 0.6927 - val_loss: 1.0017 - val_accuracy: 0.6221\n",
      "Epoch 79/100 - train_loss: 0.7668 - train_accuracy: 0.6941 - val_loss: 1.0036 - val_accuracy: 0.6221\n",
      "Epoch 80/100 - train_loss: 0.7655 - train_accuracy: 0.6995 - val_loss: 1.0055 - val_accuracy: 0.6221\n",
      "Epoch 81/100 - train_loss: 0.7641 - train_accuracy: 0.6981 - val_loss: 1.0074 - val_accuracy: 0.6221\n",
      "Epoch 82/100 - train_loss: 0.7628 - train_accuracy: 0.6995 - val_loss: 1.0094 - val_accuracy: 0.6221\n",
      "Epoch 83/100 - train_loss: 0.7616 - train_accuracy: 0.6995 - val_loss: 1.0113 - val_accuracy: 0.6279\n",
      "Epoch 84/100 - train_loss: 0.7603 - train_accuracy: 0.6995 - val_loss: 1.0131 - val_accuracy: 0.6221\n",
      "Epoch 85/100 - train_loss: 0.7590 - train_accuracy: 0.7022 - val_loss: 1.0148 - val_accuracy: 0.6221\n",
      "Epoch 86/100 - train_loss: 0.7578 - train_accuracy: 0.7049 - val_loss: 1.0165 - val_accuracy: 0.6221\n",
      "Epoch 87/100 - train_loss: 0.7565 - train_accuracy: 0.7089 - val_loss: 1.0181 - val_accuracy: 0.6221\n",
      "Epoch 88/100 - train_loss: 0.7553 - train_accuracy: 0.7129 - val_loss: 1.0197 - val_accuracy: 0.6221\n",
      "Epoch 89/100 - train_loss: 0.7541 - train_accuracy: 0.7129 - val_loss: 1.0212 - val_accuracy: 0.6163\n",
      "Epoch 90/100 - train_loss: 0.7529 - train_accuracy: 0.7156 - val_loss: 1.0226 - val_accuracy: 0.6163\n",
      "Epoch 91/100 - train_loss: 0.7517 - train_accuracy: 0.7170 - val_loss: 1.0241 - val_accuracy: 0.6105\n",
      "Epoch 92/100 - train_loss: 0.7505 - train_accuracy: 0.7183 - val_loss: 1.0255 - val_accuracy: 0.6105\n",
      "Epoch 93/100 - train_loss: 0.7493 - train_accuracy: 0.7183 - val_loss: 1.0268 - val_accuracy: 0.6047\n",
      "Epoch 94/100 - train_loss: 0.7482 - train_accuracy: 0.7183 - val_loss: 1.0282 - val_accuracy: 0.6047\n",
      "Epoch 95/100 - train_loss: 0.7470 - train_accuracy: 0.7183 - val_loss: 1.0296 - val_accuracy: 0.6047\n",
      "Epoch 96/100 - train_loss: 0.7459 - train_accuracy: 0.7197 - val_loss: 1.0310 - val_accuracy: 0.6047\n",
      "Epoch 97/100 - train_loss: 0.7447 - train_accuracy: 0.7210 - val_loss: 1.0324 - val_accuracy: 0.6047\n",
      "Epoch 98/100 - train_loss: 0.7436 - train_accuracy: 0.7210 - val_loss: 1.0339 - val_accuracy: 0.6047\n",
      "Epoch 99/100 - train_loss: 0.7425 - train_accuracy: 0.7197 - val_loss: 1.0354 - val_accuracy: 0.6047\n",
      "Epoch 100/100 - train_loss: 0.7414 - train_accuracy: 0.7210 - val_loss: 1.0369 - val_accuracy: 0.6047\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d47822c0cb9640bbb7f2d7609f28ede7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.01_act=tanh_optim=sgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/u84tpbwg' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/u84tpbwg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124808-u84tpbwg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6050be701f2a44f698b3cb4d26a053aa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124821-89lyotj0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/89lyotj0' target=\"_blank\">lr=0.01_act=tanh_optim=bgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/89lyotj0' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/89lyotj0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7899 - train_accuracy: 0.4191 - val_loss: 1.7898 - val_accuracy: 0.4593\n",
      "Epoch 2/100 - train_loss: 1.7880 - train_accuracy: 0.4151 - val_loss: 1.7878 - val_accuracy: 0.4128\n",
      "Epoch 3/100 - train_loss: 1.7861 - train_accuracy: 0.4137 - val_loss: 1.7858 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.7842 - train_accuracy: 0.4137 - val_loss: 1.7839 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.7823 - train_accuracy: 0.4137 - val_loss: 1.7819 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.7804 - train_accuracy: 0.4137 - val_loss: 1.7800 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.7786 - train_accuracy: 0.4137 - val_loss: 1.7781 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.7767 - train_accuracy: 0.4137 - val_loss: 1.7761 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.7748 - train_accuracy: 0.4137 - val_loss: 1.7742 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.7730 - train_accuracy: 0.4137 - val_loss: 1.7723 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.7712 - train_accuracy: 0.4137 - val_loss: 1.7704 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.7693 - train_accuracy: 0.4137 - val_loss: 1.7685 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.7675 - train_accuracy: 0.4137 - val_loss: 1.7666 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.7657 - train_accuracy: 0.4137 - val_loss: 1.7647 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.7638 - train_accuracy: 0.4137 - val_loss: 1.7628 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.7620 - train_accuracy: 0.4137 - val_loss: 1.7609 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.7602 - train_accuracy: 0.4137 - val_loss: 1.7590 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.7584 - train_accuracy: 0.4137 - val_loss: 1.7571 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.7566 - train_accuracy: 0.4137 - val_loss: 1.7553 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.7549 - train_accuracy: 0.4137 - val_loss: 1.7534 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.7531 - train_accuracy: 0.4137 - val_loss: 1.7516 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.7513 - train_accuracy: 0.4137 - val_loss: 1.7497 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.7495 - train_accuracy: 0.4137 - val_loss: 1.7479 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.7478 - train_accuracy: 0.4137 - val_loss: 1.7461 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.7460 - train_accuracy: 0.4137 - val_loss: 1.7442 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.7443 - train_accuracy: 0.4137 - val_loss: 1.7424 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.7425 - train_accuracy: 0.4137 - val_loss: 1.7406 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.7408 - train_accuracy: 0.4137 - val_loss: 1.7388 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.7390 - train_accuracy: 0.4137 - val_loss: 1.7370 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.7373 - train_accuracy: 0.4137 - val_loss: 1.7352 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.7356 - train_accuracy: 0.4137 - val_loss: 1.7334 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.7339 - train_accuracy: 0.4137 - val_loss: 1.7316 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.7322 - train_accuracy: 0.4137 - val_loss: 1.7298 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.7305 - train_accuracy: 0.4137 - val_loss: 1.7281 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.7288 - train_accuracy: 0.4137 - val_loss: 1.7263 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.7271 - train_accuracy: 0.4137 - val_loss: 1.7245 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.7254 - train_accuracy: 0.4137 - val_loss: 1.7228 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.7237 - train_accuracy: 0.4137 - val_loss: 1.7210 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.7220 - train_accuracy: 0.4137 - val_loss: 1.7193 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.7204 - train_accuracy: 0.4137 - val_loss: 1.7176 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.7187 - train_accuracy: 0.4137 - val_loss: 1.7158 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.7171 - train_accuracy: 0.4137 - val_loss: 1.7141 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.7154 - train_accuracy: 0.4137 - val_loss: 1.7124 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.7138 - train_accuracy: 0.4137 - val_loss: 1.7107 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.7121 - train_accuracy: 0.4137 - val_loss: 1.7090 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.7105 - train_accuracy: 0.4137 - val_loss: 1.7073 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.7089 - train_accuracy: 0.4137 - val_loss: 1.7056 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.7072 - train_accuracy: 0.4137 - val_loss: 1.7039 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.7056 - train_accuracy: 0.4137 - val_loss: 1.7022 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.7040 - train_accuracy: 0.4137 - val_loss: 1.7005 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.7024 - train_accuracy: 0.4137 - val_loss: 1.6989 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.7008 - train_accuracy: 0.4137 - val_loss: 1.6972 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.6992 - train_accuracy: 0.4137 - val_loss: 1.6955 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.6976 - train_accuracy: 0.4137 - val_loss: 1.6939 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.6960 - train_accuracy: 0.4137 - val_loss: 1.6922 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.6944 - train_accuracy: 0.4137 - val_loss: 1.6906 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.6929 - train_accuracy: 0.4137 - val_loss: 1.6889 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.6913 - train_accuracy: 0.4137 - val_loss: 1.6873 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.6897 - train_accuracy: 0.4137 - val_loss: 1.6857 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.6882 - train_accuracy: 0.4137 - val_loss: 1.6841 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.6866 - train_accuracy: 0.4137 - val_loss: 1.6824 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.6851 - train_accuracy: 0.4137 - val_loss: 1.6808 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.6836 - train_accuracy: 0.4137 - val_loss: 1.6792 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.6820 - train_accuracy: 0.4137 - val_loss: 1.6776 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.6805 - train_accuracy: 0.4137 - val_loss: 1.6760 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.6790 - train_accuracy: 0.4137 - val_loss: 1.6744 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.6774 - train_accuracy: 0.4137 - val_loss: 1.6729 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.6759 - train_accuracy: 0.4137 - val_loss: 1.6713 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.6744 - train_accuracy: 0.4137 - val_loss: 1.6697 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.6729 - train_accuracy: 0.4137 - val_loss: 1.6681 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.6714 - train_accuracy: 0.4137 - val_loss: 1.6666 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.6699 - train_accuracy: 0.4137 - val_loss: 1.6650 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.6684 - train_accuracy: 0.4137 - val_loss: 1.6635 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.6670 - train_accuracy: 0.4137 - val_loss: 1.6619 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.6655 - train_accuracy: 0.4137 - val_loss: 1.6604 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.6640 - train_accuracy: 0.4137 - val_loss: 1.6589 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.6625 - train_accuracy: 0.4137 - val_loss: 1.6573 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.6611 - train_accuracy: 0.4137 - val_loss: 1.6558 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.6596 - train_accuracy: 0.4137 - val_loss: 1.6543 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.6582 - train_accuracy: 0.4137 - val_loss: 1.6528 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.6567 - train_accuracy: 0.4137 - val_loss: 1.6513 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.6553 - train_accuracy: 0.4137 - val_loss: 1.6498 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.6539 - train_accuracy: 0.4137 - val_loss: 1.6483 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.6524 - train_accuracy: 0.4137 - val_loss: 1.6468 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.6510 - train_accuracy: 0.4137 - val_loss: 1.6453 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.6496 - train_accuracy: 0.4137 - val_loss: 1.6438 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.6482 - train_accuracy: 0.4137 - val_loss: 1.6423 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.6468 - train_accuracy: 0.4137 - val_loss: 1.6409 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.6454 - train_accuracy: 0.4137 - val_loss: 1.6394 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.6440 - train_accuracy: 0.4137 - val_loss: 1.6379 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.6426 - train_accuracy: 0.4137 - val_loss: 1.6365 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.6412 - train_accuracy: 0.4137 - val_loss: 1.6350 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.6398 - train_accuracy: 0.4137 - val_loss: 1.6336 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.6384 - train_accuracy: 0.4137 - val_loss: 1.6321 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.6370 - train_accuracy: 0.4137 - val_loss: 1.6307 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.6357 - train_accuracy: 0.4137 - val_loss: 1.6293 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.6343 - train_accuracy: 0.4137 - val_loss: 1.6278 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.6329 - train_accuracy: 0.4137 - val_loss: 1.6264 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.6316 - train_accuracy: 0.4137 - val_loss: 1.6250 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.6302 - train_accuracy: 0.4137 - val_loss: 1.6236 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.01_act=tanh_optim=bgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/89lyotj0' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/89lyotj0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124821-89lyotj0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf48faa655f4d778b2a657d87825fad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124830-g9w34tuf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/g9w34tuf' target=\"_blank\">lr=0.01_act=tanh_optim=mbgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/g9w34tuf' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/g9w34tuf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7473 - train_accuracy: 0.4137 - val_loss: 1.7456 - val_accuracy: 0.4070\n",
      "Epoch 2/100 - train_loss: 1.7064 - train_accuracy: 0.4137 - val_loss: 1.7030 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.6688 - train_accuracy: 0.4137 - val_loss: 1.6638 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.6343 - train_accuracy: 0.4137 - val_loss: 1.6277 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.6026 - train_accuracy: 0.4137 - val_loss: 1.5946 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.5735 - train_accuracy: 0.4137 - val_loss: 1.5641 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.5469 - train_accuracy: 0.4137 - val_loss: 1.5361 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.5225 - train_accuracy: 0.4137 - val_loss: 1.5105 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.5001 - train_accuracy: 0.4137 - val_loss: 1.4869 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.4796 - train_accuracy: 0.4137 - val_loss: 1.4652 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.4608 - train_accuracy: 0.4137 - val_loss: 1.4453 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.4435 - train_accuracy: 0.4137 - val_loss: 1.4271 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.4277 - train_accuracy: 0.4137 - val_loss: 1.4102 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.4132 - train_accuracy: 0.4137 - val_loss: 1.3948 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.3998 - train_accuracy: 0.4137 - val_loss: 1.3805 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.3875 - train_accuracy: 0.4137 - val_loss: 1.3673 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.3761 - train_accuracy: 0.4137 - val_loss: 1.3552 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.3656 - train_accuracy: 0.4137 - val_loss: 1.3439 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.3560 - train_accuracy: 0.4137 - val_loss: 1.3335 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.3470 - train_accuracy: 0.4137 - val_loss: 1.3239 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.3388 - train_accuracy: 0.4137 - val_loss: 1.3150 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.3311 - train_accuracy: 0.4137 - val_loss: 1.3067 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.3240 - train_accuracy: 0.4137 - val_loss: 1.2990 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.3174 - train_accuracy: 0.4137 - val_loss: 1.2918 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.3112 - train_accuracy: 0.4137 - val_loss: 1.2851 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.3055 - train_accuracy: 0.4137 - val_loss: 1.2789 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.3001 - train_accuracy: 0.4137 - val_loss: 1.2730 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.2951 - train_accuracy: 0.4137 - val_loss: 1.2676 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.2905 - train_accuracy: 0.4137 - val_loss: 1.2625 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.2861 - train_accuracy: 0.4137 - val_loss: 1.2577 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.2820 - train_accuracy: 0.4137 - val_loss: 1.2532 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.2781 - train_accuracy: 0.4137 - val_loss: 1.2490 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.2745 - train_accuracy: 0.4137 - val_loss: 1.2450 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.2711 - train_accuracy: 0.4137 - val_loss: 1.2413 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.2679 - train_accuracy: 0.4137 - val_loss: 1.2377 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.2649 - train_accuracy: 0.4137 - val_loss: 1.2344 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.2620 - train_accuracy: 0.4137 - val_loss: 1.2313 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.2593 - train_accuracy: 0.4137 - val_loss: 1.2283 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.2568 - train_accuracy: 0.4137 - val_loss: 1.2255 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.2544 - train_accuracy: 0.4137 - val_loss: 1.2228 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.2521 - train_accuracy: 0.4137 - val_loss: 1.2203 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.2499 - train_accuracy: 0.4137 - val_loss: 1.2179 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.2478 - train_accuracy: 0.4137 - val_loss: 1.2156 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.2458 - train_accuracy: 0.4137 - val_loss: 1.2135 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.2440 - train_accuracy: 0.4137 - val_loss: 1.2114 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.2422 - train_accuracy: 0.4137 - val_loss: 1.2094 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.2405 - train_accuracy: 0.4137 - val_loss: 1.2076 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.2389 - train_accuracy: 0.4137 - val_loss: 1.2058 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.2373 - train_accuracy: 0.4137 - val_loss: 1.2041 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.2358 - train_accuracy: 0.4137 - val_loss: 1.2024 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.2344 - train_accuracy: 0.4137 - val_loss: 1.2009 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.2330 - train_accuracy: 0.4137 - val_loss: 1.1994 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.2317 - train_accuracy: 0.4137 - val_loss: 1.1980 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.2305 - train_accuracy: 0.4137 - val_loss: 1.1966 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.2293 - train_accuracy: 0.4137 - val_loss: 1.1953 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.2281 - train_accuracy: 0.4137 - val_loss: 1.1940 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.2270 - train_accuracy: 0.4137 - val_loss: 1.1928 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.2259 - train_accuracy: 0.4137 - val_loss: 1.1917 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.2249 - train_accuracy: 0.4137 - val_loss: 1.1906 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.2239 - train_accuracy: 0.4137 - val_loss: 1.1895 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.2229 - train_accuracy: 0.4137 - val_loss: 1.1885 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.2220 - train_accuracy: 0.4137 - val_loss: 1.1875 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.2211 - train_accuracy: 0.4137 - val_loss: 1.1865 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.2202 - train_accuracy: 0.4137 - val_loss: 1.1856 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.2194 - train_accuracy: 0.4137 - val_loss: 1.1847 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.2186 - train_accuracy: 0.4137 - val_loss: 1.1839 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.2178 - train_accuracy: 0.4137 - val_loss: 1.1830 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.2170 - train_accuracy: 0.4137 - val_loss: 1.1822 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.2163 - train_accuracy: 0.4137 - val_loss: 1.1815 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.2156 - train_accuracy: 0.4137 - val_loss: 1.1807 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.2149 - train_accuracy: 0.4137 - val_loss: 1.1800 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.2142 - train_accuracy: 0.4137 - val_loss: 1.1793 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.2136 - train_accuracy: 0.4137 - val_loss: 1.1786 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.2130 - train_accuracy: 0.4137 - val_loss: 1.1780 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.2123 - train_accuracy: 0.4137 - val_loss: 1.1774 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.2117 - train_accuracy: 0.4137 - val_loss: 1.1768 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.2112 - train_accuracy: 0.4137 - val_loss: 1.1762 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.2106 - train_accuracy: 0.4137 - val_loss: 1.1756 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.2101 - train_accuracy: 0.4137 - val_loss: 1.1750 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.2095 - train_accuracy: 0.4137 - val_loss: 1.1745 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.2090 - train_accuracy: 0.4137 - val_loss: 1.1740 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.2085 - train_accuracy: 0.4137 - val_loss: 1.1735 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.2080 - train_accuracy: 0.4137 - val_loss: 1.1730 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.2075 - train_accuracy: 0.4137 - val_loss: 1.1725 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.2071 - train_accuracy: 0.4137 - val_loss: 1.1721 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.2066 - train_accuracy: 0.4137 - val_loss: 1.1716 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.2062 - train_accuracy: 0.4137 - val_loss: 1.1712 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.2057 - train_accuracy: 0.4137 - val_loss: 1.1708 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.2053 - train_accuracy: 0.4137 - val_loss: 1.1703 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.2049 - train_accuracy: 0.4137 - val_loss: 1.1699 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.2045 - train_accuracy: 0.4137 - val_loss: 1.1696 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.2041 - train_accuracy: 0.4137 - val_loss: 1.1692 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.2037 - train_accuracy: 0.4137 - val_loss: 1.1688 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.2033 - train_accuracy: 0.4137 - val_loss: 1.1685 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.2030 - train_accuracy: 0.4137 - val_loss: 1.1681 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.2026 - train_accuracy: 0.4137 - val_loss: 1.1678 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.2023 - train_accuracy: 0.4137 - val_loss: 1.1675 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.2019 - train_accuracy: 0.4137 - val_loss: 1.1671 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.2016 - train_accuracy: 0.4137 - val_loss: 1.1668 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.2012 - train_accuracy: 0.4137 - val_loss: 1.1665 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.01_act=tanh_optim=mbgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/g9w34tuf' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/g9w34tuf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124830-g9w34tuf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4f6df8f38b44ea8dd63676d72460ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124841-49peq582</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/49peq582' target=\"_blank\">lr=0.01_act=relu_optim=sgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/49peq582' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/49peq582</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.2835 - train_accuracy: 0.4111 - val_loss: 1.2550 - val_accuracy: 0.4419\n",
      "Epoch 2/100 - train_loss: 1.2224 - train_accuracy: 0.4111 - val_loss: 1.1883 - val_accuracy: 0.4419\n",
      "Epoch 3/100 - train_loss: 1.2038 - train_accuracy: 0.4111 - val_loss: 1.1694 - val_accuracy: 0.4419\n",
      "Epoch 4/100 - train_loss: 1.1951 - train_accuracy: 0.4111 - val_loss: 1.1619 - val_accuracy: 0.4419\n",
      "Epoch 5/100 - train_loss: 1.1903 - train_accuracy: 0.4111 - val_loss: 1.1585 - val_accuracy: 0.4419\n",
      "Epoch 6/100 - train_loss: 1.1873 - train_accuracy: 0.4111 - val_loss: 1.1571 - val_accuracy: 0.4419\n",
      "Epoch 7/100 - train_loss: 1.1853 - train_accuracy: 0.4111 - val_loss: 1.1566 - val_accuracy: 0.4419\n",
      "Epoch 8/100 - train_loss: 1.1839 - train_accuracy: 0.4111 - val_loss: 1.1566 - val_accuracy: 0.4419\n",
      "Epoch 9/100 - train_loss: 1.1828 - train_accuracy: 0.4111 - val_loss: 1.1570 - val_accuracy: 0.4419\n",
      "Epoch 10/100 - train_loss: 1.1820 - train_accuracy: 0.4111 - val_loss: 1.1575 - val_accuracy: 0.4419\n",
      "Epoch 11/100 - train_loss: 1.1814 - train_accuracy: 0.4111 - val_loss: 1.1581 - val_accuracy: 0.4419\n",
      "Epoch 12/100 - train_loss: 1.1808 - train_accuracy: 0.4111 - val_loss: 1.1587 - val_accuracy: 0.4419\n",
      "Epoch 13/100 - train_loss: 1.1804 - train_accuracy: 0.4111 - val_loss: 1.1594 - val_accuracy: 0.4419\n",
      "Epoch 14/100 - train_loss: 1.1800 - train_accuracy: 0.4111 - val_loss: 1.1601 - val_accuracy: 0.4419\n",
      "Epoch 15/100 - train_loss: 1.1797 - train_accuracy: 0.4111 - val_loss: 1.1607 - val_accuracy: 0.4419\n",
      "Epoch 16/100 - train_loss: 1.1793 - train_accuracy: 0.4111 - val_loss: 1.1614 - val_accuracy: 0.4419\n",
      "Epoch 17/100 - train_loss: 1.1791 - train_accuracy: 0.4111 - val_loss: 1.1620 - val_accuracy: 0.4419\n",
      "Epoch 18/100 - train_loss: 1.1788 - train_accuracy: 0.4111 - val_loss: 1.1625 - val_accuracy: 0.4419\n",
      "Epoch 19/100 - train_loss: 1.1784 - train_accuracy: 0.4111 - val_loss: 1.1629 - val_accuracy: 0.4419\n",
      "Epoch 20/100 - train_loss: 1.1779 - train_accuracy: 0.4111 - val_loss: 1.1631 - val_accuracy: 0.4419\n",
      "Epoch 21/100 - train_loss: 1.1766 - train_accuracy: 0.4111 - val_loss: 1.1626 - val_accuracy: 0.4419\n",
      "Epoch 22/100 - train_loss: 1.1714 - train_accuracy: 0.4340 - val_loss: 1.1587 - val_accuracy: 0.4593\n",
      "Epoch 23/100 - train_loss: 1.1501 - train_accuracy: 0.5040 - val_loss: 1.1404 - val_accuracy: 0.5465\n",
      "Epoch 24/100 - train_loss: 1.0922 - train_accuracy: 0.5404 - val_loss: 1.0880 - val_accuracy: 0.5698\n",
      "Epoch 25/100 - train_loss: 1.0065 - train_accuracy: 0.5930 - val_loss: 1.0172 - val_accuracy: 0.5930\n",
      "Epoch 26/100 - train_loss: 0.9656 - train_accuracy: 0.5984 - val_loss: 0.9876 - val_accuracy: 0.5872\n",
      "Epoch 27/100 - train_loss: 0.9465 - train_accuracy: 0.6051 - val_loss: 0.9759 - val_accuracy: 0.5872\n",
      "Epoch 28/100 - train_loss: 0.9377 - train_accuracy: 0.6092 - val_loss: 0.9781 - val_accuracy: 0.5872\n",
      "Epoch 29/100 - train_loss: 0.9329 - train_accuracy: 0.6038 - val_loss: 0.9819 - val_accuracy: 0.5814\n",
      "Epoch 30/100 - train_loss: 0.9273 - train_accuracy: 0.6024 - val_loss: 0.9869 - val_accuracy: 0.5814\n",
      "Epoch 31/100 - train_loss: 0.9237 - train_accuracy: 0.6092 - val_loss: 0.9898 - val_accuracy: 0.5814\n",
      "Epoch 32/100 - train_loss: 0.9190 - train_accuracy: 0.6146 - val_loss: 0.9945 - val_accuracy: 0.5872\n",
      "Epoch 33/100 - train_loss: 0.9229 - train_accuracy: 0.6213 - val_loss: 1.0073 - val_accuracy: 0.5988\n",
      "Epoch 34/100 - train_loss: 0.9196 - train_accuracy: 0.6199 - val_loss: 1.0100 - val_accuracy: 0.5988\n",
      "Epoch 35/100 - train_loss: 0.9174 - train_accuracy: 0.6240 - val_loss: 1.0111 - val_accuracy: 0.5988\n",
      "Epoch 36/100 - train_loss: 0.9153 - train_accuracy: 0.6240 - val_loss: 1.0130 - val_accuracy: 0.5988\n",
      "Epoch 37/100 - train_loss: 0.9130 - train_accuracy: 0.6240 - val_loss: 1.0132 - val_accuracy: 0.5988\n",
      "Epoch 38/100 - train_loss: 0.9112 - train_accuracy: 0.6280 - val_loss: 1.0143 - val_accuracy: 0.5930\n",
      "Epoch 39/100 - train_loss: 0.9088 - train_accuracy: 0.6267 - val_loss: 1.0139 - val_accuracy: 0.6047\n",
      "Epoch 40/100 - train_loss: 0.9064 - train_accuracy: 0.6348 - val_loss: 1.0139 - val_accuracy: 0.5988\n",
      "Epoch 41/100 - train_loss: 0.9039 - train_accuracy: 0.6375 - val_loss: 1.0135 - val_accuracy: 0.5872\n",
      "Epoch 42/100 - train_loss: 0.9015 - train_accuracy: 0.6361 - val_loss: 1.0136 - val_accuracy: 0.5930\n",
      "Epoch 43/100 - train_loss: 0.8980 - train_accuracy: 0.6429 - val_loss: 1.0114 - val_accuracy: 0.5872\n",
      "Epoch 44/100 - train_loss: 0.8931 - train_accuracy: 0.6523 - val_loss: 1.0082 - val_accuracy: 0.5930\n",
      "Epoch 45/100 - train_loss: 0.8879 - train_accuracy: 0.6523 - val_loss: 1.0061 - val_accuracy: 0.5814\n",
      "Epoch 46/100 - train_loss: 0.8831 - train_accuracy: 0.6509 - val_loss: 1.0043 - val_accuracy: 0.5698\n",
      "Epoch 47/100 - train_loss: 0.8790 - train_accuracy: 0.6496 - val_loss: 1.0028 - val_accuracy: 0.5814\n",
      "Epoch 48/100 - train_loss: 0.8748 - train_accuracy: 0.6509 - val_loss: 1.0023 - val_accuracy: 0.5814\n",
      "Epoch 49/100 - train_loss: 0.8701 - train_accuracy: 0.6496 - val_loss: 1.0033 - val_accuracy: 0.5523\n",
      "Epoch 50/100 - train_loss: 0.8679 - train_accuracy: 0.6523 - val_loss: 1.0019 - val_accuracy: 0.5581\n",
      "Epoch 51/100 - train_loss: 0.8648 - train_accuracy: 0.6536 - val_loss: 1.0019 - val_accuracy: 0.5581\n",
      "Epoch 52/100 - train_loss: 0.8621 - train_accuracy: 0.6577 - val_loss: 1.0030 - val_accuracy: 0.5640\n",
      "Epoch 53/100 - train_loss: 0.8592 - train_accuracy: 0.6577 - val_loss: 1.0081 - val_accuracy: 0.5698\n",
      "Epoch 54/100 - train_loss: 0.8562 - train_accuracy: 0.6617 - val_loss: 1.0091 - val_accuracy: 0.5814\n",
      "Epoch 55/100 - train_loss: 0.8549 - train_accuracy: 0.6604 - val_loss: 1.0089 - val_accuracy: 0.5814\n",
      "Epoch 56/100 - train_loss: 0.8523 - train_accuracy: 0.6550 - val_loss: 1.0104 - val_accuracy: 0.5872\n",
      "Epoch 57/100 - train_loss: 0.8503 - train_accuracy: 0.6536 - val_loss: 1.0100 - val_accuracy: 0.5872\n",
      "Epoch 58/100 - train_loss: 0.8471 - train_accuracy: 0.6523 - val_loss: 1.0090 - val_accuracy: 0.5872\n",
      "Epoch 59/100 - train_loss: 0.8468 - train_accuracy: 0.6550 - val_loss: 1.0095 - val_accuracy: 0.5872\n",
      "Epoch 60/100 - train_loss: 0.8443 - train_accuracy: 0.6590 - val_loss: 1.0102 - val_accuracy: 0.5872\n",
      "Epoch 61/100 - train_loss: 0.8421 - train_accuracy: 0.6563 - val_loss: 1.0091 - val_accuracy: 0.5872\n",
      "Epoch 62/100 - train_loss: 0.8412 - train_accuracy: 0.6590 - val_loss: 1.0102 - val_accuracy: 0.5756\n",
      "Epoch 63/100 - train_loss: 0.8405 - train_accuracy: 0.6644 - val_loss: 1.0103 - val_accuracy: 0.5756\n",
      "Epoch 64/100 - train_loss: 0.8406 - train_accuracy: 0.6631 - val_loss: 1.0072 - val_accuracy: 0.5814\n",
      "Epoch 65/100 - train_loss: 0.8402 - train_accuracy: 0.6644 - val_loss: 1.0076 - val_accuracy: 0.5814\n",
      "Epoch 66/100 - train_loss: 0.8392 - train_accuracy: 0.6644 - val_loss: 1.0080 - val_accuracy: 0.5872\n",
      "Epoch 67/100 - train_loss: 0.8371 - train_accuracy: 0.6698 - val_loss: 1.0084 - val_accuracy: 0.5872\n",
      "Epoch 68/100 - train_loss: 0.8358 - train_accuracy: 0.6671 - val_loss: 1.0069 - val_accuracy: 0.5930\n",
      "Epoch 69/100 - train_loss: 0.8346 - train_accuracy: 0.6698 - val_loss: 1.0068 - val_accuracy: 0.5930\n",
      "Epoch 70/100 - train_loss: 0.8345 - train_accuracy: 0.6712 - val_loss: 1.0059 - val_accuracy: 0.5930\n",
      "Epoch 71/100 - train_loss: 0.8341 - train_accuracy: 0.6658 - val_loss: 1.0084 - val_accuracy: 0.5814\n",
      "Epoch 72/100 - train_loss: 0.8354 - train_accuracy: 0.6617 - val_loss: 1.0101 - val_accuracy: 0.5872\n",
      "Epoch 73/100 - train_loss: 0.8347 - train_accuracy: 0.6671 - val_loss: 1.0090 - val_accuracy: 0.5814\n",
      "Epoch 74/100 - train_loss: 0.8346 - train_accuracy: 0.6644 - val_loss: 1.0113 - val_accuracy: 0.5756\n",
      "Epoch 75/100 - train_loss: 0.8310 - train_accuracy: 0.6725 - val_loss: 1.0102 - val_accuracy: 0.5872\n",
      "Epoch 76/100 - train_loss: 0.8310 - train_accuracy: 0.6685 - val_loss: 1.0080 - val_accuracy: 0.5756\n",
      "Epoch 77/100 - train_loss: 0.8305 - train_accuracy: 0.6644 - val_loss: 1.0089 - val_accuracy: 0.5814\n",
      "Epoch 78/100 - train_loss: 0.8270 - train_accuracy: 0.6712 - val_loss: 1.0057 - val_accuracy: 0.5988\n",
      "Epoch 79/100 - train_loss: 0.8279 - train_accuracy: 0.6685 - val_loss: 1.0066 - val_accuracy: 0.5756\n",
      "Epoch 80/100 - train_loss: 0.8255 - train_accuracy: 0.6752 - val_loss: 1.0060 - val_accuracy: 0.5814\n",
      "Epoch 81/100 - train_loss: 0.8237 - train_accuracy: 0.6792 - val_loss: 1.0051 - val_accuracy: 0.6047\n",
      "Epoch 82/100 - train_loss: 0.8227 - train_accuracy: 0.6752 - val_loss: 1.0040 - val_accuracy: 0.5988\n",
      "Epoch 83/100 - train_loss: 0.8227 - train_accuracy: 0.6806 - val_loss: 1.0023 - val_accuracy: 0.6105\n",
      "Epoch 84/100 - train_loss: 0.8218 - train_accuracy: 0.6779 - val_loss: 1.0048 - val_accuracy: 0.5988\n",
      "Epoch 85/100 - train_loss: 0.8223 - train_accuracy: 0.6739 - val_loss: 1.0058 - val_accuracy: 0.5988\n",
      "Epoch 86/100 - train_loss: 0.8193 - train_accuracy: 0.6779 - val_loss: 1.0037 - val_accuracy: 0.6105\n",
      "Epoch 87/100 - train_loss: 0.8183 - train_accuracy: 0.6792 - val_loss: 1.0058 - val_accuracy: 0.5988\n",
      "Epoch 88/100 - train_loss: 0.8165 - train_accuracy: 0.6739 - val_loss: 1.0014 - val_accuracy: 0.5988\n",
      "Epoch 89/100 - train_loss: 0.8156 - train_accuracy: 0.6792 - val_loss: 1.0013 - val_accuracy: 0.5988\n",
      "Epoch 90/100 - train_loss: 0.8148 - train_accuracy: 0.6779 - val_loss: 1.0020 - val_accuracy: 0.6047\n",
      "Epoch 91/100 - train_loss: 0.8134 - train_accuracy: 0.6833 - val_loss: 1.0014 - val_accuracy: 0.5988\n",
      "Epoch 92/100 - train_loss: 0.8120 - train_accuracy: 0.6900 - val_loss: 1.0008 - val_accuracy: 0.5930\n",
      "Epoch 93/100 - train_loss: 0.8121 - train_accuracy: 0.6900 - val_loss: 0.9998 - val_accuracy: 0.5988\n",
      "Epoch 94/100 - train_loss: 0.8105 - train_accuracy: 0.6873 - val_loss: 0.9979 - val_accuracy: 0.5872\n",
      "Epoch 95/100 - train_loss: 0.8109 - train_accuracy: 0.6887 - val_loss: 0.9966 - val_accuracy: 0.5930\n",
      "Epoch 96/100 - train_loss: 0.8091 - train_accuracy: 0.6900 - val_loss: 0.9938 - val_accuracy: 0.5988\n",
      "Epoch 97/100 - train_loss: 0.8098 - train_accuracy: 0.6900 - val_loss: 0.9933 - val_accuracy: 0.5814\n",
      "Epoch 98/100 - train_loss: 0.8075 - train_accuracy: 0.6941 - val_loss: 0.9923 - val_accuracy: 0.5930\n",
      "Epoch 99/100 - train_loss: 0.8062 - train_accuracy: 0.6927 - val_loss: 0.9927 - val_accuracy: 0.5930\n",
      "Epoch 100/100 - train_loss: 0.8049 - train_accuracy: 0.6887 - val_loss: 0.9906 - val_accuracy: 0.5930\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272aafa20ba54f4d9218545436696285"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.01_act=relu_optim=sgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/49peq582' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/49peq582</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124841-49peq582/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e13be1c90164855a01c6e7703030fb9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124852-qs012ws4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/qs012ws4' target=\"_blank\">lr=0.01_act=relu_optim=bgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/qs012ws4' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/qs012ws4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7899 - train_accuracy: 0.4030 - val_loss: 1.7898 - val_accuracy: 0.4012\n",
      "Epoch 2/100 - train_loss: 1.7880 - train_accuracy: 0.4137 - val_loss: 1.7878 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.7861 - train_accuracy: 0.4137 - val_loss: 1.7858 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.7842 - train_accuracy: 0.4137 - val_loss: 1.7839 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.7823 - train_accuracy: 0.4137 - val_loss: 1.7819 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.7804 - train_accuracy: 0.4137 - val_loss: 1.7800 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.7786 - train_accuracy: 0.4137 - val_loss: 1.7781 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.7767 - train_accuracy: 0.4137 - val_loss: 1.7761 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.7748 - train_accuracy: 0.4137 - val_loss: 1.7742 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.7730 - train_accuracy: 0.4137 - val_loss: 1.7723 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.7712 - train_accuracy: 0.4137 - val_loss: 1.7704 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.7693 - train_accuracy: 0.4137 - val_loss: 1.7685 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.7675 - train_accuracy: 0.4137 - val_loss: 1.7666 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.7657 - train_accuracy: 0.4137 - val_loss: 1.7647 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.7638 - train_accuracy: 0.4137 - val_loss: 1.7628 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.7620 - train_accuracy: 0.4137 - val_loss: 1.7609 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.7602 - train_accuracy: 0.4137 - val_loss: 1.7590 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.7584 - train_accuracy: 0.4137 - val_loss: 1.7572 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.7566 - train_accuracy: 0.4137 - val_loss: 1.7553 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.7549 - train_accuracy: 0.4137 - val_loss: 1.7534 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.7531 - train_accuracy: 0.4137 - val_loss: 1.7516 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.7513 - train_accuracy: 0.4137 - val_loss: 1.7497 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.7495 - train_accuracy: 0.4137 - val_loss: 1.7479 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.7478 - train_accuracy: 0.4137 - val_loss: 1.7461 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.7460 - train_accuracy: 0.4137 - val_loss: 1.7442 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.7443 - train_accuracy: 0.4137 - val_loss: 1.7424 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.7425 - train_accuracy: 0.4137 - val_loss: 1.7406 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.7408 - train_accuracy: 0.4137 - val_loss: 1.7388 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.7390 - train_accuracy: 0.4137 - val_loss: 1.7370 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.7373 - train_accuracy: 0.4137 - val_loss: 1.7352 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.7356 - train_accuracy: 0.4137 - val_loss: 1.7334 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.7339 - train_accuracy: 0.4137 - val_loss: 1.7316 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.7322 - train_accuracy: 0.4137 - val_loss: 1.7299 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.7305 - train_accuracy: 0.4137 - val_loss: 1.7281 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.7288 - train_accuracy: 0.4137 - val_loss: 1.7263 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.7271 - train_accuracy: 0.4137 - val_loss: 1.7246 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.7254 - train_accuracy: 0.4137 - val_loss: 1.7228 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.7237 - train_accuracy: 0.4137 - val_loss: 1.7211 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.7220 - train_accuracy: 0.4137 - val_loss: 1.7193 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.7204 - train_accuracy: 0.4137 - val_loss: 1.7176 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.7187 - train_accuracy: 0.4137 - val_loss: 1.7158 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.7171 - train_accuracy: 0.4137 - val_loss: 1.7141 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.7154 - train_accuracy: 0.4137 - val_loss: 1.7124 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.7138 - train_accuracy: 0.4137 - val_loss: 1.7107 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.7121 - train_accuracy: 0.4137 - val_loss: 1.7090 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.7105 - train_accuracy: 0.4137 - val_loss: 1.7073 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.7089 - train_accuracy: 0.4137 - val_loss: 1.7056 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.7072 - train_accuracy: 0.4137 - val_loss: 1.7039 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.7056 - train_accuracy: 0.4137 - val_loss: 1.7022 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.7040 - train_accuracy: 0.4137 - val_loss: 1.7005 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.7024 - train_accuracy: 0.4137 - val_loss: 1.6989 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.7008 - train_accuracy: 0.4137 - val_loss: 1.6972 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.6992 - train_accuracy: 0.4137 - val_loss: 1.6955 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.6976 - train_accuracy: 0.4137 - val_loss: 1.6939 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.6960 - train_accuracy: 0.4137 - val_loss: 1.6922 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.6945 - train_accuracy: 0.4137 - val_loss: 1.6906 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.6929 - train_accuracy: 0.4137 - val_loss: 1.6890 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.6913 - train_accuracy: 0.4137 - val_loss: 1.6873 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.6898 - train_accuracy: 0.4137 - val_loss: 1.6857 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.6882 - train_accuracy: 0.4137 - val_loss: 1.6841 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.6866 - train_accuracy: 0.4137 - val_loss: 1.6825 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.6851 - train_accuracy: 0.4137 - val_loss: 1.6808 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.6836 - train_accuracy: 0.4137 - val_loss: 1.6792 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.6820 - train_accuracy: 0.4137 - val_loss: 1.6776 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.6805 - train_accuracy: 0.4137 - val_loss: 1.6760 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.6790 - train_accuracy: 0.4137 - val_loss: 1.6745 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.6775 - train_accuracy: 0.4137 - val_loss: 1.6729 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.6759 - train_accuracy: 0.4137 - val_loss: 1.6713 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.6744 - train_accuracy: 0.4137 - val_loss: 1.6697 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.6729 - train_accuracy: 0.4137 - val_loss: 1.6682 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.6714 - train_accuracy: 0.4137 - val_loss: 1.6666 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.6699 - train_accuracy: 0.4137 - val_loss: 1.6650 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.6685 - train_accuracy: 0.4137 - val_loss: 1.6635 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.6670 - train_accuracy: 0.4137 - val_loss: 1.6620 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.6655 - train_accuracy: 0.4137 - val_loss: 1.6604 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.6640 - train_accuracy: 0.4137 - val_loss: 1.6589 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.6626 - train_accuracy: 0.4137 - val_loss: 1.6573 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.6611 - train_accuracy: 0.4137 - val_loss: 1.6558 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.6596 - train_accuracy: 0.4137 - val_loss: 1.6543 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.6582 - train_accuracy: 0.4137 - val_loss: 1.6528 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.6567 - train_accuracy: 0.4137 - val_loss: 1.6513 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.6553 - train_accuracy: 0.4137 - val_loss: 1.6498 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.6539 - train_accuracy: 0.4137 - val_loss: 1.6483 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.6524 - train_accuracy: 0.4137 - val_loss: 1.6468 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.6510 - train_accuracy: 0.4137 - val_loss: 1.6453 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.6496 - train_accuracy: 0.4137 - val_loss: 1.6438 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.6482 - train_accuracy: 0.4137 - val_loss: 1.6423 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.6468 - train_accuracy: 0.4137 - val_loss: 1.6409 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.6454 - train_accuracy: 0.4137 - val_loss: 1.6394 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.6440 - train_accuracy: 0.4137 - val_loss: 1.6379 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.6426 - train_accuracy: 0.4137 - val_loss: 1.6365 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.6412 - train_accuracy: 0.4137 - val_loss: 1.6350 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.6398 - train_accuracy: 0.4137 - val_loss: 1.6336 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.6384 - train_accuracy: 0.4137 - val_loss: 1.6321 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.6370 - train_accuracy: 0.4137 - val_loss: 1.6307 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.6357 - train_accuracy: 0.4137 - val_loss: 1.6293 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.6343 - train_accuracy: 0.4137 - val_loss: 1.6279 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.6329 - train_accuracy: 0.4137 - val_loss: 1.6264 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.6316 - train_accuracy: 0.4137 - val_loss: 1.6250 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.6302 - train_accuracy: 0.4137 - val_loss: 1.6236 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2a0221cc2444bdaa01d9123e53dae9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.01_act=relu_optim=bgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/qs012ws4' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/qs012ws4</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124852-qs012ws4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab730aa2b0404c8f80886b46b395b0a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124907-qqg3b3j8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/qqg3b3j8' target=\"_blank\">lr=0.01_act=relu_optim=mbgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/qqg3b3j8' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/qqg3b3j8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7473 - train_accuracy: 0.4137 - val_loss: 1.7456 - val_accuracy: 0.4070\n",
      "Epoch 2/100 - train_loss: 1.7064 - train_accuracy: 0.4137 - val_loss: 1.7030 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.6688 - train_accuracy: 0.4137 - val_loss: 1.6637 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.6342 - train_accuracy: 0.4137 - val_loss: 1.6276 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.6025 - train_accuracy: 0.4137 - val_loss: 1.5945 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.5734 - train_accuracy: 0.4137 - val_loss: 1.5640 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.5467 - train_accuracy: 0.4137 - val_loss: 1.5360 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.5223 - train_accuracy: 0.4137 - val_loss: 1.5103 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.4999 - train_accuracy: 0.4137 - val_loss: 1.4867 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.4794 - train_accuracy: 0.4137 - val_loss: 1.4650 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.4606 - train_accuracy: 0.4137 - val_loss: 1.4451 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.4433 - train_accuracy: 0.4137 - val_loss: 1.4268 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.4275 - train_accuracy: 0.4137 - val_loss: 1.4100 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.4129 - train_accuracy: 0.4137 - val_loss: 1.3945 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.3995 - train_accuracy: 0.4137 - val_loss: 1.3802 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.3872 - train_accuracy: 0.4137 - val_loss: 1.3670 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.3758 - train_accuracy: 0.4137 - val_loss: 1.3549 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.3653 - train_accuracy: 0.4137 - val_loss: 1.3436 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.3557 - train_accuracy: 0.4137 - val_loss: 1.3332 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.3467 - train_accuracy: 0.4137 - val_loss: 1.3235 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.3384 - train_accuracy: 0.4137 - val_loss: 1.3146 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.3307 - train_accuracy: 0.4137 - val_loss: 1.3063 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.3236 - train_accuracy: 0.4137 - val_loss: 1.2986 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.3170 - train_accuracy: 0.4137 - val_loss: 1.2914 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.3108 - train_accuracy: 0.4137 - val_loss: 1.2847 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.3050 - train_accuracy: 0.4137 - val_loss: 1.2784 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.2997 - train_accuracy: 0.4137 - val_loss: 1.2725 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.2947 - train_accuracy: 0.4137 - val_loss: 1.2671 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.2900 - train_accuracy: 0.4137 - val_loss: 1.2620 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.2856 - train_accuracy: 0.4137 - val_loss: 1.2572 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.2815 - train_accuracy: 0.4137 - val_loss: 1.2526 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.2776 - train_accuracy: 0.4137 - val_loss: 1.2484 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.2740 - train_accuracy: 0.4137 - val_loss: 1.2444 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.2706 - train_accuracy: 0.4137 - val_loss: 1.2407 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.2674 - train_accuracy: 0.4137 - val_loss: 1.2371 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.2643 - train_accuracy: 0.4137 - val_loss: 1.2338 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.2615 - train_accuracy: 0.4137 - val_loss: 1.2306 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.2588 - train_accuracy: 0.4137 - val_loss: 1.2277 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.2562 - train_accuracy: 0.4137 - val_loss: 1.2248 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.2538 - train_accuracy: 0.4137 - val_loss: 1.2222 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.2515 - train_accuracy: 0.4137 - val_loss: 1.2196 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.2493 - train_accuracy: 0.4137 - val_loss: 1.2172 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.2472 - train_accuracy: 0.4137 - val_loss: 1.2149 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.2452 - train_accuracy: 0.4137 - val_loss: 1.2127 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.2433 - train_accuracy: 0.4137 - val_loss: 1.2107 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.2415 - train_accuracy: 0.4137 - val_loss: 1.2087 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.2398 - train_accuracy: 0.4137 - val_loss: 1.2068 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.2382 - train_accuracy: 0.4137 - val_loss: 1.2050 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.2366 - train_accuracy: 0.4137 - val_loss: 1.2033 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.2351 - train_accuracy: 0.4137 - val_loss: 1.2017 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.2337 - train_accuracy: 0.4137 - val_loss: 1.2001 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.2323 - train_accuracy: 0.4137 - val_loss: 1.1986 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.2310 - train_accuracy: 0.4137 - val_loss: 1.1972 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.2298 - train_accuracy: 0.4137 - val_loss: 1.1958 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.2285 - train_accuracy: 0.4137 - val_loss: 1.1945 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.2274 - train_accuracy: 0.4137 - val_loss: 1.1932 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.2263 - train_accuracy: 0.4137 - val_loss: 1.1920 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.2252 - train_accuracy: 0.4137 - val_loss: 1.1908 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.2241 - train_accuracy: 0.4137 - val_loss: 1.1897 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.2231 - train_accuracy: 0.4137 - val_loss: 1.1887 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.2222 - train_accuracy: 0.4137 - val_loss: 1.1876 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.2212 - train_accuracy: 0.4137 - val_loss: 1.1866 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.2203 - train_accuracy: 0.4137 - val_loss: 1.1857 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.2195 - train_accuracy: 0.4137 - val_loss: 1.1847 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.2186 - train_accuracy: 0.4137 - val_loss: 1.1838 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.2178 - train_accuracy: 0.4137 - val_loss: 1.1830 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.2170 - train_accuracy: 0.4137 - val_loss: 1.1822 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.2163 - train_accuracy: 0.4137 - val_loss: 1.1814 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.2155 - train_accuracy: 0.4137 - val_loss: 1.1806 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.2148 - train_accuracy: 0.4137 - val_loss: 1.1798 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.2141 - train_accuracy: 0.4137 - val_loss: 1.1791 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.2134 - train_accuracy: 0.4137 - val_loss: 1.1784 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.2128 - train_accuracy: 0.4137 - val_loss: 1.1777 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.2121 - train_accuracy: 0.4137 - val_loss: 1.1771 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.2115 - train_accuracy: 0.4137 - val_loss: 1.1765 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.2109 - train_accuracy: 0.4137 - val_loss: 1.1758 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.2103 - train_accuracy: 0.4137 - val_loss: 1.1753 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.2098 - train_accuracy: 0.4137 - val_loss: 1.1747 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.2092 - train_accuracy: 0.4137 - val_loss: 1.1741 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.2087 - train_accuracy: 0.4137 - val_loss: 1.1736 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.2082 - train_accuracy: 0.4137 - val_loss: 1.1731 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.2077 - train_accuracy: 0.4137 - val_loss: 1.1726 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.2072 - train_accuracy: 0.4137 - val_loss: 1.1721 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.2067 - train_accuracy: 0.4137 - val_loss: 1.1716 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.2062 - train_accuracy: 0.4137 - val_loss: 1.1711 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.2057 - train_accuracy: 0.4137 - val_loss: 1.1707 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.2053 - train_accuracy: 0.4137 - val_loss: 1.1702 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.2049 - train_accuracy: 0.4137 - val_loss: 1.1698 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.2044 - train_accuracy: 0.4137 - val_loss: 1.1694 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.2040 - train_accuracy: 0.4137 - val_loss: 1.1690 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.2036 - train_accuracy: 0.4137 - val_loss: 1.1686 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.2032 - train_accuracy: 0.4137 - val_loss: 1.1683 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.2028 - train_accuracy: 0.4137 - val_loss: 1.1679 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.2025 - train_accuracy: 0.4137 - val_loss: 1.1675 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.2021 - train_accuracy: 0.4137 - val_loss: 1.1672 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.2017 - train_accuracy: 0.4137 - val_loss: 1.1669 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.2014 - train_accuracy: 0.4137 - val_loss: 1.1665 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.2010 - train_accuracy: 0.4137 - val_loss: 1.1662 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.2007 - train_accuracy: 0.4137 - val_loss: 1.1659 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.2004 - train_accuracy: 0.4137 - val_loss: 1.1656 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655be00577e140818d51bf4e164bc233"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.01_act=relu_optim=mbgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/qqg3b3j8' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/qqg3b3j8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124907-qqg3b3j8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2f91c9c75046eba6a869319bf4844c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124920-oqt4pkvu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/oqt4pkvu' target=\"_blank\">lr=0.1_act=sigmoid_optim=sgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/oqt4pkvu' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/oqt4pkvu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.2055 - train_accuracy: 0.4111 - val_loss: 1.1769 - val_accuracy: 0.4419\n",
      "Epoch 2/100 - train_loss: 1.1925 - train_accuracy: 0.4111 - val_loss: 1.1711 - val_accuracy: 0.4419\n",
      "Epoch 3/100 - train_loss: 1.1045 - train_accuracy: 0.4960 - val_loss: 1.1011 - val_accuracy: 0.5349\n",
      "Epoch 4/100 - train_loss: 0.9944 - train_accuracy: 0.5876 - val_loss: 1.0286 - val_accuracy: 0.5988\n",
      "Epoch 5/100 - train_loss: 0.9678 - train_accuracy: 0.5970 - val_loss: 1.0191 - val_accuracy: 0.6047\n",
      "Epoch 6/100 - train_loss: 0.9571 - train_accuracy: 0.5970 - val_loss: 1.0179 - val_accuracy: 0.6221\n",
      "Epoch 7/100 - train_loss: 0.9515 - train_accuracy: 0.5997 - val_loss: 1.0181 - val_accuracy: 0.6221\n",
      "Epoch 8/100 - train_loss: 0.9482 - train_accuracy: 0.5997 - val_loss: 1.0185 - val_accuracy: 0.6163\n",
      "Epoch 9/100 - train_loss: 0.9459 - train_accuracy: 0.6065 - val_loss: 1.0192 - val_accuracy: 0.6163\n",
      "Epoch 10/100 - train_loss: 0.9430 - train_accuracy: 0.6078 - val_loss: 1.0206 - val_accuracy: 0.6163\n",
      "Epoch 11/100 - train_loss: 0.9406 - train_accuracy: 0.6051 - val_loss: 1.0209 - val_accuracy: 0.6163\n",
      "Epoch 12/100 - train_loss: 0.9380 - train_accuracy: 0.6078 - val_loss: 1.0210 - val_accuracy: 0.6163\n",
      "Epoch 13/100 - train_loss: 0.9356 - train_accuracy: 0.6119 - val_loss: 1.0197 - val_accuracy: 0.6163\n",
      "Epoch 14/100 - train_loss: 0.9334 - train_accuracy: 0.6119 - val_loss: 1.0179 - val_accuracy: 0.6163\n",
      "Epoch 15/100 - train_loss: 0.9317 - train_accuracy: 0.6132 - val_loss: 1.0166 - val_accuracy: 0.6163\n",
      "Epoch 16/100 - train_loss: 0.9301 - train_accuracy: 0.6146 - val_loss: 1.0155 - val_accuracy: 0.6221\n",
      "Epoch 17/100 - train_loss: 0.9283 - train_accuracy: 0.6132 - val_loss: 1.0146 - val_accuracy: 0.6221\n",
      "Epoch 18/100 - train_loss: 0.9263 - train_accuracy: 0.6132 - val_loss: 1.0137 - val_accuracy: 0.6221\n",
      "Epoch 19/100 - train_loss: 0.9239 - train_accuracy: 0.6132 - val_loss: 1.0130 - val_accuracy: 0.6279\n",
      "Epoch 20/100 - train_loss: 0.9209 - train_accuracy: 0.6132 - val_loss: 1.0125 - val_accuracy: 0.6105\n",
      "Epoch 21/100 - train_loss: 0.9173 - train_accuracy: 0.6132 - val_loss: 1.0122 - val_accuracy: 0.6105\n",
      "Epoch 22/100 - train_loss: 0.9131 - train_accuracy: 0.6105 - val_loss: 1.0124 - val_accuracy: 0.6163\n",
      "Epoch 23/100 - train_loss: 0.9088 - train_accuracy: 0.6105 - val_loss: 1.0134 - val_accuracy: 0.6163\n",
      "Epoch 24/100 - train_loss: 0.9046 - train_accuracy: 0.6119 - val_loss: 1.0153 - val_accuracy: 0.6105\n",
      "Epoch 25/100 - train_loss: 0.9008 - train_accuracy: 0.6159 - val_loss: 1.0179 - val_accuracy: 0.6047\n",
      "Epoch 26/100 - train_loss: 0.8974 - train_accuracy: 0.6186 - val_loss: 1.0209 - val_accuracy: 0.6105\n",
      "Epoch 27/100 - train_loss: 0.8943 - train_accuracy: 0.6226 - val_loss: 1.0238 - val_accuracy: 0.5988\n",
      "Epoch 28/100 - train_loss: 0.8915 - train_accuracy: 0.6294 - val_loss: 1.0265 - val_accuracy: 0.6047\n",
      "Epoch 29/100 - train_loss: 0.8889 - train_accuracy: 0.6321 - val_loss: 1.0288 - val_accuracy: 0.6047\n",
      "Epoch 30/100 - train_loss: 0.8864 - train_accuracy: 0.6375 - val_loss: 1.0308 - val_accuracy: 0.6047\n",
      "Epoch 31/100 - train_loss: 0.8839 - train_accuracy: 0.6334 - val_loss: 1.0326 - val_accuracy: 0.5988\n",
      "Epoch 32/100 - train_loss: 0.8814 - train_accuracy: 0.6388 - val_loss: 1.0341 - val_accuracy: 0.5988\n",
      "Epoch 33/100 - train_loss: 0.8789 - train_accuracy: 0.6402 - val_loss: 1.0356 - val_accuracy: 0.5930\n",
      "Epoch 34/100 - train_loss: 0.8764 - train_accuracy: 0.6469 - val_loss: 1.0370 - val_accuracy: 0.5988\n",
      "Epoch 35/100 - train_loss: 0.8739 - train_accuracy: 0.6496 - val_loss: 1.0383 - val_accuracy: 0.5988\n",
      "Epoch 36/100 - train_loss: 0.8713 - train_accuracy: 0.6536 - val_loss: 1.0397 - val_accuracy: 0.5988\n",
      "Epoch 37/100 - train_loss: 0.8686 - train_accuracy: 0.6577 - val_loss: 1.0410 - val_accuracy: 0.5988\n",
      "Epoch 38/100 - train_loss: 0.8659 - train_accuracy: 0.6550 - val_loss: 1.0423 - val_accuracy: 0.6047\n",
      "Epoch 39/100 - train_loss: 0.8632 - train_accuracy: 0.6536 - val_loss: 1.0436 - val_accuracy: 0.5930\n",
      "Epoch 40/100 - train_loss: 0.8605 - train_accuracy: 0.6550 - val_loss: 1.0447 - val_accuracy: 0.5872\n",
      "Epoch 41/100 - train_loss: 0.8579 - train_accuracy: 0.6590 - val_loss: 1.0457 - val_accuracy: 0.6047\n",
      "Epoch 42/100 - train_loss: 0.8554 - train_accuracy: 0.6590 - val_loss: 1.0468 - val_accuracy: 0.6047\n",
      "Epoch 43/100 - train_loss: 0.8530 - train_accuracy: 0.6644 - val_loss: 1.0479 - val_accuracy: 0.6105\n",
      "Epoch 44/100 - train_loss: 0.8506 - train_accuracy: 0.6644 - val_loss: 1.0490 - val_accuracy: 0.6105\n",
      "Epoch 45/100 - train_loss: 0.8482 - train_accuracy: 0.6671 - val_loss: 1.0500 - val_accuracy: 0.6047\n",
      "Epoch 46/100 - train_loss: 0.8457 - train_accuracy: 0.6671 - val_loss: 1.0509 - val_accuracy: 0.5988\n",
      "Epoch 47/100 - train_loss: 0.8433 - train_accuracy: 0.6685 - val_loss: 1.0521 - val_accuracy: 0.5988\n",
      "Epoch 48/100 - train_loss: 0.8408 - train_accuracy: 0.6644 - val_loss: 1.0538 - val_accuracy: 0.5930\n",
      "Epoch 49/100 - train_loss: 0.8382 - train_accuracy: 0.6671 - val_loss: 1.0557 - val_accuracy: 0.5872\n",
      "Epoch 50/100 - train_loss: 0.8357 - train_accuracy: 0.6671 - val_loss: 1.0578 - val_accuracy: 0.5872\n",
      "Epoch 51/100 - train_loss: 0.8333 - train_accuracy: 0.6658 - val_loss: 1.0598 - val_accuracy: 0.5872\n",
      "Epoch 52/100 - train_loss: 0.8310 - train_accuracy: 0.6658 - val_loss: 1.0616 - val_accuracy: 0.5872\n",
      "Epoch 53/100 - train_loss: 0.8289 - train_accuracy: 0.6671 - val_loss: 1.0633 - val_accuracy: 0.5872\n",
      "Epoch 54/100 - train_loss: 0.8268 - train_accuracy: 0.6671 - val_loss: 1.0647 - val_accuracy: 0.5872\n",
      "Epoch 55/100 - train_loss: 0.8248 - train_accuracy: 0.6698 - val_loss: 1.0661 - val_accuracy: 0.5872\n",
      "Epoch 56/100 - train_loss: 0.8229 - train_accuracy: 0.6658 - val_loss: 1.0672 - val_accuracy: 0.5930\n",
      "Epoch 57/100 - train_loss: 0.8210 - train_accuracy: 0.6658 - val_loss: 1.0682 - val_accuracy: 0.5930\n",
      "Epoch 58/100 - train_loss: 0.8192 - train_accuracy: 0.6671 - val_loss: 1.0690 - val_accuracy: 0.5930\n",
      "Epoch 59/100 - train_loss: 0.8175 - train_accuracy: 0.6712 - val_loss: 1.0699 - val_accuracy: 0.5930\n",
      "Epoch 60/100 - train_loss: 0.8159 - train_accuracy: 0.6725 - val_loss: 1.0707 - val_accuracy: 0.5930\n",
      "Epoch 61/100 - train_loss: 0.8135 - train_accuracy: 0.6725 - val_loss: 1.0709 - val_accuracy: 0.5930\n",
      "Epoch 62/100 - train_loss: 0.8117 - train_accuracy: 0.6725 - val_loss: 1.0716 - val_accuracy: 0.5930\n",
      "Epoch 63/100 - train_loss: 0.8096 - train_accuracy: 0.6765 - val_loss: 1.0719 - val_accuracy: 0.5988\n",
      "Epoch 64/100 - train_loss: 0.8078 - train_accuracy: 0.6819 - val_loss: 1.0723 - val_accuracy: 0.6047\n",
      "Epoch 65/100 - train_loss: 0.8058 - train_accuracy: 0.6860 - val_loss: 1.0724 - val_accuracy: 0.5988\n",
      "Epoch 66/100 - train_loss: 0.8039 - train_accuracy: 0.6833 - val_loss: 1.0725 - val_accuracy: 0.5988\n",
      "Epoch 67/100 - train_loss: 0.8020 - train_accuracy: 0.6873 - val_loss: 1.0725 - val_accuracy: 0.5988\n",
      "Epoch 68/100 - train_loss: 0.8001 - train_accuracy: 0.6860 - val_loss: 1.0724 - val_accuracy: 0.5988\n",
      "Epoch 69/100 - train_loss: 0.7981 - train_accuracy: 0.6860 - val_loss: 1.0721 - val_accuracy: 0.5988\n",
      "Epoch 70/100 - train_loss: 0.7961 - train_accuracy: 0.6887 - val_loss: 1.0720 - val_accuracy: 0.6047\n",
      "Epoch 71/100 - train_loss: 0.7941 - train_accuracy: 0.6914 - val_loss: 1.0718 - val_accuracy: 0.6105\n",
      "Epoch 72/100 - train_loss: 0.7920 - train_accuracy: 0.6927 - val_loss: 1.0717 - val_accuracy: 0.6105\n",
      "Epoch 73/100 - train_loss: 0.7899 - train_accuracy: 0.6927 - val_loss: 1.0716 - val_accuracy: 0.6105\n",
      "Epoch 74/100 - train_loss: 0.7878 - train_accuracy: 0.6927 - val_loss: 1.0715 - val_accuracy: 0.6105\n",
      "Epoch 75/100 - train_loss: 0.7857 - train_accuracy: 0.6954 - val_loss: 1.0715 - val_accuracy: 0.6105\n",
      "Epoch 76/100 - train_loss: 0.7835 - train_accuracy: 0.6981 - val_loss: 1.0716 - val_accuracy: 0.6105\n",
      "Epoch 77/100 - train_loss: 0.7815 - train_accuracy: 0.6995 - val_loss: 1.0717 - val_accuracy: 0.6163\n",
      "Epoch 78/100 - train_loss: 0.7794 - train_accuracy: 0.6995 - val_loss: 1.0717 - val_accuracy: 0.6279\n",
      "Epoch 79/100 - train_loss: 0.7774 - train_accuracy: 0.6968 - val_loss: 1.0717 - val_accuracy: 0.6279\n",
      "Epoch 80/100 - train_loss: 0.7754 - train_accuracy: 0.6941 - val_loss: 1.0717 - val_accuracy: 0.6279\n",
      "Epoch 81/100 - train_loss: 0.7735 - train_accuracy: 0.6941 - val_loss: 1.0715 - val_accuracy: 0.6279\n",
      "Epoch 82/100 - train_loss: 0.7717 - train_accuracy: 0.6927 - val_loss: 1.0713 - val_accuracy: 0.6279\n",
      "Epoch 83/100 - train_loss: 0.7700 - train_accuracy: 0.6941 - val_loss: 1.0709 - val_accuracy: 0.6279\n",
      "Epoch 84/100 - train_loss: 0.7683 - train_accuracy: 0.6927 - val_loss: 1.0705 - val_accuracy: 0.6279\n",
      "Epoch 85/100 - train_loss: 0.7668 - train_accuracy: 0.6900 - val_loss: 1.0700 - val_accuracy: 0.6337\n",
      "Epoch 86/100 - train_loss: 0.7653 - train_accuracy: 0.6981 - val_loss: 1.0693 - val_accuracy: 0.6453\n",
      "Epoch 87/100 - train_loss: 0.7640 - train_accuracy: 0.6995 - val_loss: 1.0686 - val_accuracy: 0.6395\n",
      "Epoch 88/100 - train_loss: 0.7627 - train_accuracy: 0.7022 - val_loss: 1.0678 - val_accuracy: 0.6570\n",
      "Epoch 89/100 - train_loss: 0.7614 - train_accuracy: 0.7022 - val_loss: 1.0671 - val_accuracy: 0.6570\n",
      "Epoch 90/100 - train_loss: 0.7603 - train_accuracy: 0.7049 - val_loss: 1.0663 - val_accuracy: 0.6628\n",
      "Epoch 91/100 - train_loss: 0.7591 - train_accuracy: 0.7062 - val_loss: 1.0656 - val_accuracy: 0.6628\n",
      "Epoch 92/100 - train_loss: 0.7581 - train_accuracy: 0.7116 - val_loss: 1.0650 - val_accuracy: 0.6570\n",
      "Epoch 93/100 - train_loss: 0.7570 - train_accuracy: 0.7129 - val_loss: 1.0645 - val_accuracy: 0.6570\n",
      "Epoch 94/100 - train_loss: 0.7559 - train_accuracy: 0.7143 - val_loss: 1.0641 - val_accuracy: 0.6628\n",
      "Epoch 95/100 - train_loss: 0.7549 - train_accuracy: 0.7143 - val_loss: 1.0639 - val_accuracy: 0.6628\n",
      "Epoch 96/100 - train_loss: 0.7539 - train_accuracy: 0.7143 - val_loss: 1.0637 - val_accuracy: 0.6628\n",
      "Epoch 97/100 - train_loss: 0.7529 - train_accuracy: 0.7170 - val_loss: 1.0636 - val_accuracy: 0.6686\n",
      "Epoch 98/100 - train_loss: 0.7519 - train_accuracy: 0.7143 - val_loss: 1.0636 - val_accuracy: 0.6686\n",
      "Epoch 99/100 - train_loss: 0.7509 - train_accuracy: 0.7156 - val_loss: 1.0636 - val_accuracy: 0.6686\n",
      "Epoch 100/100 - train_loss: 0.7499 - train_accuracy: 0.7156 - val_loss: 1.0635 - val_accuracy: 0.6686\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883c6a2f62224d0c86fb0f8fe5635381"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.1_act=sigmoid_optim=sgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/oqt4pkvu' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/oqt4pkvu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124920-oqt4pkvu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ef1eed84e949fca284df1e8d3a7eb8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124939-r9ya605v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/r9ya605v' target=\"_blank\">lr=0.1_act=sigmoid_optim=bgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/r9ya605v' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/r9ya605v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7407 - train_accuracy: 0.4111 - val_loss: 1.7382 - val_accuracy: 0.4419\n",
      "Epoch 2/100 - train_loss: 1.6982 - train_accuracy: 0.4111 - val_loss: 1.6940 - val_accuracy: 0.4419\n",
      "Epoch 3/100 - train_loss: 1.6592 - train_accuracy: 0.4111 - val_loss: 1.6534 - val_accuracy: 0.4419\n",
      "Epoch 4/100 - train_loss: 1.6236 - train_accuracy: 0.4111 - val_loss: 1.6162 - val_accuracy: 0.4419\n",
      "Epoch 5/100 - train_loss: 1.5910 - train_accuracy: 0.4137 - val_loss: 1.5821 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.5612 - train_accuracy: 0.4137 - val_loss: 1.5509 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.5339 - train_accuracy: 0.4137 - val_loss: 1.5223 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.5090 - train_accuracy: 0.4137 - val_loss: 1.4961 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.4863 - train_accuracy: 0.4137 - val_loss: 1.4721 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.4655 - train_accuracy: 0.4137 - val_loss: 1.4501 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.4465 - train_accuracy: 0.4137 - val_loss: 1.4300 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.4291 - train_accuracy: 0.4137 - val_loss: 1.4115 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.4131 - train_accuracy: 0.4137 - val_loss: 1.3946 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.3985 - train_accuracy: 0.4137 - val_loss: 1.3791 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.3851 - train_accuracy: 0.4137 - val_loss: 1.3648 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.3729 - train_accuracy: 0.4137 - val_loss: 1.3517 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.3616 - train_accuracy: 0.4137 - val_loss: 1.3396 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.3512 - train_accuracy: 0.4137 - val_loss: 1.3284 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.3417 - train_accuracy: 0.4137 - val_loss: 1.3181 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.3329 - train_accuracy: 0.4137 - val_loss: 1.3087 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.3247 - train_accuracy: 0.4137 - val_loss: 1.2999 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.3172 - train_accuracy: 0.4137 - val_loss: 1.2918 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.3103 - train_accuracy: 0.4137 - val_loss: 1.2842 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.3039 - train_accuracy: 0.4137 - val_loss: 1.2773 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.2979 - train_accuracy: 0.4137 - val_loss: 1.2708 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.2924 - train_accuracy: 0.4137 - val_loss: 1.2647 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.2872 - train_accuracy: 0.4137 - val_loss: 1.2591 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.2824 - train_accuracy: 0.4137 - val_loss: 1.2539 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.2780 - train_accuracy: 0.4137 - val_loss: 1.2490 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.2738 - train_accuracy: 0.4137 - val_loss: 1.2444 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.2699 - train_accuracy: 0.4137 - val_loss: 1.2402 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.2662 - train_accuracy: 0.4137 - val_loss: 1.2362 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.2628 - train_accuracy: 0.4137 - val_loss: 1.2324 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.2596 - train_accuracy: 0.4137 - val_loss: 1.2289 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.2566 - train_accuracy: 0.4137 - val_loss: 1.2256 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.2538 - train_accuracy: 0.4137 - val_loss: 1.2225 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.2511 - train_accuracy: 0.4137 - val_loss: 1.2196 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.2486 - train_accuracy: 0.4137 - val_loss: 1.2168 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.2462 - train_accuracy: 0.4137 - val_loss: 1.2142 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.2439 - train_accuracy: 0.4137 - val_loss: 1.2117 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.2418 - train_accuracy: 0.4137 - val_loss: 1.2094 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.2398 - train_accuracy: 0.4137 - val_loss: 1.2072 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.2379 - train_accuracy: 0.4137 - val_loss: 1.2051 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.2361 - train_accuracy: 0.4137 - val_loss: 1.2031 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.2343 - train_accuracy: 0.4137 - val_loss: 1.2013 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.2327 - train_accuracy: 0.4137 - val_loss: 1.1995 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.2311 - train_accuracy: 0.4137 - val_loss: 1.1978 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.2296 - train_accuracy: 0.4137 - val_loss: 1.1962 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.2282 - train_accuracy: 0.4137 - val_loss: 1.1946 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.2269 - train_accuracy: 0.4137 - val_loss: 1.1932 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.2256 - train_accuracy: 0.4137 - val_loss: 1.1918 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.2243 - train_accuracy: 0.4137 - val_loss: 1.1905 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.2231 - train_accuracy: 0.4137 - val_loss: 1.1892 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.2220 - train_accuracy: 0.4137 - val_loss: 1.1880 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.2209 - train_accuracy: 0.4137 - val_loss: 1.1868 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.2198 - train_accuracy: 0.4137 - val_loss: 1.1857 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.2188 - train_accuracy: 0.4137 - val_loss: 1.1846 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.2178 - train_accuracy: 0.4137 - val_loss: 1.1836 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.2169 - train_accuracy: 0.4137 - val_loss: 1.1826 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.2160 - train_accuracy: 0.4137 - val_loss: 1.1817 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.2151 - train_accuracy: 0.4137 - val_loss: 1.1808 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.2143 - train_accuracy: 0.4137 - val_loss: 1.1800 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.2135 - train_accuracy: 0.4137 - val_loss: 1.1791 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.2127 - train_accuracy: 0.4137 - val_loss: 1.1783 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.2120 - train_accuracy: 0.4137 - val_loss: 1.1776 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.2112 - train_accuracy: 0.4137 - val_loss: 1.1768 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.2105 - train_accuracy: 0.4137 - val_loss: 1.1761 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.2098 - train_accuracy: 0.4137 - val_loss: 1.1754 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.2092 - train_accuracy: 0.4137 - val_loss: 1.1748 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.2086 - train_accuracy: 0.4137 - val_loss: 1.1742 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.2079 - train_accuracy: 0.4137 - val_loss: 1.1736 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.2073 - train_accuracy: 0.4137 - val_loss: 1.1730 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.2068 - train_accuracy: 0.4137 - val_loss: 1.1724 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.2062 - train_accuracy: 0.4137 - val_loss: 1.1719 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.2057 - train_accuracy: 0.4137 - val_loss: 1.1713 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.2051 - train_accuracy: 0.4137 - val_loss: 1.1708 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.2046 - train_accuracy: 0.4137 - val_loss: 1.1704 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.2041 - train_accuracy: 0.4137 - val_loss: 1.1699 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.2036 - train_accuracy: 0.4137 - val_loss: 1.1694 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.2032 - train_accuracy: 0.4137 - val_loss: 1.1690 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.2027 - train_accuracy: 0.4137 - val_loss: 1.1686 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.2023 - train_accuracy: 0.4137 - val_loss: 1.1682 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.2018 - train_accuracy: 0.4137 - val_loss: 1.1678 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.2014 - train_accuracy: 0.4137 - val_loss: 1.1674 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.2010 - train_accuracy: 0.4137 - val_loss: 1.1670 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.2006 - train_accuracy: 0.4137 - val_loss: 1.1667 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.2002 - train_accuracy: 0.4137 - val_loss: 1.1663 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.1999 - train_accuracy: 0.4137 - val_loss: 1.1660 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.1995 - train_accuracy: 0.4137 - val_loss: 1.1657 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.1991 - train_accuracy: 0.4137 - val_loss: 1.1653 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.1988 - train_accuracy: 0.4137 - val_loss: 1.1650 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.1984 - train_accuracy: 0.4137 - val_loss: 1.1648 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.1981 - train_accuracy: 0.4137 - val_loss: 1.1645 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.1978 - train_accuracy: 0.4137 - val_loss: 1.1642 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.1975 - train_accuracy: 0.4137 - val_loss: 1.1639 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.1972 - train_accuracy: 0.4137 - val_loss: 1.1637 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.1969 - train_accuracy: 0.4137 - val_loss: 1.1634 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.1966 - train_accuracy: 0.4137 - val_loss: 1.1632 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.1963 - train_accuracy: 0.4137 - val_loss: 1.1630 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.1960 - train_accuracy: 0.4137 - val_loss: 1.1627 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.1_act=sigmoid_optim=bgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/r9ya605v' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/r9ya605v</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124939-r9ya605v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843bfaaa48554254acf39fbaa63b0e5f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124949-bc0l7yv8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/bc0l7yv8' target=\"_blank\">lr=0.1_act=sigmoid_optim=mbgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/bc0l7yv8' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/bc0l7yv8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.3007 - train_accuracy: 0.4111 - val_loss: 1.2731 - val_accuracy: 0.4419\n",
      "Epoch 2/100 - train_loss: 1.2287 - train_accuracy: 0.4111 - val_loss: 1.1935 - val_accuracy: 0.4419\n",
      "Epoch 3/100 - train_loss: 1.2075 - train_accuracy: 0.4111 - val_loss: 1.1710 - val_accuracy: 0.4419\n",
      "Epoch 4/100 - train_loss: 1.1976 - train_accuracy: 0.4111 - val_loss: 1.1619 - val_accuracy: 0.4419\n",
      "Epoch 5/100 - train_loss: 1.1921 - train_accuracy: 0.4111 - val_loss: 1.1576 - val_accuracy: 0.4419\n",
      "Epoch 6/100 - train_loss: 1.1887 - train_accuracy: 0.4111 - val_loss: 1.1557 - val_accuracy: 0.4419\n",
      "Epoch 7/100 - train_loss: 1.1865 - train_accuracy: 0.4111 - val_loss: 1.1549 - val_accuracy: 0.4419\n",
      "Epoch 8/100 - train_loss: 1.1849 - train_accuracy: 0.4111 - val_loss: 1.1547 - val_accuracy: 0.4419\n",
      "Epoch 9/100 - train_loss: 1.1837 - train_accuracy: 0.4111 - val_loss: 1.1549 - val_accuracy: 0.4419\n",
      "Epoch 10/100 - train_loss: 1.1828 - train_accuracy: 0.4111 - val_loss: 1.1553 - val_accuracy: 0.4419\n",
      "Epoch 11/100 - train_loss: 1.1821 - train_accuracy: 0.4111 - val_loss: 1.1558 - val_accuracy: 0.4419\n",
      "Epoch 12/100 - train_loss: 1.1815 - train_accuracy: 0.4111 - val_loss: 1.1564 - val_accuracy: 0.4419\n",
      "Epoch 13/100 - train_loss: 1.1810 - train_accuracy: 0.4111 - val_loss: 1.1570 - val_accuracy: 0.4419\n",
      "Epoch 14/100 - train_loss: 1.1806 - train_accuracy: 0.4111 - val_loss: 1.1576 - val_accuracy: 0.4419\n",
      "Epoch 15/100 - train_loss: 1.1802 - train_accuracy: 0.4111 - val_loss: 1.1582 - val_accuracy: 0.4419\n",
      "Epoch 16/100 - train_loss: 1.1799 - train_accuracy: 0.4111 - val_loss: 1.1588 - val_accuracy: 0.4419\n",
      "Epoch 17/100 - train_loss: 1.1796 - train_accuracy: 0.4111 - val_loss: 1.1594 - val_accuracy: 0.4419\n",
      "Epoch 18/100 - train_loss: 1.1794 - train_accuracy: 0.4111 - val_loss: 1.1600 - val_accuracy: 0.4419\n",
      "Epoch 19/100 - train_loss: 1.1791 - train_accuracy: 0.4111 - val_loss: 1.1605 - val_accuracy: 0.4419\n",
      "Epoch 20/100 - train_loss: 1.1789 - train_accuracy: 0.4111 - val_loss: 1.1611 - val_accuracy: 0.4419\n",
      "Epoch 21/100 - train_loss: 1.1787 - train_accuracy: 0.4111 - val_loss: 1.1616 - val_accuracy: 0.4419\n",
      "Epoch 22/100 - train_loss: 1.1786 - train_accuracy: 0.4111 - val_loss: 1.1621 - val_accuracy: 0.4419\n",
      "Epoch 23/100 - train_loss: 1.1784 - train_accuracy: 0.4111 - val_loss: 1.1625 - val_accuracy: 0.4419\n",
      "Epoch 24/100 - train_loss: 1.1782 - train_accuracy: 0.4111 - val_loss: 1.1630 - val_accuracy: 0.4419\n",
      "Epoch 25/100 - train_loss: 1.1781 - train_accuracy: 0.4111 - val_loss: 1.1634 - val_accuracy: 0.4419\n",
      "Epoch 26/100 - train_loss: 1.1780 - train_accuracy: 0.4111 - val_loss: 1.1638 - val_accuracy: 0.4419\n",
      "Epoch 27/100 - train_loss: 1.1778 - train_accuracy: 0.4111 - val_loss: 1.1642 - val_accuracy: 0.4419\n",
      "Epoch 28/100 - train_loss: 1.1777 - train_accuracy: 0.4111 - val_loss: 1.1646 - val_accuracy: 0.4419\n",
      "Epoch 29/100 - train_loss: 1.1776 - train_accuracy: 0.4111 - val_loss: 1.1650 - val_accuracy: 0.4419\n",
      "Epoch 30/100 - train_loss: 1.1775 - train_accuracy: 0.4111 - val_loss: 1.1653 - val_accuracy: 0.4419\n",
      "Epoch 31/100 - train_loss: 1.1774 - train_accuracy: 0.4111 - val_loss: 1.1657 - val_accuracy: 0.4419\n",
      "Epoch 32/100 - train_loss: 1.1772 - train_accuracy: 0.4111 - val_loss: 1.1660 - val_accuracy: 0.4419\n",
      "Epoch 33/100 - train_loss: 1.1771 - train_accuracy: 0.4111 - val_loss: 1.1663 - val_accuracy: 0.4419\n",
      "Epoch 34/100 - train_loss: 1.1770 - train_accuracy: 0.4111 - val_loss: 1.1666 - val_accuracy: 0.4419\n",
      "Epoch 35/100 - train_loss: 1.1769 - train_accuracy: 0.4111 - val_loss: 1.1669 - val_accuracy: 0.4419\n",
      "Epoch 36/100 - train_loss: 1.1768 - train_accuracy: 0.4111 - val_loss: 1.1672 - val_accuracy: 0.4419\n",
      "Epoch 37/100 - train_loss: 1.1767 - train_accuracy: 0.4111 - val_loss: 1.1675 - val_accuracy: 0.4419\n",
      "Epoch 38/100 - train_loss: 1.1766 - train_accuracy: 0.4111 - val_loss: 1.1678 - val_accuracy: 0.4419\n",
      "Epoch 39/100 - train_loss: 1.1765 - train_accuracy: 0.4111 - val_loss: 1.1680 - val_accuracy: 0.4419\n",
      "Epoch 40/100 - train_loss: 1.1764 - train_accuracy: 0.4111 - val_loss: 1.1683 - val_accuracy: 0.4419\n",
      "Epoch 41/100 - train_loss: 1.1763 - train_accuracy: 0.4111 - val_loss: 1.1685 - val_accuracy: 0.4419\n",
      "Epoch 42/100 - train_loss: 1.1762 - train_accuracy: 0.4111 - val_loss: 1.1687 - val_accuracy: 0.4419\n",
      "Epoch 43/100 - train_loss: 1.1761 - train_accuracy: 0.4111 - val_loss: 1.1689 - val_accuracy: 0.4419\n",
      "Epoch 44/100 - train_loss: 1.1760 - train_accuracy: 0.4111 - val_loss: 1.1691 - val_accuracy: 0.4419\n",
      "Epoch 45/100 - train_loss: 1.1759 - train_accuracy: 0.4111 - val_loss: 1.1693 - val_accuracy: 0.4419\n",
      "Epoch 46/100 - train_loss: 1.1758 - train_accuracy: 0.4111 - val_loss: 1.1695 - val_accuracy: 0.4419\n",
      "Epoch 47/100 - train_loss: 1.1756 - train_accuracy: 0.4111 - val_loss: 1.1697 - val_accuracy: 0.4419\n",
      "Epoch 48/100 - train_loss: 1.1755 - train_accuracy: 0.4111 - val_loss: 1.1699 - val_accuracy: 0.4419\n",
      "Epoch 49/100 - train_loss: 1.1754 - train_accuracy: 0.4111 - val_loss: 1.1701 - val_accuracy: 0.4419\n",
      "Epoch 50/100 - train_loss: 1.1753 - train_accuracy: 0.4111 - val_loss: 1.1702 - val_accuracy: 0.4419\n",
      "Epoch 51/100 - train_loss: 1.1751 - train_accuracy: 0.4111 - val_loss: 1.1704 - val_accuracy: 0.4419\n",
      "Epoch 52/100 - train_loss: 1.1750 - train_accuracy: 0.4111 - val_loss: 1.1705 - val_accuracy: 0.4419\n",
      "Epoch 53/100 - train_loss: 1.1748 - train_accuracy: 0.4111 - val_loss: 1.1706 - val_accuracy: 0.4419\n",
      "Epoch 54/100 - train_loss: 1.1747 - train_accuracy: 0.4111 - val_loss: 1.1708 - val_accuracy: 0.4419\n",
      "Epoch 55/100 - train_loss: 1.1745 - train_accuracy: 0.4111 - val_loss: 1.1709 - val_accuracy: 0.4419\n",
      "Epoch 56/100 - train_loss: 1.1744 - train_accuracy: 0.4111 - val_loss: 1.1710 - val_accuracy: 0.4419\n",
      "Epoch 57/100 - train_loss: 1.1742 - train_accuracy: 0.4111 - val_loss: 1.1711 - val_accuracy: 0.4419\n",
      "Epoch 58/100 - train_loss: 1.1740 - train_accuracy: 0.4111 - val_loss: 1.1711 - val_accuracy: 0.4419\n",
      "Epoch 59/100 - train_loss: 1.1738 - train_accuracy: 0.4111 - val_loss: 1.1712 - val_accuracy: 0.4419\n",
      "Epoch 60/100 - train_loss: 1.1736 - train_accuracy: 0.4111 - val_loss: 1.1713 - val_accuracy: 0.4419\n",
      "Epoch 61/100 - train_loss: 1.1734 - train_accuracy: 0.4111 - val_loss: 1.1713 - val_accuracy: 0.4419\n",
      "Epoch 62/100 - train_loss: 1.1732 - train_accuracy: 0.4111 - val_loss: 1.1714 - val_accuracy: 0.4419\n",
      "Epoch 63/100 - train_loss: 1.1729 - train_accuracy: 0.4111 - val_loss: 1.1714 - val_accuracy: 0.4419\n",
      "Epoch 64/100 - train_loss: 1.1726 - train_accuracy: 0.4111 - val_loss: 1.1714 - val_accuracy: 0.4419\n",
      "Epoch 65/100 - train_loss: 1.1724 - train_accuracy: 0.4111 - val_loss: 1.1714 - val_accuracy: 0.4419\n",
      "Epoch 66/100 - train_loss: 1.1721 - train_accuracy: 0.4111 - val_loss: 1.1713 - val_accuracy: 0.4419\n",
      "Epoch 67/100 - train_loss: 1.1718 - train_accuracy: 0.4111 - val_loss: 1.1713 - val_accuracy: 0.4419\n",
      "Epoch 68/100 - train_loss: 1.1714 - train_accuracy: 0.4111 - val_loss: 1.1712 - val_accuracy: 0.4419\n",
      "Epoch 69/100 - train_loss: 1.1711 - train_accuracy: 0.4111 - val_loss: 1.1712 - val_accuracy: 0.4419\n",
      "Epoch 70/100 - train_loss: 1.1707 - train_accuracy: 0.4111 - val_loss: 1.1711 - val_accuracy: 0.4419\n",
      "Epoch 71/100 - train_loss: 1.1703 - train_accuracy: 0.4111 - val_loss: 1.1709 - val_accuracy: 0.4419\n",
      "Epoch 72/100 - train_loss: 1.1699 - train_accuracy: 0.4111 - val_loss: 1.1708 - val_accuracy: 0.4419\n",
      "Epoch 73/100 - train_loss: 1.1694 - train_accuracy: 0.4111 - val_loss: 1.1706 - val_accuracy: 0.4419\n",
      "Epoch 74/100 - train_loss: 1.1689 - train_accuracy: 0.4111 - val_loss: 1.1704 - val_accuracy: 0.4419\n",
      "Epoch 75/100 - train_loss: 1.1684 - train_accuracy: 0.4111 - val_loss: 1.1702 - val_accuracy: 0.4419\n",
      "Epoch 76/100 - train_loss: 1.1678 - train_accuracy: 0.4097 - val_loss: 1.1699 - val_accuracy: 0.4419\n",
      "Epoch 77/100 - train_loss: 1.1672 - train_accuracy: 0.4097 - val_loss: 1.1696 - val_accuracy: 0.4419\n",
      "Epoch 78/100 - train_loss: 1.1666 - train_accuracy: 0.4111 - val_loss: 1.1693 - val_accuracy: 0.4419\n",
      "Epoch 79/100 - train_loss: 1.1659 - train_accuracy: 0.4111 - val_loss: 1.1689 - val_accuracy: 0.4419\n",
      "Epoch 80/100 - train_loss: 1.1651 - train_accuracy: 0.4111 - val_loss: 1.1685 - val_accuracy: 0.4419\n",
      "Epoch 81/100 - train_loss: 1.1643 - train_accuracy: 0.4137 - val_loss: 1.1681 - val_accuracy: 0.4419\n",
      "Epoch 82/100 - train_loss: 1.1634 - train_accuracy: 0.4164 - val_loss: 1.1675 - val_accuracy: 0.4477\n",
      "Epoch 83/100 - train_loss: 1.1625 - train_accuracy: 0.4218 - val_loss: 1.1670 - val_accuracy: 0.4535\n",
      "Epoch 84/100 - train_loss: 1.1615 - train_accuracy: 0.4286 - val_loss: 1.1664 - val_accuracy: 0.4593\n",
      "Epoch 85/100 - train_loss: 1.1605 - train_accuracy: 0.4407 - val_loss: 1.1657 - val_accuracy: 0.4593\n",
      "Epoch 86/100 - train_loss: 1.1593 - train_accuracy: 0.4434 - val_loss: 1.1650 - val_accuracy: 0.4593\n",
      "Epoch 87/100 - train_loss: 1.1581 - train_accuracy: 0.4488 - val_loss: 1.1641 - val_accuracy: 0.4593\n",
      "Epoch 88/100 - train_loss: 1.1568 - train_accuracy: 0.4501 - val_loss: 1.1633 - val_accuracy: 0.4767\n",
      "Epoch 89/100 - train_loss: 1.1554 - train_accuracy: 0.4609 - val_loss: 1.1623 - val_accuracy: 0.4826\n",
      "Epoch 90/100 - train_loss: 1.1539 - train_accuracy: 0.4677 - val_loss: 1.1613 - val_accuracy: 0.4826\n",
      "Epoch 91/100 - train_loss: 1.1523 - train_accuracy: 0.4757 - val_loss: 1.1602 - val_accuracy: 0.4942\n",
      "Epoch 92/100 - train_loss: 1.1506 - train_accuracy: 0.4798 - val_loss: 1.1589 - val_accuracy: 0.5116\n",
      "Epoch 93/100 - train_loss: 1.1488 - train_accuracy: 0.4852 - val_loss: 1.1576 - val_accuracy: 0.5058\n",
      "Epoch 94/100 - train_loss: 1.1468 - train_accuracy: 0.4879 - val_loss: 1.1562 - val_accuracy: 0.5174\n",
      "Epoch 95/100 - train_loss: 1.1448 - train_accuracy: 0.4919 - val_loss: 1.1547 - val_accuracy: 0.5233\n",
      "Epoch 96/100 - train_loss: 1.1426 - train_accuracy: 0.4946 - val_loss: 1.1531 - val_accuracy: 0.5349\n",
      "Epoch 97/100 - train_loss: 1.1403 - train_accuracy: 0.5000 - val_loss: 1.1514 - val_accuracy: 0.5291\n",
      "Epoch 98/100 - train_loss: 1.1378 - train_accuracy: 0.5000 - val_loss: 1.1495 - val_accuracy: 0.5349\n",
      "Epoch 99/100 - train_loss: 1.1353 - train_accuracy: 0.5000 - val_loss: 1.1476 - val_accuracy: 0.5233\n",
      "Epoch 100/100 - train_loss: 1.1325 - train_accuracy: 0.5067 - val_loss: 1.1455 - val_accuracy: 0.5407\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.1_act=sigmoid_optim=mbgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/bc0l7yv8' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/bc0l7yv8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124949-bc0l7yv8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9848dc091ddd4c108b3fa60aca754f93"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_124959-e13gg5pm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/e13gg5pm' target=\"_blank\">lr=0.1_act=tanh_optim=sgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/e13gg5pm' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/e13gg5pm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.1929 - train_accuracy: 0.4111 - val_loss: 1.1606 - val_accuracy: 0.4419\n",
      "Epoch 2/100 - train_loss: 1.0011 - train_accuracy: 0.5701 - val_loss: 0.9806 - val_accuracy: 0.6105\n",
      "Epoch 3/100 - train_loss: 0.9931 - train_accuracy: 0.5755 - val_loss: 0.9848 - val_accuracy: 0.6337\n",
      "Epoch 4/100 - train_loss: 0.9715 - train_accuracy: 0.5930 - val_loss: 0.9949 - val_accuracy: 0.5988\n",
      "Epoch 5/100 - train_loss: 0.9753 - train_accuracy: 0.5782 - val_loss: 0.9832 - val_accuracy: 0.6163\n",
      "Epoch 6/100 - train_loss: 0.9887 - train_accuracy: 0.5863 - val_loss: 1.0188 - val_accuracy: 0.6047\n",
      "Epoch 7/100 - train_loss: 0.9901 - train_accuracy: 0.5377 - val_loss: 1.0001 - val_accuracy: 0.5930\n",
      "Epoch 8/100 - train_loss: 0.9774 - train_accuracy: 0.5849 - val_loss: 1.0117 - val_accuracy: 0.6105\n",
      "Epoch 9/100 - train_loss: 0.9870 - train_accuracy: 0.5782 - val_loss: 1.0182 - val_accuracy: 0.6163\n",
      "Epoch 10/100 - train_loss: 0.9908 - train_accuracy: 0.5121 - val_loss: 1.0134 - val_accuracy: 0.5523\n",
      "Epoch 11/100 - train_loss: 0.9818 - train_accuracy: 0.5216 - val_loss: 1.0051 - val_accuracy: 0.5581\n",
      "Epoch 12/100 - train_loss: 0.9815 - train_accuracy: 0.5216 - val_loss: 1.0008 - val_accuracy: 0.5581\n",
      "Epoch 13/100 - train_loss: 1.0027 - train_accuracy: 0.5162 - val_loss: 0.9953 - val_accuracy: 0.5872\n",
      "Epoch 14/100 - train_loss: 0.9994 - train_accuracy: 0.5391 - val_loss: 0.9920 - val_accuracy: 0.5872\n",
      "Epoch 15/100 - train_loss: 0.9731 - train_accuracy: 0.5391 - val_loss: 1.0006 - val_accuracy: 0.5756\n",
      "Epoch 16/100 - train_loss: 0.9599 - train_accuracy: 0.5458 - val_loss: 0.9957 - val_accuracy: 0.5640\n",
      "Epoch 17/100 - train_loss: 0.9615 - train_accuracy: 0.5283 - val_loss: 0.9826 - val_accuracy: 0.5581\n",
      "Epoch 18/100 - train_loss: 0.9494 - train_accuracy: 0.5445 - val_loss: 1.0008 - val_accuracy: 0.5756\n",
      "Epoch 19/100 - train_loss: 0.9501 - train_accuracy: 0.5566 - val_loss: 0.9956 - val_accuracy: 0.5814\n",
      "Epoch 20/100 - train_loss: 0.9386 - train_accuracy: 0.5930 - val_loss: 1.0082 - val_accuracy: 0.5756\n",
      "Epoch 21/100 - train_loss: 0.9580 - train_accuracy: 0.6011 - val_loss: 1.0607 - val_accuracy: 0.5523\n",
      "Epoch 22/100 - train_loss: 0.9333 - train_accuracy: 0.6092 - val_loss: 1.0262 - val_accuracy: 0.5930\n",
      "Epoch 23/100 - train_loss: 0.9046 - train_accuracy: 0.6078 - val_loss: 1.0399 - val_accuracy: 0.5640\n",
      "Epoch 24/100 - train_loss: 0.9216 - train_accuracy: 0.6051 - val_loss: 1.0540 - val_accuracy: 0.5930\n",
      "Epoch 25/100 - train_loss: 0.9537 - train_accuracy: 0.5930 - val_loss: 1.0648 - val_accuracy: 0.5756\n",
      "Epoch 26/100 - train_loss: 0.9253 - train_accuracy: 0.5984 - val_loss: 1.0658 - val_accuracy: 0.5814\n",
      "Epoch 27/100 - train_loss: 0.9270 - train_accuracy: 0.6092 - val_loss: 1.0584 - val_accuracy: 0.5814\n",
      "Epoch 28/100 - train_loss: 0.9124 - train_accuracy: 0.6119 - val_loss: 1.0987 - val_accuracy: 0.5233\n",
      "Epoch 29/100 - train_loss: 0.8909 - train_accuracy: 0.6173 - val_loss: 1.0975 - val_accuracy: 0.5349\n",
      "Epoch 30/100 - train_loss: 0.8868 - train_accuracy: 0.6321 - val_loss: 1.0586 - val_accuracy: 0.5407\n",
      "Epoch 31/100 - train_loss: 0.8804 - train_accuracy: 0.6078 - val_loss: 1.0375 - val_accuracy: 0.5291\n",
      "Epoch 32/100 - train_loss: 0.8718 - train_accuracy: 0.6173 - val_loss: 1.0886 - val_accuracy: 0.5174\n",
      "Epoch 33/100 - train_loss: 0.8910 - train_accuracy: 0.5795 - val_loss: 1.0821 - val_accuracy: 0.5465\n",
      "Epoch 34/100 - train_loss: 0.9177 - train_accuracy: 0.5943 - val_loss: 1.1293 - val_accuracy: 0.5233\n",
      "Epoch 35/100 - train_loss: 0.9143 - train_accuracy: 0.5903 - val_loss: 1.1814 - val_accuracy: 0.5349\n",
      "Epoch 36/100 - train_loss: 0.9164 - train_accuracy: 0.5957 - val_loss: 1.1835 - val_accuracy: 0.5291\n",
      "Epoch 37/100 - train_loss: 0.9913 - train_accuracy: 0.5916 - val_loss: 1.2169 - val_accuracy: 0.5349\n",
      "Epoch 38/100 - train_loss: 0.9401 - train_accuracy: 0.6092 - val_loss: 1.1660 - val_accuracy: 0.5116\n",
      "Epoch 39/100 - train_loss: 0.9122 - train_accuracy: 0.6065 - val_loss: 1.1331 - val_accuracy: 0.5233\n",
      "Epoch 40/100 - train_loss: 0.8989 - train_accuracy: 0.6550 - val_loss: 1.1478 - val_accuracy: 0.5465\n",
      "Epoch 41/100 - train_loss: 0.8854 - train_accuracy: 0.6604 - val_loss: 1.1830 - val_accuracy: 0.5349\n",
      "Epoch 42/100 - train_loss: 0.9050 - train_accuracy: 0.6294 - val_loss: 1.1190 - val_accuracy: 0.5407\n",
      "Epoch 43/100 - train_loss: 0.8840 - train_accuracy: 0.6348 - val_loss: 1.1457 - val_accuracy: 0.5349\n",
      "Epoch 44/100 - train_loss: 0.8978 - train_accuracy: 0.6011 - val_loss: 1.1763 - val_accuracy: 0.5349\n",
      "Epoch 45/100 - train_loss: 0.8766 - train_accuracy: 0.6173 - val_loss: 1.1303 - val_accuracy: 0.5291\n",
      "Epoch 46/100 - train_loss: 0.9215 - train_accuracy: 0.5930 - val_loss: 1.1243 - val_accuracy: 0.5291\n",
      "Epoch 47/100 - train_loss: 0.8866 - train_accuracy: 0.6051 - val_loss: 1.1136 - val_accuracy: 0.5000\n",
      "Epoch 48/100 - train_loss: 0.8745 - train_accuracy: 0.6159 - val_loss: 1.1345 - val_accuracy: 0.5233\n",
      "Epoch 49/100 - train_loss: 0.8975 - train_accuracy: 0.6092 - val_loss: 1.1560 - val_accuracy: 0.5000\n",
      "Epoch 50/100 - train_loss: 0.8644 - train_accuracy: 0.6173 - val_loss: 1.0710 - val_accuracy: 0.5640\n",
      "Epoch 51/100 - train_loss: 0.9046 - train_accuracy: 0.6105 - val_loss: 1.0844 - val_accuracy: 0.5349\n",
      "Epoch 52/100 - train_loss: 0.8755 - train_accuracy: 0.6375 - val_loss: 1.0787 - val_accuracy: 0.5581\n",
      "Epoch 53/100 - train_loss: 0.8901 - train_accuracy: 0.6159 - val_loss: 1.1001 - val_accuracy: 0.5581\n",
      "Epoch 54/100 - train_loss: 0.9182 - train_accuracy: 0.6456 - val_loss: 1.1603 - val_accuracy: 0.5465\n",
      "Epoch 55/100 - train_loss: 0.8986 - train_accuracy: 0.6280 - val_loss: 1.1372 - val_accuracy: 0.5291\n",
      "Epoch 56/100 - train_loss: 0.8948 - train_accuracy: 0.6092 - val_loss: 1.1279 - val_accuracy: 0.5000\n",
      "Epoch 57/100 - train_loss: 0.8945 - train_accuracy: 0.6132 - val_loss: 1.1238 - val_accuracy: 0.5640\n",
      "Epoch 58/100 - train_loss: 0.8693 - train_accuracy: 0.6550 - val_loss: 1.1324 - val_accuracy: 0.5523\n",
      "Epoch 59/100 - train_loss: 0.9018 - train_accuracy: 0.6536 - val_loss: 1.1520 - val_accuracy: 0.5523\n",
      "Epoch 60/100 - train_loss: 0.8497 - train_accuracy: 0.6617 - val_loss: 1.0987 - val_accuracy: 0.5523\n",
      "Epoch 61/100 - train_loss: 0.8637 - train_accuracy: 0.6563 - val_loss: 1.1737 - val_accuracy: 0.5640\n",
      "Epoch 62/100 - train_loss: 0.8893 - train_accuracy: 0.6119 - val_loss: 1.0877 - val_accuracy: 0.5581\n",
      "Epoch 63/100 - train_loss: 0.8680 - train_accuracy: 0.6334 - val_loss: 1.0808 - val_accuracy: 0.5174\n",
      "Epoch 64/100 - train_loss: 0.8883 - train_accuracy: 0.6226 - val_loss: 1.0826 - val_accuracy: 0.5407\n",
      "Epoch 65/100 - train_loss: 0.8891 - train_accuracy: 0.6213 - val_loss: 1.1121 - val_accuracy: 0.5407\n",
      "Epoch 66/100 - train_loss: 0.9447 - train_accuracy: 0.5889 - val_loss: 1.2004 - val_accuracy: 0.5058\n",
      "Epoch 67/100 - train_loss: 0.9278 - train_accuracy: 0.5809 - val_loss: 1.1671 - val_accuracy: 0.5291\n",
      "Epoch 68/100 - train_loss: 0.9549 - train_accuracy: 0.6226 - val_loss: 1.2194 - val_accuracy: 0.5291\n",
      "Epoch 69/100 - train_loss: 0.9521 - train_accuracy: 0.6105 - val_loss: 1.1765 - val_accuracy: 0.5233\n",
      "Epoch 70/100 - train_loss: 0.9231 - train_accuracy: 0.6092 - val_loss: 1.1507 - val_accuracy: 0.5058\n",
      "Epoch 71/100 - train_loss: 0.9387 - train_accuracy: 0.6186 - val_loss: 1.1333 - val_accuracy: 0.5523\n",
      "Epoch 72/100 - train_loss: 0.9152 - train_accuracy: 0.6348 - val_loss: 1.2159 - val_accuracy: 0.5349\n",
      "Epoch 73/100 - train_loss: 0.9085 - train_accuracy: 0.6173 - val_loss: 1.1447 - val_accuracy: 0.5465\n",
      "Epoch 74/100 - train_loss: 0.9313 - train_accuracy: 0.6186 - val_loss: 1.1820 - val_accuracy: 0.5233\n",
      "Epoch 75/100 - train_loss: 0.8904 - train_accuracy: 0.6199 - val_loss: 1.1496 - val_accuracy: 0.5581\n",
      "Epoch 76/100 - train_loss: 0.8646 - train_accuracy: 0.6334 - val_loss: 1.1161 - val_accuracy: 0.5233\n",
      "Epoch 77/100 - train_loss: 0.8657 - train_accuracy: 0.6375 - val_loss: 1.1289 - val_accuracy: 0.5698\n",
      "Epoch 78/100 - train_loss: 0.8827 - train_accuracy: 0.6199 - val_loss: 1.0671 - val_accuracy: 0.5349\n",
      "Epoch 79/100 - train_loss: 0.8952 - train_accuracy: 0.6267 - val_loss: 1.1431 - val_accuracy: 0.5523\n",
      "Epoch 80/100 - train_loss: 0.9379 - train_accuracy: 0.5970 - val_loss: 1.1473 - val_accuracy: 0.5291\n",
      "Epoch 81/100 - train_loss: 0.9203 - train_accuracy: 0.6105 - val_loss: 1.1499 - val_accuracy: 0.5523\n",
      "Epoch 82/100 - train_loss: 0.9026 - train_accuracy: 0.6226 - val_loss: 1.0854 - val_accuracy: 0.5814\n",
      "Epoch 83/100 - train_loss: 0.8904 - train_accuracy: 0.6307 - val_loss: 1.0965 - val_accuracy: 0.5523\n",
      "Epoch 84/100 - train_loss: 0.9139 - train_accuracy: 0.6240 - val_loss: 1.1526 - val_accuracy: 0.5523\n",
      "Epoch 85/100 - train_loss: 0.8999 - train_accuracy: 0.6159 - val_loss: 1.1334 - val_accuracy: 0.5523\n",
      "Epoch 86/100 - train_loss: 0.9583 - train_accuracy: 0.6078 - val_loss: 1.1886 - val_accuracy: 0.5698\n",
      "Epoch 87/100 - train_loss: 0.9206 - train_accuracy: 0.6159 - val_loss: 1.1379 - val_accuracy: 0.5523\n",
      "Epoch 88/100 - train_loss: 0.9697 - train_accuracy: 0.5970 - val_loss: 1.2031 - val_accuracy: 0.5349\n",
      "Epoch 89/100 - train_loss: 0.9391 - train_accuracy: 0.6294 - val_loss: 1.1237 - val_accuracy: 0.5930\n",
      "Epoch 90/100 - train_loss: 0.9701 - train_accuracy: 0.6065 - val_loss: 1.2179 - val_accuracy: 0.5523\n",
      "Epoch 91/100 - train_loss: 0.8959 - train_accuracy: 0.6563 - val_loss: 1.1343 - val_accuracy: 0.5930\n",
      "Epoch 92/100 - train_loss: 0.9327 - train_accuracy: 0.6146 - val_loss: 1.1227 - val_accuracy: 0.5756\n",
      "Epoch 93/100 - train_loss: 0.9518 - train_accuracy: 0.6186 - val_loss: 1.1942 - val_accuracy: 0.5756\n",
      "Epoch 94/100 - train_loss: 0.9097 - train_accuracy: 0.6334 - val_loss: 1.2418 - val_accuracy: 0.5407\n",
      "Epoch 95/100 - train_loss: 0.9303 - train_accuracy: 0.6321 - val_loss: 1.1556 - val_accuracy: 0.5407\n",
      "Epoch 96/100 - train_loss: 0.9406 - train_accuracy: 0.6509 - val_loss: 1.2328 - val_accuracy: 0.5640\n",
      "Epoch 97/100 - train_loss: 0.9408 - train_accuracy: 0.6402 - val_loss: 1.1685 - val_accuracy: 0.5988\n",
      "Epoch 98/100 - train_loss: 0.9367 - train_accuracy: 0.6509 - val_loss: 1.1857 - val_accuracy: 0.5988\n",
      "Epoch 99/100 - train_loss: 0.9473 - train_accuracy: 0.5957 - val_loss: 1.1768 - val_accuracy: 0.5465\n",
      "Epoch 100/100 - train_loss: 0.9618 - train_accuracy: 0.6051 - val_loss: 1.2321 - val_accuracy: 0.5523\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e57e167c60b49629801b97c08f79200"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.1_act=tanh_optim=sgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/e13gg5pm' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/e13gg5pm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124959-e13gg5pm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c60e7667ac84d35ba013d96097cca03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_125013-8c9m0665</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/8c9m0665' target=\"_blank\">lr=0.1_act=tanh_optim=bgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/8c9m0665' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/8c9m0665</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7728 - train_accuracy: 0.4151 - val_loss: 1.7721 - val_accuracy: 0.4070\n",
      "Epoch 2/100 - train_loss: 1.7546 - train_accuracy: 0.4137 - val_loss: 1.7531 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.7369 - train_accuracy: 0.4137 - val_loss: 1.7348 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.7198 - train_accuracy: 0.4137 - val_loss: 1.7170 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.7033 - train_accuracy: 0.4137 - val_loss: 1.6998 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.6874 - train_accuracy: 0.4137 - val_loss: 1.6833 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.6721 - train_accuracy: 0.4137 - val_loss: 1.6672 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.6572 - train_accuracy: 0.4137 - val_loss: 1.6518 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.6429 - train_accuracy: 0.4137 - val_loss: 1.6368 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.6291 - train_accuracy: 0.4137 - val_loss: 1.6224 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.6158 - train_accuracy: 0.4137 - val_loss: 1.6085 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.6029 - train_accuracy: 0.4137 - val_loss: 1.5950 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.5905 - train_accuracy: 0.4137 - val_loss: 1.5821 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.5786 - train_accuracy: 0.4137 - val_loss: 1.5695 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.5670 - train_accuracy: 0.4137 - val_loss: 1.5574 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.5559 - train_accuracy: 0.4137 - val_loss: 1.5458 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.5452 - train_accuracy: 0.4137 - val_loss: 1.5345 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.5349 - train_accuracy: 0.4137 - val_loss: 1.5237 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.5249 - train_accuracy: 0.4137 - val_loss: 1.5132 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.5153 - train_accuracy: 0.4137 - val_loss: 1.5031 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.5060 - train_accuracy: 0.4137 - val_loss: 1.4933 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.4971 - train_accuracy: 0.4137 - val_loss: 1.4839 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.4884 - train_accuracy: 0.4137 - val_loss: 1.4748 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.4801 - train_accuracy: 0.4137 - val_loss: 1.4660 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.4721 - train_accuracy: 0.4137 - val_loss: 1.4575 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.4644 - train_accuracy: 0.4137 - val_loss: 1.4493 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.4569 - train_accuracy: 0.4137 - val_loss: 1.4414 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.4497 - train_accuracy: 0.4137 - val_loss: 1.4338 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.4427 - train_accuracy: 0.4137 - val_loss: 1.4264 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.4360 - train_accuracy: 0.4137 - val_loss: 1.4193 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.4295 - train_accuracy: 0.4137 - val_loss: 1.4124 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.4233 - train_accuracy: 0.4137 - val_loss: 1.4058 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.4173 - train_accuracy: 0.4137 - val_loss: 1.3994 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.4114 - train_accuracy: 0.4137 - val_loss: 1.3932 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.4058 - train_accuracy: 0.4137 - val_loss: 1.3872 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.4003 - train_accuracy: 0.4137 - val_loss: 1.3814 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.3951 - train_accuracy: 0.4137 - val_loss: 1.3757 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.3900 - train_accuracy: 0.4137 - val_loss: 1.3703 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.3851 - train_accuracy: 0.4137 - val_loss: 1.3651 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.3803 - train_accuracy: 0.4137 - val_loss: 1.3600 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.3758 - train_accuracy: 0.4137 - val_loss: 1.3551 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.3713 - train_accuracy: 0.4137 - val_loss: 1.3503 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.3670 - train_accuracy: 0.4137 - val_loss: 1.3457 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.3629 - train_accuracy: 0.4137 - val_loss: 1.3413 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.3588 - train_accuracy: 0.4137 - val_loss: 1.3370 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.3550 - train_accuracy: 0.4137 - val_loss: 1.3328 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.3512 - train_accuracy: 0.4137 - val_loss: 1.3287 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.3475 - train_accuracy: 0.4137 - val_loss: 1.3248 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.3440 - train_accuracy: 0.4137 - val_loss: 1.3210 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.3406 - train_accuracy: 0.4137 - val_loss: 1.3173 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.3373 - train_accuracy: 0.4137 - val_loss: 1.3137 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.3340 - train_accuracy: 0.4137 - val_loss: 1.3102 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.3309 - train_accuracy: 0.4137 - val_loss: 1.3069 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.3279 - train_accuracy: 0.4137 - val_loss: 1.3036 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.3250 - train_accuracy: 0.4137 - val_loss: 1.3004 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.3221 - train_accuracy: 0.4137 - val_loss: 1.2974 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.3194 - train_accuracy: 0.4137 - val_loss: 1.2944 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.3167 - train_accuracy: 0.4137 - val_loss: 1.2915 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.3141 - train_accuracy: 0.4137 - val_loss: 1.2887 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.3116 - train_accuracy: 0.4137 - val_loss: 1.2859 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.3091 - train_accuracy: 0.4137 - val_loss: 1.2833 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.3067 - train_accuracy: 0.4137 - val_loss: 1.2807 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.3044 - train_accuracy: 0.4137 - val_loss: 1.2782 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.3022 - train_accuracy: 0.4137 - val_loss: 1.2757 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.3000 - train_accuracy: 0.4137 - val_loss: 1.2733 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.2978 - train_accuracy: 0.4137 - val_loss: 1.2710 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.2958 - train_accuracy: 0.4137 - val_loss: 1.2688 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.2938 - train_accuracy: 0.4137 - val_loss: 1.2666 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.2918 - train_accuracy: 0.4137 - val_loss: 1.2645 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.2899 - train_accuracy: 0.4137 - val_loss: 1.2624 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.2881 - train_accuracy: 0.4137 - val_loss: 1.2604 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.2862 - train_accuracy: 0.4137 - val_loss: 1.2584 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.2845 - train_accuracy: 0.4137 - val_loss: 1.2565 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.2828 - train_accuracy: 0.4137 - val_loss: 1.2546 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.2811 - train_accuracy: 0.4137 - val_loss: 1.2528 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.2795 - train_accuracy: 0.4137 - val_loss: 1.2510 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.2779 - train_accuracy: 0.4137 - val_loss: 1.2493 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.2764 - train_accuracy: 0.4137 - val_loss: 1.2476 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.2749 - train_accuracy: 0.4137 - val_loss: 1.2460 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.2734 - train_accuracy: 0.4137 - val_loss: 1.2444 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.2720 - train_accuracy: 0.4137 - val_loss: 1.2428 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.2706 - train_accuracy: 0.4137 - val_loss: 1.2413 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.2692 - train_accuracy: 0.4137 - val_loss: 1.2398 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.2679 - train_accuracy: 0.4137 - val_loss: 1.2383 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.2666 - train_accuracy: 0.4137 - val_loss: 1.2369 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.2653 - train_accuracy: 0.4137 - val_loss: 1.2355 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.2641 - train_accuracy: 0.4137 - val_loss: 1.2342 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.2629 - train_accuracy: 0.4137 - val_loss: 1.2328 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.2617 - train_accuracy: 0.4137 - val_loss: 1.2315 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.2605 - train_accuracy: 0.4137 - val_loss: 1.2303 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.2594 - train_accuracy: 0.4137 - val_loss: 1.2290 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.2583 - train_accuracy: 0.4137 - val_loss: 1.2278 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.2572 - train_accuracy: 0.4137 - val_loss: 1.2267 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.2562 - train_accuracy: 0.4137 - val_loss: 1.2255 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.2552 - train_accuracy: 0.4137 - val_loss: 1.2244 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.2542 - train_accuracy: 0.4137 - val_loss: 1.2233 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.2532 - train_accuracy: 0.4137 - val_loss: 1.2222 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.2522 - train_accuracy: 0.4137 - val_loss: 1.2211 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.2513 - train_accuracy: 0.4137 - val_loss: 1.2201 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.2503 - train_accuracy: 0.4137 - val_loss: 1.2191 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4432a987d441e3a057cbe537a13390"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.1_act=tanh_optim=bgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/8c9m0665' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/8c9m0665</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_125013-8c9m0665/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3956a064d5449e0ac7332fcfca17f85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_125023-3cs4rv4y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/3cs4rv4y' target=\"_blank\">lr=0.1_act=tanh_optim=mbgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/3cs4rv4y' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/3cs4rv4y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.4773 - train_accuracy: 0.4137 - val_loss: 1.4628 - val_accuracy: 0.4070\n",
      "Epoch 2/100 - train_loss: 1.3450 - train_accuracy: 0.4137 - val_loss: 1.3216 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.2847 - train_accuracy: 0.4137 - val_loss: 1.2560 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.2534 - train_accuracy: 0.4137 - val_loss: 1.2215 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.2352 - train_accuracy: 0.4137 - val_loss: 1.2015 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.2234 - train_accuracy: 0.4137 - val_loss: 1.1887 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.2153 - train_accuracy: 0.4137 - val_loss: 1.1801 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.2093 - train_accuracy: 0.4137 - val_loss: 1.1740 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.2048 - train_accuracy: 0.4137 - val_loss: 1.1695 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.2012 - train_accuracy: 0.4137 - val_loss: 1.1662 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.1983 - train_accuracy: 0.4137 - val_loss: 1.1636 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.1959 - train_accuracy: 0.4137 - val_loss: 1.1616 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.1940 - train_accuracy: 0.4137 - val_loss: 1.1601 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.1923 - train_accuracy: 0.4137 - val_loss: 1.1589 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.1909 - train_accuracy: 0.4137 - val_loss: 1.1580 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.1898 - train_accuracy: 0.4137 - val_loss: 1.1573 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.1887 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.1878 - train_accuracy: 0.4137 - val_loss: 1.1564 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.1871 - train_accuracy: 0.4137 - val_loss: 1.1561 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.1864 - train_accuracy: 0.4137 - val_loss: 1.1559 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.1858 - train_accuracy: 0.4137 - val_loss: 1.1557 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.1852 - train_accuracy: 0.4137 - val_loss: 1.1556 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.1847 - train_accuracy: 0.4137 - val_loss: 1.1556 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.1843 - train_accuracy: 0.4137 - val_loss: 1.1556 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.1839 - train_accuracy: 0.4137 - val_loss: 1.1557 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.1835 - train_accuracy: 0.4137 - val_loss: 1.1558 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.1831 - train_accuracy: 0.4137 - val_loss: 1.1559 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.1828 - train_accuracy: 0.4164 - val_loss: 1.1560 - val_accuracy: 0.4128\n",
      "Epoch 29/100 - train_loss: 1.1825 - train_accuracy: 0.4326 - val_loss: 1.1561 - val_accuracy: 0.4128\n",
      "Epoch 30/100 - train_loss: 1.1822 - train_accuracy: 0.4838 - val_loss: 1.1562 - val_accuracy: 0.5000\n",
      "Epoch 31/100 - train_loss: 1.1819 - train_accuracy: 0.5485 - val_loss: 1.1564 - val_accuracy: 0.5698\n",
      "Epoch 32/100 - train_loss: 1.1815 - train_accuracy: 0.5647 - val_loss: 1.1565 - val_accuracy: 0.5349\n",
      "Epoch 33/100 - train_loss: 1.1812 - train_accuracy: 0.5701 - val_loss: 1.1566 - val_accuracy: 0.5233\n",
      "Epoch 34/100 - train_loss: 1.1808 - train_accuracy: 0.5593 - val_loss: 1.1566 - val_accuracy: 0.5349\n",
      "Epoch 35/100 - train_loss: 1.1803 - train_accuracy: 0.5485 - val_loss: 1.1565 - val_accuracy: 0.5349\n",
      "Epoch 36/100 - train_loss: 1.1796 - train_accuracy: 0.5593 - val_loss: 1.1563 - val_accuracy: 0.5349\n",
      "Epoch 37/100 - train_loss: 1.1784 - train_accuracy: 0.5580 - val_loss: 1.1556 - val_accuracy: 0.5349\n",
      "Epoch 38/100 - train_loss: 1.1763 - train_accuracy: 0.5606 - val_loss: 1.1541 - val_accuracy: 0.5349\n",
      "Epoch 39/100 - train_loss: 1.1717 - train_accuracy: 0.5606 - val_loss: 1.1504 - val_accuracy: 0.5291\n",
      "Epoch 40/100 - train_loss: 1.1600 - train_accuracy: 0.5647 - val_loss: 1.1404 - val_accuracy: 0.5291\n",
      "Epoch 41/100 - train_loss: 1.1281 - train_accuracy: 0.5674 - val_loss: 1.1125 - val_accuracy: 0.5349\n",
      "Epoch 42/100 - train_loss: 1.0684 - train_accuracy: 0.5714 - val_loss: 1.0604 - val_accuracy: 0.5465\n",
      "Epoch 43/100 - train_loss: 1.0178 - train_accuracy: 0.5863 - val_loss: 1.0169 - val_accuracy: 0.5581\n",
      "Epoch 44/100 - train_loss: 0.9911 - train_accuracy: 0.6011 - val_loss: 0.9936 - val_accuracy: 0.5872\n",
      "Epoch 45/100 - train_loss: 0.9771 - train_accuracy: 0.6011 - val_loss: 0.9805 - val_accuracy: 0.5930\n",
      "Epoch 46/100 - train_loss: 0.9683 - train_accuracy: 0.6024 - val_loss: 0.9718 - val_accuracy: 0.5988\n",
      "Epoch 47/100 - train_loss: 0.9620 - train_accuracy: 0.6011 - val_loss: 0.9654 - val_accuracy: 0.6047\n",
      "Epoch 48/100 - train_loss: 0.9573 - train_accuracy: 0.5997 - val_loss: 0.9608 - val_accuracy: 0.6105\n",
      "Epoch 49/100 - train_loss: 0.9535 - train_accuracy: 0.5984 - val_loss: 0.9575 - val_accuracy: 0.6105\n",
      "Epoch 50/100 - train_loss: 0.9504 - train_accuracy: 0.5970 - val_loss: 0.9553 - val_accuracy: 0.6105\n",
      "Epoch 51/100 - train_loss: 0.9479 - train_accuracy: 0.5957 - val_loss: 0.9538 - val_accuracy: 0.6163\n",
      "Epoch 52/100 - train_loss: 0.9458 - train_accuracy: 0.5984 - val_loss: 0.9529 - val_accuracy: 0.6163\n",
      "Epoch 53/100 - train_loss: 0.9440 - train_accuracy: 0.6051 - val_loss: 0.9523 - val_accuracy: 0.6163\n",
      "Epoch 54/100 - train_loss: 0.9424 - train_accuracy: 0.6065 - val_loss: 0.9520 - val_accuracy: 0.6163\n",
      "Epoch 55/100 - train_loss: 0.9411 - train_accuracy: 0.6078 - val_loss: 0.9520 - val_accuracy: 0.6163\n",
      "Epoch 56/100 - train_loss: 0.9399 - train_accuracy: 0.6065 - val_loss: 0.9520 - val_accuracy: 0.6163\n",
      "Epoch 57/100 - train_loss: 0.9389 - train_accuracy: 0.6092 - val_loss: 0.9522 - val_accuracy: 0.6163\n",
      "Epoch 58/100 - train_loss: 0.9380 - train_accuracy: 0.6092 - val_loss: 0.9524 - val_accuracy: 0.6163\n",
      "Epoch 59/100 - train_loss: 0.9372 - train_accuracy: 0.6105 - val_loss: 0.9526 - val_accuracy: 0.6163\n",
      "Epoch 60/100 - train_loss: 0.9365 - train_accuracy: 0.6092 - val_loss: 0.9529 - val_accuracy: 0.6163\n",
      "Epoch 61/100 - train_loss: 0.9358 - train_accuracy: 0.6092 - val_loss: 0.9532 - val_accuracy: 0.6163\n",
      "Epoch 62/100 - train_loss: 0.9352 - train_accuracy: 0.6092 - val_loss: 0.9534 - val_accuracy: 0.6163\n",
      "Epoch 63/100 - train_loss: 0.9346 - train_accuracy: 0.6092 - val_loss: 0.9537 - val_accuracy: 0.6163\n",
      "Epoch 64/100 - train_loss: 0.9341 - train_accuracy: 0.6105 - val_loss: 0.9539 - val_accuracy: 0.6163\n",
      "Epoch 65/100 - train_loss: 0.9336 - train_accuracy: 0.6105 - val_loss: 0.9542 - val_accuracy: 0.6163\n",
      "Epoch 66/100 - train_loss: 0.9331 - train_accuracy: 0.6105 - val_loss: 0.9544 - val_accuracy: 0.6163\n",
      "Epoch 67/100 - train_loss: 0.9326 - train_accuracy: 0.6105 - val_loss: 0.9546 - val_accuracy: 0.6163\n",
      "Epoch 68/100 - train_loss: 0.9321 - train_accuracy: 0.6105 - val_loss: 0.9548 - val_accuracy: 0.6163\n",
      "Epoch 69/100 - train_loss: 0.9316 - train_accuracy: 0.6105 - val_loss: 0.9550 - val_accuracy: 0.6163\n",
      "Epoch 70/100 - train_loss: 0.9310 - train_accuracy: 0.6105 - val_loss: 0.9551 - val_accuracy: 0.6163\n",
      "Epoch 71/100 - train_loss: 0.9305 - train_accuracy: 0.6105 - val_loss: 0.9553 - val_accuracy: 0.6105\n",
      "Epoch 72/100 - train_loss: 0.9299 - train_accuracy: 0.6105 - val_loss: 0.9554 - val_accuracy: 0.6105\n",
      "Epoch 73/100 - train_loss: 0.9292 - train_accuracy: 0.6092 - val_loss: 0.9556 - val_accuracy: 0.6047\n",
      "Epoch 74/100 - train_loss: 0.9285 - train_accuracy: 0.6092 - val_loss: 0.9557 - val_accuracy: 0.6047\n",
      "Epoch 75/100 - train_loss: 0.9278 - train_accuracy: 0.6105 - val_loss: 0.9558 - val_accuracy: 0.6047\n",
      "Epoch 76/100 - train_loss: 0.9270 - train_accuracy: 0.6105 - val_loss: 0.9560 - val_accuracy: 0.6047\n",
      "Epoch 77/100 - train_loss: 0.9262 - train_accuracy: 0.6092 - val_loss: 0.9561 - val_accuracy: 0.6047\n",
      "Epoch 78/100 - train_loss: 0.9253 - train_accuracy: 0.6092 - val_loss: 0.9562 - val_accuracy: 0.6047\n",
      "Epoch 79/100 - train_loss: 0.9243 - train_accuracy: 0.6159 - val_loss: 0.9563 - val_accuracy: 0.6047\n",
      "Epoch 80/100 - train_loss: 0.9232 - train_accuracy: 0.6159 - val_loss: 0.9565 - val_accuracy: 0.6047\n",
      "Epoch 81/100 - train_loss: 0.9221 - train_accuracy: 0.6146 - val_loss: 0.9566 - val_accuracy: 0.6047\n",
      "Epoch 82/100 - train_loss: 0.9209 - train_accuracy: 0.6159 - val_loss: 0.9568 - val_accuracy: 0.6047\n",
      "Epoch 83/100 - train_loss: 0.9196 - train_accuracy: 0.6199 - val_loss: 0.9570 - val_accuracy: 0.6047\n",
      "Epoch 84/100 - train_loss: 0.9183 - train_accuracy: 0.6186 - val_loss: 0.9572 - val_accuracy: 0.6105\n",
      "Epoch 85/100 - train_loss: 0.9169 - train_accuracy: 0.6226 - val_loss: 0.9574 - val_accuracy: 0.6105\n",
      "Epoch 86/100 - train_loss: 0.9155 - train_accuracy: 0.6213 - val_loss: 0.9576 - val_accuracy: 0.6105\n",
      "Epoch 87/100 - train_loss: 0.9140 - train_accuracy: 0.6226 - val_loss: 0.9578 - val_accuracy: 0.6163\n",
      "Epoch 88/100 - train_loss: 0.9125 - train_accuracy: 0.6186 - val_loss: 0.9581 - val_accuracy: 0.6105\n",
      "Epoch 89/100 - train_loss: 0.9110 - train_accuracy: 0.6186 - val_loss: 0.9583 - val_accuracy: 0.6105\n",
      "Epoch 90/100 - train_loss: 0.9094 - train_accuracy: 0.6199 - val_loss: 0.9586 - val_accuracy: 0.6105\n",
      "Epoch 91/100 - train_loss: 0.9078 - train_accuracy: 0.6186 - val_loss: 0.9588 - val_accuracy: 0.6105\n",
      "Epoch 92/100 - train_loss: 0.9061 - train_accuracy: 0.6199 - val_loss: 0.9591 - val_accuracy: 0.6105\n",
      "Epoch 93/100 - train_loss: 0.9045 - train_accuracy: 0.6199 - val_loss: 0.9593 - val_accuracy: 0.6105\n",
      "Epoch 94/100 - train_loss: 0.9029 - train_accuracy: 0.6199 - val_loss: 0.9596 - val_accuracy: 0.6105\n",
      "Epoch 95/100 - train_loss: 0.9012 - train_accuracy: 0.6280 - val_loss: 0.9598 - val_accuracy: 0.6163\n",
      "Epoch 96/100 - train_loss: 0.8995 - train_accuracy: 0.6280 - val_loss: 0.9600 - val_accuracy: 0.6163\n",
      "Epoch 97/100 - train_loss: 0.8978 - train_accuracy: 0.6334 - val_loss: 0.9601 - val_accuracy: 0.6163\n",
      "Epoch 98/100 - train_loss: 0.8961 - train_accuracy: 0.6348 - val_loss: 0.9602 - val_accuracy: 0.6221\n",
      "Epoch 99/100 - train_loss: 0.8943 - train_accuracy: 0.6388 - val_loss: 0.9603 - val_accuracy: 0.6163\n",
      "Epoch 100/100 - train_loss: 0.8926 - train_accuracy: 0.6388 - val_loss: 0.9604 - val_accuracy: 0.6163\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c0bd835f31412b9116f4e7a3dcf32f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.1_act=tanh_optim=mbgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/3cs4rv4y' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/3cs4rv4y</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_125023-3cs4rv4y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfd40200e6546b684a51c1845005fbe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_125035-1vw9hweh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/1vw9hweh' target=\"_blank\">lr=0.1_act=relu_optim=sgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/1vw9hweh' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/1vw9hweh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.1931 - train_accuracy: 0.4111 - val_loss: 1.1607 - val_accuracy: 0.4419\n",
      "Epoch 2/100 - train_loss: 1.1897 - train_accuracy: 0.4111 - val_loss: 1.1665 - val_accuracy: 0.4419\n",
      "Epoch 3/100 - train_loss: 1.0915 - train_accuracy: 0.5243 - val_loss: 1.0736 - val_accuracy: 0.5581\n",
      "Epoch 4/100 - train_loss: 1.0253 - train_accuracy: 0.5741 - val_loss: 1.0365 - val_accuracy: 0.6163\n",
      "Epoch 5/100 - train_loss: 1.0565 - train_accuracy: 0.5337 - val_loss: 1.0564 - val_accuracy: 0.5698\n",
      "Epoch 6/100 - train_loss: 1.0251 - train_accuracy: 0.5903 - val_loss: 1.0339 - val_accuracy: 0.5988\n",
      "Epoch 7/100 - train_loss: 1.0564 - train_accuracy: 0.5566 - val_loss: 1.0618 - val_accuracy: 0.5988\n",
      "Epoch 8/100 - train_loss: 1.0068 - train_accuracy: 0.5997 - val_loss: 1.0434 - val_accuracy: 0.5930\n",
      "Epoch 9/100 - train_loss: 1.0361 - train_accuracy: 0.5889 - val_loss: 1.0570 - val_accuracy: 0.5930\n",
      "Epoch 10/100 - train_loss: 1.0377 - train_accuracy: 0.5957 - val_loss: 1.0576 - val_accuracy: 0.5988\n",
      "Epoch 11/100 - train_loss: 1.0273 - train_accuracy: 0.5903 - val_loss: 1.0671 - val_accuracy: 0.5814\n",
      "Epoch 12/100 - train_loss: 1.0299 - train_accuracy: 0.5997 - val_loss: 1.0524 - val_accuracy: 0.5988\n",
      "Epoch 13/100 - train_loss: 1.0106 - train_accuracy: 0.6051 - val_loss: 1.0486 - val_accuracy: 0.5930\n",
      "Epoch 14/100 - train_loss: 1.0160 - train_accuracy: 0.5957 - val_loss: 1.0664 - val_accuracy: 0.5756\n",
      "Epoch 15/100 - train_loss: 1.0423 - train_accuracy: 0.5714 - val_loss: 1.0931 - val_accuracy: 0.5640\n",
      "Epoch 16/100 - train_loss: 0.9677 - train_accuracy: 0.6011 - val_loss: 1.0349 - val_accuracy: 0.5930\n",
      "Epoch 17/100 - train_loss: 1.0012 - train_accuracy: 0.6078 - val_loss: 1.0515 - val_accuracy: 0.5930\n",
      "Epoch 18/100 - train_loss: 0.9692 - train_accuracy: 0.6146 - val_loss: 1.0442 - val_accuracy: 0.5988\n",
      "Epoch 19/100 - train_loss: 0.9563 - train_accuracy: 0.6307 - val_loss: 1.0193 - val_accuracy: 0.6163\n",
      "Epoch 20/100 - train_loss: 0.9783 - train_accuracy: 0.6226 - val_loss: 1.0146 - val_accuracy: 0.6395\n",
      "Epoch 21/100 - train_loss: 0.9650 - train_accuracy: 0.6361 - val_loss: 1.0610 - val_accuracy: 0.5988\n",
      "Epoch 22/100 - train_loss: 0.9338 - train_accuracy: 0.6496 - val_loss: 1.0622 - val_accuracy: 0.6163\n",
      "Epoch 23/100 - train_loss: 0.9484 - train_accuracy: 0.6361 - val_loss: 1.0364 - val_accuracy: 0.5988\n",
      "Epoch 24/100 - train_loss: 0.9113 - train_accuracy: 0.6429 - val_loss: 1.0340 - val_accuracy: 0.6221\n",
      "Epoch 25/100 - train_loss: 0.9107 - train_accuracy: 0.6429 - val_loss: 1.0150 - val_accuracy: 0.6163\n",
      "Epoch 26/100 - train_loss: 0.9457 - train_accuracy: 0.6280 - val_loss: 1.0427 - val_accuracy: 0.6105\n",
      "Epoch 27/100 - train_loss: 1.1018 - train_accuracy: 0.6065 - val_loss: 1.1544 - val_accuracy: 0.5756\n",
      "Epoch 28/100 - train_loss: 0.9952 - train_accuracy: 0.6186 - val_loss: 1.0751 - val_accuracy: 0.5872\n",
      "Epoch 29/100 - train_loss: 0.9607 - train_accuracy: 0.6402 - val_loss: 1.0641 - val_accuracy: 0.5930\n",
      "Epoch 30/100 - train_loss: 0.9543 - train_accuracy: 0.6388 - val_loss: 1.0805 - val_accuracy: 0.5930\n",
      "Epoch 31/100 - train_loss: 0.9706 - train_accuracy: 0.6361 - val_loss: 1.0475 - val_accuracy: 0.5988\n",
      "Epoch 32/100 - train_loss: 0.9515 - train_accuracy: 0.6361 - val_loss: 1.0198 - val_accuracy: 0.5988\n",
      "Epoch 33/100 - train_loss: 0.9584 - train_accuracy: 0.6321 - val_loss: 1.0453 - val_accuracy: 0.6105\n",
      "Epoch 34/100 - train_loss: 0.9601 - train_accuracy: 0.6213 - val_loss: 1.0178 - val_accuracy: 0.5988\n",
      "Epoch 35/100 - train_loss: 0.9554 - train_accuracy: 0.6280 - val_loss: 1.0027 - val_accuracy: 0.6163\n",
      "Epoch 36/100 - train_loss: 0.9422 - train_accuracy: 0.6388 - val_loss: 1.0257 - val_accuracy: 0.6105\n",
      "Epoch 37/100 - train_loss: 0.9508 - train_accuracy: 0.6375 - val_loss: 1.0137 - val_accuracy: 0.6105\n",
      "Epoch 38/100 - train_loss: 0.9612 - train_accuracy: 0.6280 - val_loss: 1.0390 - val_accuracy: 0.5930\n",
      "Epoch 39/100 - train_loss: 0.9470 - train_accuracy: 0.6456 - val_loss: 1.0241 - val_accuracy: 0.6047\n",
      "Epoch 40/100 - train_loss: 0.9912 - train_accuracy: 0.5930 - val_loss: 1.0320 - val_accuracy: 0.5930\n",
      "Epoch 41/100 - train_loss: 0.9376 - train_accuracy: 0.6375 - val_loss: 1.0216 - val_accuracy: 0.6047\n",
      "Epoch 42/100 - train_loss: 0.9369 - train_accuracy: 0.6388 - val_loss: 1.0177 - val_accuracy: 0.6047\n",
      "Epoch 43/100 - train_loss: 0.9319 - train_accuracy: 0.6415 - val_loss: 1.0407 - val_accuracy: 0.5988\n",
      "Epoch 44/100 - train_loss: 0.9577 - train_accuracy: 0.6294 - val_loss: 1.0288 - val_accuracy: 0.6105\n",
      "Epoch 45/100 - train_loss: 0.9403 - train_accuracy: 0.6375 - val_loss: 1.0484 - val_accuracy: 0.6105\n",
      "Epoch 46/100 - train_loss: 0.9332 - train_accuracy: 0.6550 - val_loss: 1.0493 - val_accuracy: 0.5988\n",
      "Epoch 47/100 - train_loss: 0.9851 - train_accuracy: 0.6388 - val_loss: 1.0482 - val_accuracy: 0.5988\n",
      "Epoch 48/100 - train_loss: 0.9549 - train_accuracy: 0.6442 - val_loss: 1.0476 - val_accuracy: 0.6105\n",
      "Epoch 49/100 - train_loss: 0.9513 - train_accuracy: 0.6456 - val_loss: 1.0476 - val_accuracy: 0.5930\n",
      "Epoch 50/100 - train_loss: 0.9537 - train_accuracy: 0.6402 - val_loss: 1.0522 - val_accuracy: 0.5988\n",
      "Epoch 51/100 - train_loss: 0.9680 - train_accuracy: 0.6402 - val_loss: 1.0544 - val_accuracy: 0.6047\n",
      "Epoch 52/100 - train_loss: 0.9650 - train_accuracy: 0.6294 - val_loss: 1.0686 - val_accuracy: 0.6047\n",
      "Epoch 53/100 - train_loss: 0.9703 - train_accuracy: 0.6429 - val_loss: 1.0554 - val_accuracy: 0.5930\n",
      "Epoch 54/100 - train_loss: 0.9809 - train_accuracy: 0.6240 - val_loss: 1.0963 - val_accuracy: 0.5698\n",
      "Epoch 55/100 - train_loss: 0.9750 - train_accuracy: 0.6375 - val_loss: 1.0662 - val_accuracy: 0.5872\n",
      "Epoch 56/100 - train_loss: 0.9652 - train_accuracy: 0.6482 - val_loss: 1.0621 - val_accuracy: 0.5988\n",
      "Epoch 57/100 - train_loss: 0.9972 - train_accuracy: 0.6442 - val_loss: 1.0698 - val_accuracy: 0.5930\n",
      "Epoch 58/100 - train_loss: 0.9567 - train_accuracy: 0.6307 - val_loss: 1.0716 - val_accuracy: 0.5988\n",
      "Epoch 59/100 - train_loss: 0.9552 - train_accuracy: 0.6240 - val_loss: 1.0701 - val_accuracy: 0.5872\n",
      "Epoch 60/100 - train_loss: 0.9408 - train_accuracy: 0.6375 - val_loss: 1.0488 - val_accuracy: 0.5988\n",
      "Epoch 61/100 - train_loss: 0.9352 - train_accuracy: 0.6429 - val_loss: 1.0692 - val_accuracy: 0.5872\n",
      "Epoch 62/100 - train_loss: 0.9349 - train_accuracy: 0.6402 - val_loss: 1.0445 - val_accuracy: 0.6047\n",
      "Epoch 63/100 - train_loss: 0.9314 - train_accuracy: 0.6536 - val_loss: 1.0783 - val_accuracy: 0.5872\n",
      "Epoch 64/100 - train_loss: 0.9383 - train_accuracy: 0.6523 - val_loss: 1.0849 - val_accuracy: 0.5872\n",
      "Epoch 65/100 - train_loss: 0.9352 - train_accuracy: 0.6509 - val_loss: 1.0822 - val_accuracy: 0.5988\n",
      "Epoch 66/100 - train_loss: 0.9416 - train_accuracy: 0.6415 - val_loss: 1.0649 - val_accuracy: 0.6047\n",
      "Epoch 67/100 - train_loss: 0.9445 - train_accuracy: 0.6388 - val_loss: 1.0603 - val_accuracy: 0.5988\n",
      "Epoch 68/100 - train_loss: 0.9406 - train_accuracy: 0.6375 - val_loss: 1.0644 - val_accuracy: 0.6047\n",
      "Epoch 69/100 - train_loss: 0.9407 - train_accuracy: 0.6550 - val_loss: 1.0770 - val_accuracy: 0.5814\n",
      "Epoch 70/100 - train_loss: 0.9331 - train_accuracy: 0.6509 - val_loss: 1.0713 - val_accuracy: 0.5872\n",
      "Epoch 71/100 - train_loss: 0.9377 - train_accuracy: 0.6550 - val_loss: 1.0622 - val_accuracy: 0.5930\n",
      "Epoch 72/100 - train_loss: 0.9463 - train_accuracy: 0.6523 - val_loss: 1.0622 - val_accuracy: 0.5814\n",
      "Epoch 73/100 - train_loss: 0.9414 - train_accuracy: 0.6563 - val_loss: 1.0716 - val_accuracy: 0.5872\n",
      "Epoch 74/100 - train_loss: 0.9412 - train_accuracy: 0.6536 - val_loss: 1.0625 - val_accuracy: 0.5872\n",
      "Epoch 75/100 - train_loss: 0.9268 - train_accuracy: 0.6577 - val_loss: 1.0854 - val_accuracy: 0.5872\n",
      "Epoch 76/100 - train_loss: 0.9517 - train_accuracy: 0.6617 - val_loss: 1.0755 - val_accuracy: 0.5872\n",
      "Epoch 77/100 - train_loss: 0.9302 - train_accuracy: 0.6496 - val_loss: 1.0974 - val_accuracy: 0.5872\n",
      "Epoch 78/100 - train_loss: 0.9442 - train_accuracy: 0.6550 - val_loss: 1.0787 - val_accuracy: 0.5814\n",
      "Epoch 79/100 - train_loss: 0.9437 - train_accuracy: 0.6577 - val_loss: 1.0979 - val_accuracy: 0.5814\n",
      "Epoch 80/100 - train_loss: 0.9354 - train_accuracy: 0.6604 - val_loss: 1.0489 - val_accuracy: 0.5930\n",
      "Epoch 81/100 - train_loss: 0.9773 - train_accuracy: 0.6334 - val_loss: 1.1298 - val_accuracy: 0.5640\n",
      "Epoch 82/100 - train_loss: 0.9538 - train_accuracy: 0.6509 - val_loss: 1.0699 - val_accuracy: 0.5698\n",
      "Epoch 83/100 - train_loss: 0.9732 - train_accuracy: 0.6375 - val_loss: 1.0863 - val_accuracy: 0.5756\n",
      "Epoch 84/100 - train_loss: 0.9593 - train_accuracy: 0.6496 - val_loss: 1.0372 - val_accuracy: 0.6105\n",
      "Epoch 85/100 - train_loss: 0.9763 - train_accuracy: 0.6375 - val_loss: 1.0829 - val_accuracy: 0.5872\n",
      "Epoch 86/100 - train_loss: 0.9417 - train_accuracy: 0.6523 - val_loss: 1.0954 - val_accuracy: 0.5581\n",
      "Epoch 87/100 - train_loss: 0.9232 - train_accuracy: 0.6577 - val_loss: 1.0788 - val_accuracy: 0.5872\n",
      "Epoch 88/100 - train_loss: 0.9274 - train_accuracy: 0.6590 - val_loss: 1.0872 - val_accuracy: 0.5814\n",
      "Epoch 89/100 - train_loss: 0.9315 - train_accuracy: 0.6550 - val_loss: 1.0870 - val_accuracy: 0.5640\n",
      "Epoch 90/100 - train_loss: 0.9330 - train_accuracy: 0.6631 - val_loss: 1.0670 - val_accuracy: 0.5872\n",
      "Epoch 91/100 - train_loss: 0.9644 - train_accuracy: 0.6456 - val_loss: 1.0542 - val_accuracy: 0.5988\n",
      "Epoch 92/100 - train_loss: 0.9488 - train_accuracy: 0.6523 - val_loss: 1.0565 - val_accuracy: 0.5988\n",
      "Epoch 93/100 - train_loss: 0.9473 - train_accuracy: 0.6563 - val_loss: 1.0661 - val_accuracy: 0.5988\n",
      "Epoch 94/100 - train_loss: 0.9394 - train_accuracy: 0.6631 - val_loss: 1.0510 - val_accuracy: 0.6047\n",
      "Epoch 95/100 - train_loss: 0.9617 - train_accuracy: 0.6429 - val_loss: 1.0743 - val_accuracy: 0.5930\n",
      "Epoch 96/100 - train_loss: 0.9428 - train_accuracy: 0.6631 - val_loss: 1.0550 - val_accuracy: 0.6105\n",
      "Epoch 97/100 - train_loss: 0.9397 - train_accuracy: 0.6631 - val_loss: 1.0817 - val_accuracy: 0.5988\n",
      "Epoch 98/100 - train_loss: 0.9387 - train_accuracy: 0.6604 - val_loss: 1.0896 - val_accuracy: 0.5930\n",
      "Epoch 99/100 - train_loss: 0.9422 - train_accuracy: 0.6644 - val_loss: 1.0540 - val_accuracy: 0.6163\n",
      "Epoch 100/100 - train_loss: 0.9400 - train_accuracy: 0.6617 - val_loss: 1.0461 - val_accuracy: 0.6163\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b062de5408460e9ebf209fbd64dfd3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.1_act=relu_optim=sgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/1vw9hweh' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/1vw9hweh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_125035-1vw9hweh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7852e97b3943259fc5cb7958dd8bc7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_125050-cipb0e9e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/cipb0e9e' target=\"_blank\">lr=0.1_act=relu_optim=bgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/cipb0e9e' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/cipb0e9e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.7729 - train_accuracy: 0.4137 - val_loss: 1.7721 - val_accuracy: 0.4070\n",
      "Epoch 2/100 - train_loss: 1.7546 - train_accuracy: 0.4137 - val_loss: 1.7532 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.7369 - train_accuracy: 0.4137 - val_loss: 1.7348 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.7199 - train_accuracy: 0.4137 - val_loss: 1.7171 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.7034 - train_accuracy: 0.4137 - val_loss: 1.6999 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.6875 - train_accuracy: 0.4137 - val_loss: 1.6833 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.6721 - train_accuracy: 0.4137 - val_loss: 1.6673 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.6573 - train_accuracy: 0.4137 - val_loss: 1.6519 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.6430 - train_accuracy: 0.4137 - val_loss: 1.6369 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.6292 - train_accuracy: 0.4137 - val_loss: 1.6225 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.6159 - train_accuracy: 0.4137 - val_loss: 1.6086 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.6030 - train_accuracy: 0.4137 - val_loss: 1.5952 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.5906 - train_accuracy: 0.4137 - val_loss: 1.5822 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.5787 - train_accuracy: 0.4137 - val_loss: 1.5697 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.5672 - train_accuracy: 0.4137 - val_loss: 1.5576 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.5561 - train_accuracy: 0.4137 - val_loss: 1.5459 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.5453 - train_accuracy: 0.4137 - val_loss: 1.5347 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.5350 - train_accuracy: 0.4137 - val_loss: 1.5238 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.5250 - train_accuracy: 0.4137 - val_loss: 1.5134 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.5154 - train_accuracy: 0.4137 - val_loss: 1.5032 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.5062 - train_accuracy: 0.4137 - val_loss: 1.4935 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.4972 - train_accuracy: 0.4137 - val_loss: 1.4841 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.4886 - train_accuracy: 0.4137 - val_loss: 1.4750 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.4803 - train_accuracy: 0.4137 - val_loss: 1.4662 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.4723 - train_accuracy: 0.4137 - val_loss: 1.4577 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.4646 - train_accuracy: 0.4137 - val_loss: 1.4496 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.4571 - train_accuracy: 0.4137 - val_loss: 1.4417 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.4499 - train_accuracy: 0.4137 - val_loss: 1.4340 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.4430 - train_accuracy: 0.4137 - val_loss: 1.4267 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.4363 - train_accuracy: 0.4137 - val_loss: 1.4196 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.4298 - train_accuracy: 0.4137 - val_loss: 1.4127 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.4235 - train_accuracy: 0.4137 - val_loss: 1.4061 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.4175 - train_accuracy: 0.4137 - val_loss: 1.3997 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.4117 - train_accuracy: 0.4137 - val_loss: 1.3935 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.4061 - train_accuracy: 0.4137 - val_loss: 1.3875 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.4006 - train_accuracy: 0.4137 - val_loss: 1.3817 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.3954 - train_accuracy: 0.4137 - val_loss: 1.3761 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.3903 - train_accuracy: 0.4137 - val_loss: 1.3707 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.3854 - train_accuracy: 0.4137 - val_loss: 1.3654 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.3806 - train_accuracy: 0.4137 - val_loss: 1.3603 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.3761 - train_accuracy: 0.4137 - val_loss: 1.3554 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.3716 - train_accuracy: 0.4137 - val_loss: 1.3507 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.3673 - train_accuracy: 0.4137 - val_loss: 1.3461 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.3632 - train_accuracy: 0.4137 - val_loss: 1.3416 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.3592 - train_accuracy: 0.4137 - val_loss: 1.3373 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.3553 - train_accuracy: 0.4137 - val_loss: 1.3332 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.3515 - train_accuracy: 0.4137 - val_loss: 1.3291 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.3479 - train_accuracy: 0.4137 - val_loss: 1.3252 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.3443 - train_accuracy: 0.4137 - val_loss: 1.3214 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.3409 - train_accuracy: 0.4137 - val_loss: 1.3177 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.3376 - train_accuracy: 0.4137 - val_loss: 1.3141 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.3344 - train_accuracy: 0.4137 - val_loss: 1.3107 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.3313 - train_accuracy: 0.4137 - val_loss: 1.3073 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.3283 - train_accuracy: 0.4137 - val_loss: 1.3041 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.3253 - train_accuracy: 0.4137 - val_loss: 1.3009 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.3225 - train_accuracy: 0.4137 - val_loss: 1.2978 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.3198 - train_accuracy: 0.4137 - val_loss: 1.2948 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.3171 - train_accuracy: 0.4137 - val_loss: 1.2920 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.3145 - train_accuracy: 0.4137 - val_loss: 1.2891 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.3120 - train_accuracy: 0.4137 - val_loss: 1.2864 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.3095 - train_accuracy: 0.4137 - val_loss: 1.2838 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.3071 - train_accuracy: 0.4137 - val_loss: 1.2812 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.3048 - train_accuracy: 0.4137 - val_loss: 1.2787 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.3026 - train_accuracy: 0.4137 - val_loss: 1.2762 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.3004 - train_accuracy: 0.4137 - val_loss: 1.2739 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.2983 - train_accuracy: 0.4137 - val_loss: 1.2716 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.2962 - train_accuracy: 0.4137 - val_loss: 1.2693 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.2942 - train_accuracy: 0.4137 - val_loss: 1.2671 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.2923 - train_accuracy: 0.4137 - val_loss: 1.2650 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.2904 - train_accuracy: 0.4137 - val_loss: 1.2629 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.2885 - train_accuracy: 0.4137 - val_loss: 1.2609 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.2867 - train_accuracy: 0.4137 - val_loss: 1.2590 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.2850 - train_accuracy: 0.4137 - val_loss: 1.2570 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.2833 - train_accuracy: 0.4137 - val_loss: 1.2552 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.2816 - train_accuracy: 0.4137 - val_loss: 1.2534 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.2800 - train_accuracy: 0.4137 - val_loss: 1.2516 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.2784 - train_accuracy: 0.4137 - val_loss: 1.2499 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.2769 - train_accuracy: 0.4137 - val_loss: 1.2482 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.2754 - train_accuracy: 0.4137 - val_loss: 1.2466 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.2739 - train_accuracy: 0.4137 - val_loss: 1.2450 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.2725 - train_accuracy: 0.4137 - val_loss: 1.2434 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.2711 - train_accuracy: 0.4137 - val_loss: 1.2419 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.2697 - train_accuracy: 0.4137 - val_loss: 1.2404 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.2684 - train_accuracy: 0.4137 - val_loss: 1.2389 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.2671 - train_accuracy: 0.4137 - val_loss: 1.2375 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.2659 - train_accuracy: 0.4137 - val_loss: 1.2361 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.2646 - train_accuracy: 0.4137 - val_loss: 1.2348 - val_accuracy: 0.4070\n",
      "Epoch 88/100 - train_loss: 1.2634 - train_accuracy: 0.4137 - val_loss: 1.2335 - val_accuracy: 0.4070\n",
      "Epoch 89/100 - train_loss: 1.2622 - train_accuracy: 0.4137 - val_loss: 1.2322 - val_accuracy: 0.4070\n",
      "Epoch 90/100 - train_loss: 1.2611 - train_accuracy: 0.4137 - val_loss: 1.2309 - val_accuracy: 0.4070\n",
      "Epoch 91/100 - train_loss: 1.2600 - train_accuracy: 0.4137 - val_loss: 1.2297 - val_accuracy: 0.4070\n",
      "Epoch 92/100 - train_loss: 1.2589 - train_accuracy: 0.4137 - val_loss: 1.2285 - val_accuracy: 0.4070\n",
      "Epoch 93/100 - train_loss: 1.2578 - train_accuracy: 0.4137 - val_loss: 1.2273 - val_accuracy: 0.4070\n",
      "Epoch 94/100 - train_loss: 1.2567 - train_accuracy: 0.4137 - val_loss: 1.2262 - val_accuracy: 0.4070\n",
      "Epoch 95/100 - train_loss: 1.2557 - train_accuracy: 0.4137 - val_loss: 1.2250 - val_accuracy: 0.4070\n",
      "Epoch 96/100 - train_loss: 1.2547 - train_accuracy: 0.4137 - val_loss: 1.2239 - val_accuracy: 0.4070\n",
      "Epoch 97/100 - train_loss: 1.2537 - train_accuracy: 0.4137 - val_loss: 1.2229 - val_accuracy: 0.4070\n",
      "Epoch 98/100 - train_loss: 1.2528 - train_accuracy: 0.4137 - val_loss: 1.2218 - val_accuracy: 0.4070\n",
      "Epoch 99/100 - train_loss: 1.2518 - train_accuracy: 0.4137 - val_loss: 1.2208 - val_accuracy: 0.4070\n",
      "Epoch 100/100 - train_loss: 1.2509 - train_accuracy: 0.4137 - val_loss: 1.2198 - val_accuracy: 0.4070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.1_act=relu_optim=bgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/cipb0e9e' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/cipb0e9e</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_125050-cipb0e9e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly. Reconnecting the current kernel may help.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab5c3c4888341699b7d09428a5e230c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rohan/SMAI/A3/wandb/run-20231026_125104-4isvdo9w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/4isvdo9w' target=\"_blank\">lr=0.1_act=relu_optim=mbgd</a></strong> to <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/rohan-victorious108/uncategorized' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/4isvdo9w' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/4isvdo9w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - train_loss: 1.4774 - train_accuracy: 0.4137 - val_loss: 1.4628 - val_accuracy: 0.4070\n",
      "Epoch 2/100 - train_loss: 1.3451 - train_accuracy: 0.4137 - val_loss: 1.3217 - val_accuracy: 0.4070\n",
      "Epoch 3/100 - train_loss: 1.2848 - train_accuracy: 0.4137 - val_loss: 1.2561 - val_accuracy: 0.4070\n",
      "Epoch 4/100 - train_loss: 1.2536 - train_accuracy: 0.4137 - val_loss: 1.2217 - val_accuracy: 0.4070\n",
      "Epoch 5/100 - train_loss: 1.2353 - train_accuracy: 0.4137 - val_loss: 1.2017 - val_accuracy: 0.4070\n",
      "Epoch 6/100 - train_loss: 1.2236 - train_accuracy: 0.4137 - val_loss: 1.1890 - val_accuracy: 0.4070\n",
      "Epoch 7/100 - train_loss: 1.2155 - train_accuracy: 0.4137 - val_loss: 1.1803 - val_accuracy: 0.4070\n",
      "Epoch 8/100 - train_loss: 1.2095 - train_accuracy: 0.4137 - val_loss: 1.1742 - val_accuracy: 0.4070\n",
      "Epoch 9/100 - train_loss: 1.2050 - train_accuracy: 0.4137 - val_loss: 1.1698 - val_accuracy: 0.4070\n",
      "Epoch 10/100 - train_loss: 1.2014 - train_accuracy: 0.4137 - val_loss: 1.1664 - val_accuracy: 0.4070\n",
      "Epoch 11/100 - train_loss: 1.1985 - train_accuracy: 0.4137 - val_loss: 1.1638 - val_accuracy: 0.4070\n",
      "Epoch 12/100 - train_loss: 1.1962 - train_accuracy: 0.4137 - val_loss: 1.1619 - val_accuracy: 0.4070\n",
      "Epoch 13/100 - train_loss: 1.1942 - train_accuracy: 0.4137 - val_loss: 1.1603 - val_accuracy: 0.4070\n",
      "Epoch 14/100 - train_loss: 1.1926 - train_accuracy: 0.4137 - val_loss: 1.1592 - val_accuracy: 0.4070\n",
      "Epoch 15/100 - train_loss: 1.1912 - train_accuracy: 0.4137 - val_loss: 1.1582 - val_accuracy: 0.4070\n",
      "Epoch 16/100 - train_loss: 1.1900 - train_accuracy: 0.4137 - val_loss: 1.1575 - val_accuracy: 0.4070\n",
      "Epoch 17/100 - train_loss: 1.1890 - train_accuracy: 0.4137 - val_loss: 1.1569 - val_accuracy: 0.4070\n",
      "Epoch 18/100 - train_loss: 1.1881 - train_accuracy: 0.4137 - val_loss: 1.1565 - val_accuracy: 0.4070\n",
      "Epoch 19/100 - train_loss: 1.1873 - train_accuracy: 0.4137 - val_loss: 1.1562 - val_accuracy: 0.4070\n",
      "Epoch 20/100 - train_loss: 1.1867 - train_accuracy: 0.4137 - val_loss: 1.1560 - val_accuracy: 0.4070\n",
      "Epoch 21/100 - train_loss: 1.1860 - train_accuracy: 0.4137 - val_loss: 1.1558 - val_accuracy: 0.4070\n",
      "Epoch 22/100 - train_loss: 1.1855 - train_accuracy: 0.4137 - val_loss: 1.1558 - val_accuracy: 0.4070\n",
      "Epoch 23/100 - train_loss: 1.1850 - train_accuracy: 0.4137 - val_loss: 1.1557 - val_accuracy: 0.4070\n",
      "Epoch 24/100 - train_loss: 1.1846 - train_accuracy: 0.4137 - val_loss: 1.1557 - val_accuracy: 0.4070\n",
      "Epoch 25/100 - train_loss: 1.1842 - train_accuracy: 0.4137 - val_loss: 1.1558 - val_accuracy: 0.4070\n",
      "Epoch 26/100 - train_loss: 1.1838 - train_accuracy: 0.4137 - val_loss: 1.1558 - val_accuracy: 0.4070\n",
      "Epoch 27/100 - train_loss: 1.1835 - train_accuracy: 0.4137 - val_loss: 1.1559 - val_accuracy: 0.4070\n",
      "Epoch 28/100 - train_loss: 1.1832 - train_accuracy: 0.4137 - val_loss: 1.1560 - val_accuracy: 0.4070\n",
      "Epoch 29/100 - train_loss: 1.1829 - train_accuracy: 0.4137 - val_loss: 1.1562 - val_accuracy: 0.4070\n",
      "Epoch 30/100 - train_loss: 1.1826 - train_accuracy: 0.4137 - val_loss: 1.1563 - val_accuracy: 0.4070\n",
      "Epoch 31/100 - train_loss: 1.1824 - train_accuracy: 0.4137 - val_loss: 1.1565 - val_accuracy: 0.4070\n",
      "Epoch 32/100 - train_loss: 1.1821 - train_accuracy: 0.4137 - val_loss: 1.1567 - val_accuracy: 0.4070\n",
      "Epoch 33/100 - train_loss: 1.1819 - train_accuracy: 0.4137 - val_loss: 1.1568 - val_accuracy: 0.4070\n",
      "Epoch 34/100 - train_loss: 1.1817 - train_accuracy: 0.4137 - val_loss: 1.1570 - val_accuracy: 0.4070\n",
      "Epoch 35/100 - train_loss: 1.1815 - train_accuracy: 0.4137 - val_loss: 1.1572 - val_accuracy: 0.4070\n",
      "Epoch 36/100 - train_loss: 1.1814 - train_accuracy: 0.4137 - val_loss: 1.1574 - val_accuracy: 0.4070\n",
      "Epoch 37/100 - train_loss: 1.1812 - train_accuracy: 0.4137 - val_loss: 1.1576 - val_accuracy: 0.4070\n",
      "Epoch 38/100 - train_loss: 1.1810 - train_accuracy: 0.4137 - val_loss: 1.1578 - val_accuracy: 0.4070\n",
      "Epoch 39/100 - train_loss: 1.1809 - train_accuracy: 0.4137 - val_loss: 1.1580 - val_accuracy: 0.4070\n",
      "Epoch 40/100 - train_loss: 1.1808 - train_accuracy: 0.4137 - val_loss: 1.1582 - val_accuracy: 0.4070\n",
      "Epoch 41/100 - train_loss: 1.1806 - train_accuracy: 0.4137 - val_loss: 1.1584 - val_accuracy: 0.4070\n",
      "Epoch 42/100 - train_loss: 1.1805 - train_accuracy: 0.4137 - val_loss: 1.1586 - val_accuracy: 0.4070\n",
      "Epoch 43/100 - train_loss: 1.1804 - train_accuracy: 0.4137 - val_loss: 1.1588 - val_accuracy: 0.4070\n",
      "Epoch 44/100 - train_loss: 1.1803 - train_accuracy: 0.4137 - val_loss: 1.1590 - val_accuracy: 0.4070\n",
      "Epoch 45/100 - train_loss: 1.1802 - train_accuracy: 0.4137 - val_loss: 1.1593 - val_accuracy: 0.4070\n",
      "Epoch 46/100 - train_loss: 1.1800 - train_accuracy: 0.4137 - val_loss: 1.1595 - val_accuracy: 0.4070\n",
      "Epoch 47/100 - train_loss: 1.1799 - train_accuracy: 0.4137 - val_loss: 1.1597 - val_accuracy: 0.4070\n",
      "Epoch 48/100 - train_loss: 1.1799 - train_accuracy: 0.4137 - val_loss: 1.1599 - val_accuracy: 0.4070\n",
      "Epoch 49/100 - train_loss: 1.1798 - train_accuracy: 0.4137 - val_loss: 1.1601 - val_accuracy: 0.4070\n",
      "Epoch 50/100 - train_loss: 1.1797 - train_accuracy: 0.4137 - val_loss: 1.1603 - val_accuracy: 0.4070\n",
      "Epoch 51/100 - train_loss: 1.1796 - train_accuracy: 0.4137 - val_loss: 1.1605 - val_accuracy: 0.4070\n",
      "Epoch 52/100 - train_loss: 1.1795 - train_accuracy: 0.4137 - val_loss: 1.1607 - val_accuracy: 0.4070\n",
      "Epoch 53/100 - train_loss: 1.1794 - train_accuracy: 0.4137 - val_loss: 1.1609 - val_accuracy: 0.4070\n",
      "Epoch 54/100 - train_loss: 1.1794 - train_accuracy: 0.4137 - val_loss: 1.1610 - val_accuracy: 0.4070\n",
      "Epoch 55/100 - train_loss: 1.1793 - train_accuracy: 0.4137 - val_loss: 1.1612 - val_accuracy: 0.4070\n",
      "Epoch 56/100 - train_loss: 1.1792 - train_accuracy: 0.4137 - val_loss: 1.1614 - val_accuracy: 0.4070\n",
      "Epoch 57/100 - train_loss: 1.1792 - train_accuracy: 0.4137 - val_loss: 1.1616 - val_accuracy: 0.4070\n",
      "Epoch 58/100 - train_loss: 1.1791 - train_accuracy: 0.4137 - val_loss: 1.1618 - val_accuracy: 0.4070\n",
      "Epoch 59/100 - train_loss: 1.1790 - train_accuracy: 0.4137 - val_loss: 1.1620 - val_accuracy: 0.4070\n",
      "Epoch 60/100 - train_loss: 1.1790 - train_accuracy: 0.4137 - val_loss: 1.1622 - val_accuracy: 0.4070\n",
      "Epoch 61/100 - train_loss: 1.1789 - train_accuracy: 0.4137 - val_loss: 1.1623 - val_accuracy: 0.4070\n",
      "Epoch 62/100 - train_loss: 1.1789 - train_accuracy: 0.4137 - val_loss: 1.1625 - val_accuracy: 0.4070\n",
      "Epoch 63/100 - train_loss: 1.1788 - train_accuracy: 0.4137 - val_loss: 1.1627 - val_accuracy: 0.4070\n",
      "Epoch 64/100 - train_loss: 1.1788 - train_accuracy: 0.4137 - val_loss: 1.1628 - val_accuracy: 0.4070\n",
      "Epoch 65/100 - train_loss: 1.1787 - train_accuracy: 0.4137 - val_loss: 1.1630 - val_accuracy: 0.4070\n",
      "Epoch 66/100 - train_loss: 1.1787 - train_accuracy: 0.4137 - val_loss: 1.1632 - val_accuracy: 0.4070\n",
      "Epoch 67/100 - train_loss: 1.1786 - train_accuracy: 0.4137 - val_loss: 1.1633 - val_accuracy: 0.4070\n",
      "Epoch 68/100 - train_loss: 1.1786 - train_accuracy: 0.4137 - val_loss: 1.1635 - val_accuracy: 0.4070\n",
      "Epoch 69/100 - train_loss: 1.1785 - train_accuracy: 0.4137 - val_loss: 1.1637 - val_accuracy: 0.4070\n",
      "Epoch 70/100 - train_loss: 1.1785 - train_accuracy: 0.4137 - val_loss: 1.1638 - val_accuracy: 0.4070\n",
      "Epoch 71/100 - train_loss: 1.1784 - train_accuracy: 0.4137 - val_loss: 1.1640 - val_accuracy: 0.4070\n",
      "Epoch 72/100 - train_loss: 1.1784 - train_accuracy: 0.4137 - val_loss: 1.1641 - val_accuracy: 0.4070\n",
      "Epoch 73/100 - train_loss: 1.1784 - train_accuracy: 0.4137 - val_loss: 1.1643 - val_accuracy: 0.4070\n",
      "Epoch 74/100 - train_loss: 1.1783 - train_accuracy: 0.4137 - val_loss: 1.1644 - val_accuracy: 0.4070\n",
      "Epoch 75/100 - train_loss: 1.1783 - train_accuracy: 0.4137 - val_loss: 1.1646 - val_accuracy: 0.4070\n",
      "Epoch 76/100 - train_loss: 1.1783 - train_accuracy: 0.4137 - val_loss: 1.1647 - val_accuracy: 0.4070\n",
      "Epoch 77/100 - train_loss: 1.1782 - train_accuracy: 0.4137 - val_loss: 1.1649 - val_accuracy: 0.4070\n",
      "Epoch 78/100 - train_loss: 1.1782 - train_accuracy: 0.4137 - val_loss: 1.1650 - val_accuracy: 0.4070\n",
      "Epoch 79/100 - train_loss: 1.1782 - train_accuracy: 0.4137 - val_loss: 1.1652 - val_accuracy: 0.4070\n",
      "Epoch 80/100 - train_loss: 1.1781 - train_accuracy: 0.4137 - val_loss: 1.1653 - val_accuracy: 0.4070\n",
      "Epoch 81/100 - train_loss: 1.1781 - train_accuracy: 0.4137 - val_loss: 1.1654 - val_accuracy: 0.4070\n",
      "Epoch 82/100 - train_loss: 1.1781 - train_accuracy: 0.4137 - val_loss: 1.1656 - val_accuracy: 0.4070\n",
      "Epoch 83/100 - train_loss: 1.1780 - train_accuracy: 0.4137 - val_loss: 1.1657 - val_accuracy: 0.4070\n",
      "Epoch 84/100 - train_loss: 1.1780 - train_accuracy: 0.4137 - val_loss: 1.1659 - val_accuracy: 0.4070\n",
      "Epoch 85/100 - train_loss: 1.1780 - train_accuracy: 0.4137 - val_loss: 1.1660 - val_accuracy: 0.4070\n",
      "Epoch 86/100 - train_loss: 1.1779 - train_accuracy: 0.4164 - val_loss: 1.1661 - val_accuracy: 0.4070\n",
      "Epoch 87/100 - train_loss: 1.1779 - train_accuracy: 0.4164 - val_loss: 1.1662 - val_accuracy: 0.4128\n",
      "Epoch 88/100 - train_loss: 1.1779 - train_accuracy: 0.4313 - val_loss: 1.1664 - val_accuracy: 0.4186\n",
      "Epoch 89/100 - train_loss: 1.1779 - train_accuracy: 0.4623 - val_loss: 1.1665 - val_accuracy: 0.4535\n",
      "Epoch 90/100 - train_loss: 1.1778 - train_accuracy: 0.5040 - val_loss: 1.1666 - val_accuracy: 0.4826\n",
      "Epoch 91/100 - train_loss: 1.1778 - train_accuracy: 0.5458 - val_loss: 1.1667 - val_accuracy: 0.4942\n",
      "Epoch 92/100 - train_loss: 1.1778 - train_accuracy: 0.5539 - val_loss: 1.1668 - val_accuracy: 0.5058\n",
      "Epoch 93/100 - train_loss: 1.1777 - train_accuracy: 0.5431 - val_loss: 1.1670 - val_accuracy: 0.5291\n",
      "Epoch 94/100 - train_loss: 1.1777 - train_accuracy: 0.5243 - val_loss: 1.1671 - val_accuracy: 0.5465\n",
      "Epoch 95/100 - train_loss: 1.1777 - train_accuracy: 0.5081 - val_loss: 1.1672 - val_accuracy: 0.5465\n",
      "Epoch 96/100 - train_loss: 1.1776 - train_accuracy: 0.4838 - val_loss: 1.1673 - val_accuracy: 0.5058\n",
      "Epoch 97/100 - train_loss: 1.1776 - train_accuracy: 0.4825 - val_loss: 1.1674 - val_accuracy: 0.5000\n",
      "Epoch 98/100 - train_loss: 1.1776 - train_accuracy: 0.4717 - val_loss: 1.1675 - val_accuracy: 0.4884\n",
      "Epoch 99/100 - train_loss: 1.1775 - train_accuracy: 0.4623 - val_loss: 1.1676 - val_accuracy: 0.4767\n",
      "Epoch 100/100 - train_loss: 1.1775 - train_accuracy: 0.4555 - val_loss: 1.1676 - val_accuracy: 0.4826\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lr=0.1_act=relu_optim=mbgd</strong> at: <a href='https://wandb.ai/rohan-victorious108/uncategorized/runs/4isvdo9w' target=\"_blank\">https://wandb.ai/rohan-victorious108/uncategorized/runs/4isvdo9w</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_125104-4isvdo9w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"mlp-new-2\")\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "activation_functions = ['sigmoid', 'tanh', 'relu']\n",
    "optimzer = ['sgd','bgd','mbgd']\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for activation in activation_functions:\n",
    "        for optim in optimzer:\n",
    "        # Set hyperparameters for this run\n",
    "            config = {\n",
    "                \"learning_rate\": lr,\n",
    "                \"activation_function\": activation\n",
    "            }\n",
    "\n",
    "            # Create a new run for this hyperparameter set\n",
    "            run = wandb.init(name=f'lr={lr}_act={activation}_optim={optim}',config=config,reinit=True)\n",
    "            mlp = MLPClassifier(learning_rate=lr, activation=activation, optimizer=optim, hidden_layers=[8, 6],    epochs=100)\n",
    "            mlp.fit(train_X,y_true,val_X,y_true_val)\n",
    "        \n",
    "\n",
    "        \n",
    "            # End the run\n",
    "            run.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6381fb41",
   "metadata": {},
   "source": [
    "### Multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93315f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skmultilearn.problem_transform import LabelPowerset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076b210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./advertisement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668085f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_ds = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fe0526e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sports', 'clothing', 'electronics', 'food', 'books', 'furniture', 'beauty', 'home'}\n"
     ]
    }
   ],
   "source": [
    "unique_labels  = set()\n",
    "\n",
    "for arr in np_ds:\n",
    "    lbl_str = arr[-1].split(' ')\n",
    "    for lbl in lbl_str:\n",
    "        unique_labels.add(lbl)\n",
    "\n",
    "print(unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2230708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>income</th>\n",
       "      <th>education</th>\n",
       "      <th>married</th>\n",
       "      <th>children</th>\n",
       "      <th>city</th>\n",
       "      <th>occupation</th>\n",
       "      <th>purchase_amount</th>\n",
       "      <th>most bought item</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>Female</td>\n",
       "      <td>53229.101074</td>\n",
       "      <td>High School</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Crystalburgh</td>\n",
       "      <td>Businessman</td>\n",
       "      <td>115.135586</td>\n",
       "      <td>lipstick</td>\n",
       "      <td>furniture beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>Female</td>\n",
       "      <td>30066.046684</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>Margaretburgh</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>101.694559</td>\n",
       "      <td>biscuits</td>\n",
       "      <td>clothing electronics food sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>44792.627094</td>\n",
       "      <td>Master</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>New Paul</td>\n",
       "      <td>Businessman</td>\n",
       "      <td>86.847281</td>\n",
       "      <td>carpet</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>Female</td>\n",
       "      <td>51266.767047</td>\n",
       "      <td>Master</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>Frankport</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>95.145103</td>\n",
       "      <td>laptop</td>\n",
       "      <td>sports electronics books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>Female</td>\n",
       "      <td>35325.309005</td>\n",
       "      <td>PhD</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>South Raventown</td>\n",
       "      <td>Housewife</td>\n",
       "      <td>110.564517</td>\n",
       "      <td>dictionary</td>\n",
       "      <td>books beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>57</td>\n",
       "      <td>Female</td>\n",
       "      <td>60907.844629</td>\n",
       "      <td>PhD</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Gutierrezborough</td>\n",
       "      <td>HR</td>\n",
       "      <td>78.831675</td>\n",
       "      <td>shoes</td>\n",
       "      <td>clothing home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>27</td>\n",
       "      <td>Female</td>\n",
       "      <td>51033.627935</td>\n",
       "      <td>PhD</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>Lake Tiffanyhaven</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>138.672618</td>\n",
       "      <td>maggi</td>\n",
       "      <td>food books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>44</td>\n",
       "      <td>Female</td>\n",
       "      <td>34683.652618</td>\n",
       "      <td>Master</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>West Darlenemouth</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>89.101465</td>\n",
       "      <td>bat</td>\n",
       "      <td>sports furniture food home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>51</td>\n",
       "      <td>Female</td>\n",
       "      <td>48815.172160</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>New Anthonytown</td>\n",
       "      <td>Salesman</td>\n",
       "      <td>65.797622</td>\n",
       "      <td>biscuits</td>\n",
       "      <td>beauty clothing food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>40527.463149</td>\n",
       "      <td>High School</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Markfurt</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>101.597316</td>\n",
       "      <td>gloves</td>\n",
       "      <td>books clothing sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>Female</td>\n",
       "      <td>40668.478951</td>\n",
       "      <td>High School</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>South Tina</td>\n",
       "      <td>Businessman</td>\n",
       "      <td>124.487105</td>\n",
       "      <td>curtains</td>\n",
       "      <td>furniture electronics home sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>47</td>\n",
       "      <td>Female</td>\n",
       "      <td>50595.565055</td>\n",
       "      <td>Master</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>Stevenfort</td>\n",
       "      <td>Businessman</td>\n",
       "      <td>112.431977</td>\n",
       "      <td>chips</td>\n",
       "      <td>food furniture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>36</td>\n",
       "      <td>Female</td>\n",
       "      <td>34611.512757</td>\n",
       "      <td>High School</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>South Jeffreyberg</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>79.246469</td>\n",
       "      <td>chips</td>\n",
       "      <td>home food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>25</td>\n",
       "      <td>Female</td>\n",
       "      <td>46971.308307</td>\n",
       "      <td>Master</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>South Steven</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>109.716368</td>\n",
       "      <td>bat</td>\n",
       "      <td>beauty sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>50</td>\n",
       "      <td>Female</td>\n",
       "      <td>45270.638579</td>\n",
       "      <td>Master</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Tracieshire</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>92.149693</td>\n",
       "      <td>curtains</td>\n",
       "      <td>food home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>50</td>\n",
       "      <td>Female</td>\n",
       "      <td>40800.945097</td>\n",
       "      <td>High School</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>East Kristy</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>80.740713</td>\n",
       "      <td>encyclopedia</td>\n",
       "      <td>books clothing sports beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>31</td>\n",
       "      <td>Female</td>\n",
       "      <td>57019.169559</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Port Royport</td>\n",
       "      <td>Retired</td>\n",
       "      <td>119.808572</td>\n",
       "      <td>mobile</td>\n",
       "      <td>electronics furniture food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>57</td>\n",
       "      <td>Female</td>\n",
       "      <td>62617.100460</td>\n",
       "      <td>High School</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>Floresfurt</td>\n",
       "      <td>Artist</td>\n",
       "      <td>128.026275</td>\n",
       "      <td>chair</td>\n",
       "      <td>furniture beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>37</td>\n",
       "      <td>Female</td>\n",
       "      <td>58970.417842</td>\n",
       "      <td>PhD</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Timothyview</td>\n",
       "      <td>Retired</td>\n",
       "      <td>90.760070</td>\n",
       "      <td>shirt</td>\n",
       "      <td>clothing sports books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>Female</td>\n",
       "      <td>47683.671082</td>\n",
       "      <td>High School</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>South Julie</td>\n",
       "      <td>Retired</td>\n",
       "      <td>71.159528</td>\n",
       "      <td>chips</td>\n",
       "      <td>food electronics clothing books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>28</td>\n",
       "      <td>Female</td>\n",
       "      <td>42792.335252</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>Wesleytown</td>\n",
       "      <td>Businessman</td>\n",
       "      <td>93.007199</td>\n",
       "      <td>novel</td>\n",
       "      <td>beauty food home books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>34</td>\n",
       "      <td>Female</td>\n",
       "      <td>50458.706952</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Port Ralph</td>\n",
       "      <td>Businessman</td>\n",
       "      <td>109.910188</td>\n",
       "      <td>dictionary</td>\n",
       "      <td>books clothing sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>30</td>\n",
       "      <td>Female</td>\n",
       "      <td>75087.875430</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>East David</td>\n",
       "      <td>Salesman</td>\n",
       "      <td>117.514230</td>\n",
       "      <td>maggi</td>\n",
       "      <td>food furniture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>21</td>\n",
       "      <td>Female</td>\n",
       "      <td>55900.658343</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>North Bradyview</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>97.657083</td>\n",
       "      <td>sofa</td>\n",
       "      <td>home electronics clothing books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>44</td>\n",
       "      <td>Female</td>\n",
       "      <td>67543.429801</td>\n",
       "      <td>PhD</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>South Dawnberg</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>67.325065</td>\n",
       "      <td>cream</td>\n",
       "      <td>beauty books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>46</td>\n",
       "      <td>Female</td>\n",
       "      <td>60468.953352</td>\n",
       "      <td>Master</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>Stephenfort</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>103.702310</td>\n",
       "      <td>monitor</td>\n",
       "      <td>electronics beauty books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>37</td>\n",
       "      <td>Female</td>\n",
       "      <td>46212.644202</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>North James</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>83.486272</td>\n",
       "      <td>maggi</td>\n",
       "      <td>food home sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>58</td>\n",
       "      <td>Female</td>\n",
       "      <td>51057.893167</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>New Elizabeth</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>133.761082</td>\n",
       "      <td>lipstick</td>\n",
       "      <td>home beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>Female</td>\n",
       "      <td>48484.665228</td>\n",
       "      <td>High School</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>North Jerry</td>\n",
       "      <td>Salesman</td>\n",
       "      <td>77.034180</td>\n",
       "      <td>curtains</td>\n",
       "      <td>beauty furniture clothing home sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>35</td>\n",
       "      <td>Female</td>\n",
       "      <td>40317.993846</td>\n",
       "      <td>High School</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Aarontown</td>\n",
       "      <td>HR</td>\n",
       "      <td>104.835738</td>\n",
       "      <td>novel</td>\n",
       "      <td>books electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>58886.929583</td>\n",
       "      <td>High School</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>West Daniel</td>\n",
       "      <td>Salesman</td>\n",
       "      <td>152.104284</td>\n",
       "      <td>shirt</td>\n",
       "      <td>electronics clothing food books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>29</td>\n",
       "      <td>Female</td>\n",
       "      <td>52268.350643</td>\n",
       "      <td>High School</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>Lake Amanda</td>\n",
       "      <td>HR</td>\n",
       "      <td>83.563695</td>\n",
       "      <td>carpet</td>\n",
       "      <td>clothing home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>Female</td>\n",
       "      <td>47076.585366</td>\n",
       "      <td>PhD</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>Popehaven</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>127.547936</td>\n",
       "      <td>dictionary</td>\n",
       "      <td>food books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>33</td>\n",
       "      <td>Female</td>\n",
       "      <td>63572.503025</td>\n",
       "      <td>Master</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>Donnabury</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>120.039730</td>\n",
       "      <td>sofa</td>\n",
       "      <td>home electronics beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>19</td>\n",
       "      <td>Female</td>\n",
       "      <td>50332.892201</td>\n",
       "      <td>High School</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Chelseachester</td>\n",
       "      <td>Businessman</td>\n",
       "      <td>120.837078</td>\n",
       "      <td>ball</td>\n",
       "      <td>beauty sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>24</td>\n",
       "      <td>Female</td>\n",
       "      <td>44552.098947</td>\n",
       "      <td>High School</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>North Andrew</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>121.685146</td>\n",
       "      <td>dictionary</td>\n",
       "      <td>books home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>44</td>\n",
       "      <td>Female</td>\n",
       "      <td>42183.725846</td>\n",
       "      <td>Master</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Hahnview</td>\n",
       "      <td>Doctor</td>\n",
       "      <td>80.536493</td>\n",
       "      <td>mobile</td>\n",
       "      <td>electronics furniture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>46</td>\n",
       "      <td>Female</td>\n",
       "      <td>39312.647642</td>\n",
       "      <td>Master</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Padillaland</td>\n",
       "      <td>HR</td>\n",
       "      <td>112.164650</td>\n",
       "      <td>lipstick</td>\n",
       "      <td>food furniture beauty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>46</td>\n",
       "      <td>Female</td>\n",
       "      <td>40586.188383</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Martinezmouth</td>\n",
       "      <td>Doctor</td>\n",
       "      <td>108.018873</td>\n",
       "      <td>novel</td>\n",
       "      <td>home electronics furniture beauty books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>37</td>\n",
       "      <td>Female</td>\n",
       "      <td>43664.679377</td>\n",
       "      <td>PhD</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>Mariemouth</td>\n",
       "      <td>Retired</td>\n",
       "      <td>108.957984</td>\n",
       "      <td>bat</td>\n",
       "      <td>sports electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>49</td>\n",
       "      <td>Female</td>\n",
       "      <td>60730.377787</td>\n",
       "      <td>Master</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>West Gwendolynton</td>\n",
       "      <td>Salesman</td>\n",
       "      <td>105.168194</td>\n",
       "      <td>bat</td>\n",
       "      <td>books electronics food home sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>57</td>\n",
       "      <td>Female</td>\n",
       "      <td>59157.507783</td>\n",
       "      <td>High School</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Johnsonborough</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>119.540709</td>\n",
       "      <td>chips</td>\n",
       "      <td>furniture food home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>59</td>\n",
       "      <td>Female</td>\n",
       "      <td>27444.718354</td>\n",
       "      <td>PhD</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>South Krista</td>\n",
       "      <td>Housewife</td>\n",
       "      <td>71.062235</td>\n",
       "      <td>dictionary</td>\n",
       "      <td>home electronics furniture books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>28</td>\n",
       "      <td>Female</td>\n",
       "      <td>36344.056143</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>West Kimberlyview</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>119.720852</td>\n",
       "      <td>gloves</td>\n",
       "      <td>sports books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>39</td>\n",
       "      <td>Female</td>\n",
       "      <td>45885.227816</td>\n",
       "      <td>PhD</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>West Amyberg</td>\n",
       "      <td>HR</td>\n",
       "      <td>107.941510</td>\n",
       "      <td>carpet</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>39</td>\n",
       "      <td>Female</td>\n",
       "      <td>62148.036992</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>North Sydney</td>\n",
       "      <td>Salesman</td>\n",
       "      <td>109.619903</td>\n",
       "      <td>carpet</td>\n",
       "      <td>sports home books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>61</td>\n",
       "      <td>Female</td>\n",
       "      <td>49355.042834</td>\n",
       "      <td>Master</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>West Martha</td>\n",
       "      <td>Artist</td>\n",
       "      <td>86.603423</td>\n",
       "      <td>lipstick</td>\n",
       "      <td>beauty electronics home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>62</td>\n",
       "      <td>Female</td>\n",
       "      <td>43882.131763</td>\n",
       "      <td>High School</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>South Kristen</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>130.201356</td>\n",
       "      <td>chips</td>\n",
       "      <td>food clothing home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>42</td>\n",
       "      <td>Female</td>\n",
       "      <td>56639.407246</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>Charleneville</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>109.070753</td>\n",
       "      <td>novel</td>\n",
       "      <td>home books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>57</td>\n",
       "      <td>Female</td>\n",
       "      <td>72345.800985</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>North Mistyfort</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>80.335840</td>\n",
       "      <td>bed</td>\n",
       "      <td>beauty furniture sports</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  gender        income    education  married  children  \\\n",
       "1    24  Female  53229.101074  High School    False         1   \n",
       "2    45  Female  30066.046684     Bachelor     True         3   \n",
       "4    29  Female  44792.627094       Master    False         0   \n",
       "5    20  Female  51266.767047       Master     True         3   \n",
       "7    20  Female  35325.309005          PhD    False         3   \n",
       "10   57  Female  60907.844629          PhD     True         0   \n",
       "11   27  Female  51033.627935          PhD     True         2   \n",
       "13   44  Female  34683.652618       Master     True         2   \n",
       "14   51  Female  48815.172160     Bachelor    False         0   \n",
       "15   42  Female  40527.463149  High School    False         2   \n",
       "17   25  Female  40668.478951  High School    False         1   \n",
       "19   47  Female  50595.565055       Master     True         1   \n",
       "20   36  Female  34611.512757  High School     True         1   \n",
       "22   25  Female  46971.308307       Master    False         1   \n",
       "27   50  Female  45270.638579       Master    False         0   \n",
       "31   50  Female  40800.945097  High School     True         2   \n",
       "32   31  Female  57019.169559     Bachelor    False         0   \n",
       "33   57  Female  62617.100460  High School     True         3   \n",
       "34   37  Female  58970.417842          PhD    False         2   \n",
       "35   53  Female  47683.671082  High School     True         0   \n",
       "38   28  Female  42792.335252     Bachelor     True         2   \n",
       "39   34  Female  50458.706952     Bachelor    False         1   \n",
       "43   30  Female  75087.875430     Bachelor     True         0   \n",
       "44   21  Female  55900.658343     Bachelor     True         0   \n",
       "46   44  Female  67543.429801          PhD     True         2   \n",
       "48   46  Female  60468.953352       Master     True         2   \n",
       "51   37  Female  46212.644202     Bachelor     True         0   \n",
       "52   58  Female  51057.893167     Bachelor     True         1   \n",
       "54   54  Female  48484.665228  High School    False         3   \n",
       "55   35  Female  40317.993846  High School     True         0   \n",
       "56   42  Female  58886.929583  High School     True         3   \n",
       "58   29  Female  52268.350643  High School    False         3   \n",
       "59   59  Female  47076.585366          PhD    False         3   \n",
       "60   33  Female  63572.503025       Master     True         2   \n",
       "62   19  Female  50332.892201  High School    False         2   \n",
       "63   24  Female  44552.098947  High School    False         0   \n",
       "64   44  Female  42183.725846       Master    False         1   \n",
       "67   46  Female  39312.647642       Master    False         1   \n",
       "68   46  Female  40586.188383     Bachelor    False         1   \n",
       "69   37  Female  43664.679377          PhD     True         0   \n",
       "71   49  Female  60730.377787       Master     True         0   \n",
       "73   57  Female  59157.507783  High School    False         0   \n",
       "76   59  Female  27444.718354          PhD    False         0   \n",
       "77   28  Female  36344.056143     Bachelor    False         1   \n",
       "84   39  Female  45885.227816          PhD     True         3   \n",
       "85   39  Female  62148.036992     Bachelor    False         2   \n",
       "86   61  Female  49355.042834       Master     True         0   \n",
       "91   62  Female  43882.131763  High School    False         3   \n",
       "92   42  Female  56639.407246     Bachelor    False         3   \n",
       "94   57  Female  72345.800985     Bachelor    False         3   \n",
       "\n",
       "                 city   occupation  purchase_amount most bought item  \\\n",
       "1        Crystalburgh  Businessman       115.135586         lipstick   \n",
       "2       Margaretburgh     Engineer       101.694559         biscuits   \n",
       "4            New Paul  Businessman        86.847281           carpet   \n",
       "5           Frankport    Scientist        95.145103           laptop   \n",
       "7     South Raventown    Housewife       110.564517       dictionary   \n",
       "10   Gutierrezborough           HR        78.831675            shoes   \n",
       "11  Lake Tiffanyhaven    Scientist       138.672618            maggi   \n",
       "13  West Darlenemouth     Engineer        89.101465              bat   \n",
       "14    New Anthonytown     Salesman        65.797622         biscuits   \n",
       "15           Markfurt     Engineer       101.597316           gloves   \n",
       "17         South Tina  Businessman       124.487105         curtains   \n",
       "19         Stevenfort  Businessman       112.431977            chips   \n",
       "20  South Jeffreyberg       Lawyer        79.246469            chips   \n",
       "22       South Steven     Engineer       109.716368              bat   \n",
       "27        Tracieshire     Engineer        92.149693         curtains   \n",
       "31        East Kristy   Unemployed        80.740713     encyclopedia   \n",
       "32       Port Royport      Retired       119.808572           mobile   \n",
       "33         Floresfurt       Artist       128.026275            chair   \n",
       "34        Timothyview      Retired        90.760070            shirt   \n",
       "35        South Julie      Retired        71.159528            chips   \n",
       "38         Wesleytown  Businessman        93.007199            novel   \n",
       "39         Port Ralph  Businessman       109.910188       dictionary   \n",
       "43         East David     Salesman       117.514230            maggi   \n",
       "44    North Bradyview     Engineer        97.657083             sofa   \n",
       "46     South Dawnberg       Lawyer        67.325065            cream   \n",
       "48        Stephenfort   Unemployed       103.702310          monitor   \n",
       "51        North James    Scientist        83.486272            maggi   \n",
       "52      New Elizabeth     Engineer       133.761082         lipstick   \n",
       "54        North Jerry     Salesman        77.034180         curtains   \n",
       "55          Aarontown           HR       104.835738            novel   \n",
       "56        West Daniel     Salesman       152.104284            shirt   \n",
       "58        Lake Amanda           HR        83.563695           carpet   \n",
       "59          Popehaven   Unemployed       127.547936       dictionary   \n",
       "60          Donnabury       Lawyer       120.039730             sofa   \n",
       "62     Chelseachester  Businessman       120.837078             ball   \n",
       "63       North Andrew     Engineer       121.685146       dictionary   \n",
       "64           Hahnview       Doctor        80.536493           mobile   \n",
       "67        Padillaland           HR       112.164650         lipstick   \n",
       "68      Martinezmouth       Doctor       108.018873            novel   \n",
       "69         Mariemouth      Retired       108.957984              bat   \n",
       "71  West Gwendolynton     Salesman       105.168194              bat   \n",
       "73     Johnsonborough    Scientist       119.540709            chips   \n",
       "76       South Krista    Housewife        71.062235       dictionary   \n",
       "77  West Kimberlyview    Scientist       119.720852           gloves   \n",
       "84       West Amyberg           HR       107.941510           carpet   \n",
       "85       North Sydney     Salesman       109.619903           carpet   \n",
       "86        West Martha       Artist        86.603423         lipstick   \n",
       "91      South Kristen   Unemployed       130.201356            chips   \n",
       "92      Charleneville     Engineer       109.070753            novel   \n",
       "94    North Mistyfort       Lawyer        80.335840              bed   \n",
       "\n",
       "                                     labels  \n",
       "1                          furniture beauty  \n",
       "2          clothing electronics food sports  \n",
       "4                                      home  \n",
       "5                  sports electronics books  \n",
       "7                              books beauty  \n",
       "10                            clothing home  \n",
       "11                               food books  \n",
       "13               sports furniture food home  \n",
       "14                     beauty clothing food  \n",
       "15                    books clothing sports  \n",
       "17        furniture electronics home sports  \n",
       "19                           food furniture  \n",
       "20                                home food  \n",
       "22                            beauty sports  \n",
       "27                                food home  \n",
       "31             books clothing sports beauty  \n",
       "32               electronics furniture food  \n",
       "33                         furniture beauty  \n",
       "34                    clothing sports books  \n",
       "35          food electronics clothing books  \n",
       "38                   beauty food home books  \n",
       "39                    books clothing sports  \n",
       "43                           food furniture  \n",
       "44          home electronics clothing books  \n",
       "46                             beauty books  \n",
       "48                 electronics beauty books  \n",
       "51                         food home sports  \n",
       "52                              home beauty  \n",
       "54    beauty furniture clothing home sports  \n",
       "55                        books electronics  \n",
       "56          electronics clothing food books  \n",
       "58                            clothing home  \n",
       "59                               food books  \n",
       "60                  home electronics beauty  \n",
       "62                            beauty sports  \n",
       "63                               books home  \n",
       "64                    electronics furniture  \n",
       "67                    food furniture beauty  \n",
       "68  home electronics furniture beauty books  \n",
       "69                       sports electronics  \n",
       "71       books electronics food home sports  \n",
       "73                      furniture food home  \n",
       "76         home electronics furniture books  \n",
       "77                             sports books  \n",
       "84                                     home  \n",
       "85                        sports home books  \n",
       "86                  beauty electronics home  \n",
       "91                       food clothing home  \n",
       "92                               home books  \n",
       "94                  beauty furniture sports  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"gender\"]==\"Female\"].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f503db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "female_df = df[df[\"gender\"]==\"Female\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f4461a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'furniture': 179, 'beauty': 183, 'clothing': 162, 'electronics': 147, 'food': 173, 'sports': 183, 'home': 172, 'books': 177}\n"
     ]
    }
   ],
   "source": [
    "np_ds_female = np.array(female_df)\n",
    "\n",
    "map_labels1 = {}\n",
    "\n",
    "for arr in np_ds_female:\n",
    "    lbl_str = arr[-1].split(' ')\n",
    "    for lbl in lbl_str:\n",
    "        try:\n",
    "            map_labels1[lbl]+=1\n",
    "        except:\n",
    "            map_labels1[lbl]=1\n",
    "\n",
    "print(map_labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e792e638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_ds_female.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8908304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1376\n"
     ]
    }
   ],
   "source": [
    "print(sum(list(map_labels1.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "247b1c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_df = df[df[\"gender\"]==\"Male\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6b656af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'electronics': 185, 'clothing': 172, 'sports': 179, 'food': 170, 'beauty': 173, 'furniture': 157, 'home': 168, 'books': 178}\n"
     ]
    }
   ],
   "source": [
    "np_ds_male = np.array(male_df)\n",
    "\n",
    "map_labels2 = {}\n",
    "\n",
    "for arr in np_ds_male:\n",
    "    lbl_str = arr[-1].split(' ')\n",
    "    for lbl in lbl_str:\n",
    "        try:\n",
    "            map_labels2[lbl]+=1\n",
    "        except:\n",
    "            map_labels2[lbl]=1\n",
    "\n",
    "print(map_labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59054bdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Find common keys\n",
    "common_keys = set(map_labels1.keys()) & set(map_labels2.keys())\n",
    "\n",
    "# Prepare data for plotting\n",
    "values1 = [map_labels1[key] for key in common_keys]\n",
    "values2 = [map_labels2[key] for key in common_keys]\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Setting up the x-axis positions\n",
    "x = np.arange(len(common_keys))\n",
    "\n",
    "# Setting up the width for bars\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, values1, width, label='female')\n",
    "rects2 = ax.bar(x + width/2, values2, width, label='male')\n",
    "\n",
    "# Adding labels, title, and legend\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Comparison between values of common keys in Map Labels 1 and Map Labels 2')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(common_keys)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cff9f56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting  the train val test split\n",
    "# keeping the ration 70 : 15 : 15\n",
    "test = 0.10\n",
    "val = 0.15\n",
    "train = 0.80\n",
    "\n",
    "df = pd.read_csv('./advertisement.csv')\n",
    "data = np.array(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff7657f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can see column : 1,3,4,6,7,9 are categorical , hence need to be encoded.\n",
    "\"\"\"\n",
    "\n",
    "for column in [1,3,4,6,7,9]:\n",
    "    label_encode = LabelEncoder()\n",
    "    encoded_feature = label_encode.fit_transform(data[:,column])\n",
    "    data[:,column] = encoded_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27cdd52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 11)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f14843e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[45 1 61271.953358890096 ... 2 87.69711834242366 16]\n",
      " [24 0 53229.101073561775 ... 1 115.13558622549095 13]\n",
      " [45 0 30066.04668359846 ... 3 101.69455857438264 3]\n",
      " ...\n",
      " [64 0 68740.44200637276 ... 8 101.4346500112247 1]\n",
      " [18 0 44348.44667993933 ... 7 97.64998780156692 19]\n",
      " [61 1 39160.08367652033 ... 5 105.59148548653422 20]]\n"
     ]
    }
   ],
   "source": [
    "print(data[:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45d00b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_featur_sample = data[:,:-1].copy() \n",
    "my_featur_sample  = my_featur_sample.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "975a2d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_featur_sample = (my_featur_sample - np.mean(my_featur_sample,axis=0))/(np.std(my_featur_sample,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c486e213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(np.array([[1],[2],[3]]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28b74546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45 1 61271.953358890096 2 0 3 366 2 87.69711834242366 16\n",
      " 'electronics clothing sports']\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "872e5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:,:-1] = my_featur_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cecbbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 10) (800, 8)\n",
      "(50, 10) (50, 8)\n",
      "(150, 10) (150, 8)\n"
     ]
    }
   ],
   "source": [
    "label_encode_mlb = MultiLabelBinarizer()\n",
    "\n",
    "formatted_list = []\n",
    "\n",
    "for lbl_string in data[:,-1]:\n",
    "    formatted_list.append(lbl_string.split(' '))\n",
    "    \n",
    "encoded_labels = label_encode_mlb.fit_transform(formatted_list)\n",
    "    \n",
    "train_x,train_y = data[:int(train*data.shape[0]),:-1],encoded_labels[:int(train*data.shape[0])]\n",
    "val_x,val_y = data[int(train*data.shape[0]):int(train*data.shape[0])+int(val*data.shape[0]),:-1],encoded_labels[int(train*data.shape[0]):int(train*data.shape[0])+int(val*data.shape[0])]\n",
    "test_x,test_y = data[int(train*data.shape[0])+int(val*data.shape[0]):int(train*data.shape[0])+int(val*data.shape[0])+int(test*data.shape[0]),:-1],encoded_labels[int(train*data.shape[0])+int(val*data.shape[0]):int(train*data.shape[0])+int(val*data.shape[0])+int(test*data.shape[0])]\n",
    "\n",
    "print(train_x.shape,train_y.shape)\n",
    "print(test_x.shape,test_y.shape)\n",
    "print(val_x.shape,val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a90fc144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ball', 'bat', 'bed', 'biscuits', 'carpet', 'chair', 'chips',\n",
       "       'cream', 'curtains', 'dictionary', 'encyclopedia', 'gloves',\n",
       "       'laptop', 'lipstick', 'maggi', 'mobile', 'monitor', 'novel',\n",
       "       'pants', 'perfume', 'shirt', 'shoes', 'sofa', 'table'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encode.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcf16943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 8)\n",
      "(800, 10)\n"
     ]
    }
   ],
   "source": [
    "print(train_y.shape)\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "47a1d621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3021780994086131 1.0120728718702643 1.2055331632498885 ...\n",
      "  -0.97278093031368 -0.6588987724783364 0.6479072726304642]\n",
      " [-1.2217748514993776 -0.9880711436836177 0.39226380262719973 ...\n",
      "  -1.2875967653666507 0.6901873453247652 0.21548862571358032]\n",
      " [0.3021780994086131 -0.9880711436836177 -1.9499155344417818 ...\n",
      "  -0.657965095260709 0.029323047420286356 -1.2259068640093658]\n",
      " ...\n",
      " [-1.0040672870839504 -0.9880711436836177 -0.1949772876203305 ...\n",
      "  0.2864824098982035 0.21119436800313393 1.6568841154365266]\n",
      " [-1.3669132277763292 1.0120728718702643 0.009224830104372281 ...\n",
      "  0.2864824098982035 1.151382853576948 1.6568841154365266]\n",
      " [0.8827316045164192 -0.9880711436836177 -2.2453989253520885 ...\n",
      "  -1.6024126004196217 2.136759555367617 0.9361863705750534]]\n"
     ]
    }
   ],
   "source": [
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71f31229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# taken help from chatgpt\n",
    "\n",
    "\"\"\"\n",
    "Method of finding accuracy : \n",
    "- Though logically not that correct, due to accuracy constraints I used the hamming loss method indirectly, \n",
    "to calculate accuracy. \n",
    "\"\"\"\n",
    "\n",
    "class MLPClassifier_multilabel:\n",
    "    def __init__(self, learning_rate=0.01, activation='sigmoid', optimizer='sgd', hidden_layers=[10], epochs=100):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.epochs = epochs\n",
    "        self.activation_functions = {\n",
    "            'sigmoid': (self._sigmoid, self._sigmoid_derivative),\n",
    "            'tanh': (np.tanh, self._tanh_derivative),\n",
    "            'relu': (self._relu, self._relu_derivative)\n",
    "        }\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _sigmoid_derivative(self, z):\n",
    "        return z * (1 - z)\n",
    "\n",
    "    def _relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def _relu_derivative(self, z):\n",
    "        z[z <= 0] = 0\n",
    "        z[z > 0] = 1\n",
    "        return z\n",
    "\n",
    "    def _tanh_derivative(self, z):\n",
    "        return 1.0 - z**2\n",
    "\n",
    "    def _initialize_weights(self, input_size, output_size):\n",
    "        layer_sizes = [input_size] + self.hidden_layers + [output_size]\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01)\n",
    "            self.biases.append(np.zeros((1, layer_sizes[i+1])))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        activations = [X]\n",
    "        Zs = []\n",
    "\n",
    "        activation_func, _ = self.activation_functions[self.activation]\n",
    "\n",
    "        for i in range(len(self.weights)):\n",
    "            Z = np.dot(activations[-1], self.weights[i]) + self.biases[i]\n",
    "            Zs.append(Z)\n",
    "            if i == len(self.weights) - 1:  # If it's the last layer, apply sigmoid for multilabel\n",
    "                activations.append(self._sigmoid(Z))\n",
    "            else:\n",
    "                activations.append(activation_func(Z))\n",
    "\n",
    "        return activations, Zs\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        logprobs = np.multiply(np.log(y_pred), y_true) + np.multiply(np.log(1 - y_pred), (1 - y_true))\n",
    "        loss = - np.sum(logprobs) / m\n",
    "        return loss\n",
    "\n",
    "    def back_propagation(self, y, activations, Zs):\n",
    "        m = y.shape[0]\n",
    "        grads = {\"dw\": [], \"db\": []}\n",
    "\n",
    "        dZ = activations[-1] - y\n",
    "        dW = np.dot(activations[-2].T, dZ) / m\n",
    "        db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        grads[\"dw\"].append(dW)\n",
    "        grads[\"db\"].append(db)\n",
    "\n",
    "        _, activation_derivative = self.activation_functions[self.activation]\n",
    "\n",
    "        for i in range(len(self.hidden_layers), 0, -1):\n",
    "            dZ = np.dot(dZ, self.weights[i].T) * activation_derivative(activations[i])\n",
    "            dW = np.dot(activations[i-1].T, dZ) / m\n",
    "            db = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "            grads[\"dw\"].insert(0, dW)\n",
    "            grads[\"db\"].insert(0, db)\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def update_weights(self, grads):\n",
    "        for i in range(len(self.weights)):\n",
    "            self.weights[i] -= self.learning_rate * grads[\"dw\"][i]\n",
    "            self.biases[i] -= self.learning_rate * grads[\"db\"][i]\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        self._initialize_weights(X_train.shape[1], y_train.shape[1])\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            if self.optimizer == 'sgd':\n",
    "                for i in range(X_train.shape[0]):\n",
    "                    activations, Zs = self.forward_propagation(X_train[i:i+1])\n",
    "                    grads = self.back_propagation(y_train[i:i+1], activations, Zs)\n",
    "                    self.update_weights(grads)\n",
    "            elif self.optimizer == 'bgd':\n",
    "                activations, Zs = self.forward_propagation(X_train)\n",
    "                grads = self.back_propagation(y_train, activations, Zs)\n",
    "                self.update_weights(grads)\n",
    "            elif self.optimizer == 'mbgd':\n",
    "                batch_size = 32\n",
    "                for i in range(0, X_train.shape[0], batch_size):\n",
    "                    activations, Zs = self.forward_propagation(X_train[i:i+batch_size])\n",
    "                    grads = self.back_propagation(y_train[i:i+batch_size], activations, Zs)\n",
    "                    self.update_weights(grads)\n",
    "\n",
    "            train_activations, _ = self.forward_propagation(X_train)\n",
    "            print(train_activations[-1].shape,y_train.shape)\n",
    "            train_loss = self.compute_loss(y_train, train_activations[-1])\n",
    "            train_accuracy = np.mean((train_activations[-1] >= 0.5) == y_train)\n",
    "\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_activations, _ = self.forward_propagation(X_val)\n",
    "                val_loss = self.compute_loss(y_val, val_activations[-1])\n",
    "                val_accuracy = np.mean((val_activations[-1] > 0.5) == y_val)\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs} - train_loss: {train_loss:.4f} - train_accuracy: {train_accuracy:.4f} - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy:.4f}\")\n",
    "            else:\n",
    "                print(f\"Epoch {epoch+1}/{self.epochs} - train_loss: {train_loss:.4f} - train_accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        activations, _ = self.forward_propagation(X)\n",
    "        return (activations[-1] > 0.5).astype(int)\n",
    "\n",
    "    def inference(self, test_X, test_Y):\n",
    "        y_pred = self.predict(test_X)\n",
    "        # y_true_labels = np.argmax(test_Y, axis=1)\n",
    "        return np.mean(y_pred==test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3527a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 8) (800, 8)\n",
      "Epoch 1/1000 - train_loss: 5.1449 - train_accuracy: 0.6573 - val_loss: 5.2032 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 2/1000 - train_loss: 5.1448 - train_accuracy: 0.6573 - val_loss: 5.2037 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 3/1000 - train_loss: 5.1447 - train_accuracy: 0.6573 - val_loss: 5.2037 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 4/1000 - train_loss: 5.1446 - train_accuracy: 0.6573 - val_loss: 5.2037 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 5/1000 - train_loss: 5.1445 - train_accuracy: 0.6573 - val_loss: 5.2037 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 6/1000 - train_loss: 5.1444 - train_accuracy: 0.6573 - val_loss: 5.2037 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 7/1000 - train_loss: 5.1443 - train_accuracy: 0.6573 - val_loss: 5.2038 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 8/1000 - train_loss: 5.1442 - train_accuracy: 0.6573 - val_loss: 5.2038 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 9/1000 - train_loss: 5.1441 - train_accuracy: 0.6573 - val_loss: 5.2038 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 10/1000 - train_loss: 5.1441 - train_accuracy: 0.6573 - val_loss: 5.2039 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 11/1000 - train_loss: 5.1440 - train_accuracy: 0.6573 - val_loss: 5.2039 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 12/1000 - train_loss: 5.1439 - train_accuracy: 0.6573 - val_loss: 5.2039 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 13/1000 - train_loss: 5.1438 - train_accuracy: 0.6573 - val_loss: 5.2040 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 14/1000 - train_loss: 5.1437 - train_accuracy: 0.6573 - val_loss: 5.2040 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 15/1000 - train_loss: 5.1437 - train_accuracy: 0.6573 - val_loss: 5.2041 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 16/1000 - train_loss: 5.1436 - train_accuracy: 0.6573 - val_loss: 5.2041 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 17/1000 - train_loss: 5.1435 - train_accuracy: 0.6573 - val_loss: 5.2041 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 18/1000 - train_loss: 5.1434 - train_accuracy: 0.6573 - val_loss: 5.2042 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 19/1000 - train_loss: 5.1434 - train_accuracy: 0.6573 - val_loss: 5.2042 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 20/1000 - train_loss: 5.1433 - train_accuracy: 0.6573 - val_loss: 5.2043 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 21/1000 - train_loss: 5.1432 - train_accuracy: 0.6573 - val_loss: 5.2043 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 22/1000 - train_loss: 5.1431 - train_accuracy: 0.6573 - val_loss: 5.2044 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 23/1000 - train_loss: 5.1430 - train_accuracy: 0.6573 - val_loss: 5.2044 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 24/1000 - train_loss: 5.1429 - train_accuracy: 0.6573 - val_loss: 5.2045 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 25/1000 - train_loss: 5.1429 - train_accuracy: 0.6573 - val_loss: 5.2045 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 26/1000 - train_loss: 5.1428 - train_accuracy: 0.6573 - val_loss: 5.2046 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 27/1000 - train_loss: 5.1427 - train_accuracy: 0.6573 - val_loss: 5.2046 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 28/1000 - train_loss: 5.1426 - train_accuracy: 0.6573 - val_loss: 5.2046 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 29/1000 - train_loss: 5.1425 - train_accuracy: 0.6573 - val_loss: 5.2047 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 30/1000 - train_loss: 5.1424 - train_accuracy: 0.6573 - val_loss: 5.2047 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 31/1000 - train_loss: 5.1423 - train_accuracy: 0.6573 - val_loss: 5.2048 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 32/1000 - train_loss: 5.1422 - train_accuracy: 0.6573 - val_loss: 5.2048 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 33/1000 - train_loss: 5.1421 - train_accuracy: 0.6573 - val_loss: 5.2049 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 34/1000 - train_loss: 5.1420 - train_accuracy: 0.6573 - val_loss: 5.2049 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 35/1000 - train_loss: 5.1419 - train_accuracy: 0.6573 - val_loss: 5.2049 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 36/1000 - train_loss: 5.1417 - train_accuracy: 0.6573 - val_loss: 5.2050 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 37/1000 - train_loss: 5.1416 - train_accuracy: 0.6573 - val_loss: 5.2050 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 38/1000 - train_loss: 5.1415 - train_accuracy: 0.6573 - val_loss: 5.2051 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 39/1000 - train_loss: 5.1413 - train_accuracy: 0.6573 - val_loss: 5.2051 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 40/1000 - train_loss: 5.1412 - train_accuracy: 0.6573 - val_loss: 5.2051 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 41/1000 - train_loss: 5.1411 - train_accuracy: 0.6573 - val_loss: 5.2052 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 42/1000 - train_loss: 5.1409 - train_accuracy: 0.6573 - val_loss: 5.2052 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 43/1000 - train_loss: 5.1407 - train_accuracy: 0.6573 - val_loss: 5.2052 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 44/1000 - train_loss: 5.1406 - train_accuracy: 0.6573 - val_loss: 5.2052 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 45/1000 - train_loss: 5.1404 - train_accuracy: 0.6573 - val_loss: 5.2053 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 46/1000 - train_loss: 5.1402 - train_accuracy: 0.6573 - val_loss: 5.2053 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 47/1000 - train_loss: 5.1400 - train_accuracy: 0.6573 - val_loss: 5.2053 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 48/1000 - train_loss: 5.1397 - train_accuracy: 0.6573 - val_loss: 5.2053 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 49/1000 - train_loss: 5.1395 - train_accuracy: 0.6573 - val_loss: 5.2053 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 50/1000 - train_loss: 5.1392 - train_accuracy: 0.6573 - val_loss: 5.2053 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 51/1000 - train_loss: 5.1389 - train_accuracy: 0.6573 - val_loss: 5.2052 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 52/1000 - train_loss: 5.1386 - train_accuracy: 0.6573 - val_loss: 5.2052 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 53/1000 - train_loss: 5.1383 - train_accuracy: 0.6573 - val_loss: 5.2051 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 54/1000 - train_loss: 5.1379 - train_accuracy: 0.6573 - val_loss: 5.2050 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 55/1000 - train_loss: 5.1374 - train_accuracy: 0.6573 - val_loss: 5.2049 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 56/1000 - train_loss: 5.1369 - train_accuracy: 0.6573 - val_loss: 5.2047 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 57/1000 - train_loss: 5.1364 - train_accuracy: 0.6573 - val_loss: 5.2045 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 58/1000 - train_loss: 5.1358 - train_accuracy: 0.6573 - val_loss: 5.2042 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 59/1000 - train_loss: 5.1350 - train_accuracy: 0.6573 - val_loss: 5.2039 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 60/1000 - train_loss: 5.1342 - train_accuracy: 0.6573 - val_loss: 5.2035 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 61/1000 - train_loss: 5.1333 - train_accuracy: 0.6573 - val_loss: 5.2030 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 62/1000 - train_loss: 5.1322 - train_accuracy: 0.6573 - val_loss: 5.2024 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 63/1000 - train_loss: 5.1309 - train_accuracy: 0.6573 - val_loss: 5.2016 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 64/1000 - train_loss: 5.1295 - train_accuracy: 0.6573 - val_loss: 5.2007 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 65/1000 - train_loss: 5.1278 - train_accuracy: 0.6573 - val_loss: 5.1996 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 66/1000 - train_loss: 5.1259 - train_accuracy: 0.6573 - val_loss: 5.1982 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 67/1000 - train_loss: 5.1237 - train_accuracy: 0.6573 - val_loss: 5.1967 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 68/1000 - train_loss: 5.1211 - train_accuracy: 0.6573 - val_loss: 5.1948 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 69/1000 - train_loss: 5.1183 - train_accuracy: 0.6573 - val_loss: 5.1926 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 70/1000 - train_loss: 5.1150 - train_accuracy: 0.6573 - val_loss: 5.1901 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 71/1000 - train_loss: 5.1114 - train_accuracy: 0.6573 - val_loss: 5.1873 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 72/1000 - train_loss: 5.1074 - train_accuracy: 0.6573 - val_loss: 5.1841 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 73/1000 - train_loss: 5.1031 - train_accuracy: 0.6573 - val_loss: 5.1806 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 74/1000 - train_loss: 5.0986 - train_accuracy: 0.6573 - val_loss: 5.1768 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 75/1000 - train_loss: 5.0938 - train_accuracy: 0.6573 - val_loss: 5.1728 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 76/1000 - train_loss: 5.0890 - train_accuracy: 0.6573 - val_loss: 5.1687 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 77/1000 - train_loss: 5.0842 - train_accuracy: 0.6573 - val_loss: 5.1645 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 78/1000 - train_loss: 5.0795 - train_accuracy: 0.6573 - val_loss: 5.1605 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 79/1000 - train_loss: 5.0751 - train_accuracy: 0.6575 - val_loss: 5.1566 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 80/1000 - train_loss: 5.0711 - train_accuracy: 0.6587 - val_loss: 5.1531 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 81/1000 - train_loss: 5.0675 - train_accuracy: 0.6595 - val_loss: 5.1498 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 82/1000 - train_loss: 5.0644 - train_accuracy: 0.6619 - val_loss: 5.1470 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 83/1000 - train_loss: 5.0617 - train_accuracy: 0.6631 - val_loss: 5.1445 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 84/1000 - train_loss: 5.0594 - train_accuracy: 0.6647 - val_loss: 5.1423 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 85/1000 - train_loss: 5.0574 - train_accuracy: 0.6658 - val_loss: 5.1405 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 86/1000 - train_loss: 5.0558 - train_accuracy: 0.6664 - val_loss: 5.1389 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 87/1000 - train_loss: 5.0545 - train_accuracy: 0.6670 - val_loss: 5.1377 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 88/1000 - train_loss: 5.0534 - train_accuracy: 0.6673 - val_loss: 5.1366 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 89/1000 - train_loss: 5.0524 - train_accuracy: 0.6673 - val_loss: 5.1357 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 90/1000 - train_loss: 5.0517 - train_accuracy: 0.6669 - val_loss: 5.1349 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 91/1000 - train_loss: 5.0510 - train_accuracy: 0.6663 - val_loss: 5.1343 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 92/1000 - train_loss: 5.0505 - train_accuracy: 0.6661 - val_loss: 5.1338 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 93/1000 - train_loss: 5.0500 - train_accuracy: 0.6658 - val_loss: 5.1333 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 94/1000 - train_loss: 5.0496 - train_accuracy: 0.6655 - val_loss: 5.1330 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 95/1000 - train_loss: 5.0493 - train_accuracy: 0.6655 - val_loss: 5.1327 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 96/1000 - train_loss: 5.0490 - train_accuracy: 0.6655 - val_loss: 5.1324 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 97/1000 - train_loss: 5.0487 - train_accuracy: 0.6655 - val_loss: 5.1322 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 98/1000 - train_loss: 5.0485 - train_accuracy: 0.6655 - val_loss: 5.1321 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 99/1000 - train_loss: 5.0483 - train_accuracy: 0.6655 - val_loss: 5.1319 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 100/1000 - train_loss: 5.0481 - train_accuracy: 0.6655 - val_loss: 5.1319 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 101/1000 - train_loss: 5.0479 - train_accuracy: 0.6653 - val_loss: 5.1318 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 102/1000 - train_loss: 5.0477 - train_accuracy: 0.6653 - val_loss: 5.1317 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 103/1000 - train_loss: 5.0476 - train_accuracy: 0.6653 - val_loss: 5.1317 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 104/1000 - train_loss: 5.0475 - train_accuracy: 0.6653 - val_loss: 5.1317 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 105/1000 - train_loss: 5.0473 - train_accuracy: 0.6652 - val_loss: 5.1317 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 106/1000 - train_loss: 5.0472 - train_accuracy: 0.6652 - val_loss: 5.1317 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 107/1000 - train_loss: 5.0471 - train_accuracy: 0.6648 - val_loss: 5.1318 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 108/1000 - train_loss: 5.0470 - train_accuracy: 0.6647 - val_loss: 5.1318 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 109/1000 - train_loss: 5.0469 - train_accuracy: 0.6647 - val_loss: 5.1318 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 110/1000 - train_loss: 5.0468 - train_accuracy: 0.6644 - val_loss: 5.1319 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 111/1000 - train_loss: 5.0467 - train_accuracy: 0.6645 - val_loss: 5.1319 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 112/1000 - train_loss: 5.0466 - train_accuracy: 0.6644 - val_loss: 5.1320 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 113/1000 - train_loss: 5.0465 - train_accuracy: 0.6644 - val_loss: 5.1321 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 114/1000 - train_loss: 5.0465 - train_accuracy: 0.6644 - val_loss: 5.1321 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 115/1000 - train_loss: 5.0464 - train_accuracy: 0.6644 - val_loss: 5.1322 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 116/1000 - train_loss: 5.0463 - train_accuracy: 0.6644 - val_loss: 5.1323 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 117/1000 - train_loss: 5.0462 - train_accuracy: 0.6644 - val_loss: 5.1324 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 118/1000 - train_loss: 5.0461 - train_accuracy: 0.6644 - val_loss: 5.1324 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 119/1000 - train_loss: 5.0461 - train_accuracy: 0.6644 - val_loss: 5.1325 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 120/1000 - train_loss: 5.0460 - train_accuracy: 0.6644 - val_loss: 5.1326 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 121/1000 - train_loss: 5.0459 - train_accuracy: 0.6644 - val_loss: 5.1327 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 122/1000 - train_loss: 5.0459 - train_accuracy: 0.6644 - val_loss: 5.1328 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 123/1000 - train_loss: 5.0458 - train_accuracy: 0.6644 - val_loss: 5.1329 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 124/1000 - train_loss: 5.0457 - train_accuracy: 0.6644 - val_loss: 5.1329 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 125/1000 - train_loss: 5.0457 - train_accuracy: 0.6644 - val_loss: 5.1330 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 126/1000 - train_loss: 5.0456 - train_accuracy: 0.6644 - val_loss: 5.1331 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 127/1000 - train_loss: 5.0455 - train_accuracy: 0.6642 - val_loss: 5.1332 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 128/1000 - train_loss: 5.0455 - train_accuracy: 0.6642 - val_loss: 5.1333 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 129/1000 - train_loss: 5.0454 - train_accuracy: 0.6642 - val_loss: 5.1334 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 130/1000 - train_loss: 5.0453 - train_accuracy: 0.6642 - val_loss: 5.1335 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 131/1000 - train_loss: 5.0453 - train_accuracy: 0.6642 - val_loss: 5.1335 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 132/1000 - train_loss: 5.0452 - train_accuracy: 0.6642 - val_loss: 5.1336 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 133/1000 - train_loss: 5.0452 - train_accuracy: 0.6642 - val_loss: 5.1337 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 134/1000 - train_loss: 5.0451 - train_accuracy: 0.6642 - val_loss: 5.1338 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 135/1000 - train_loss: 5.0450 - train_accuracy: 0.6642 - val_loss: 5.1339 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 136/1000 - train_loss: 5.0450 - train_accuracy: 0.6642 - val_loss: 5.1339 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 137/1000 - train_loss: 5.0449 - train_accuracy: 0.6642 - val_loss: 5.1340 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 138/1000 - train_loss: 5.0449 - train_accuracy: 0.6641 - val_loss: 5.1341 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 139/1000 - train_loss: 5.0448 - train_accuracy: 0.6641 - val_loss: 5.1342 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 140/1000 - train_loss: 5.0448 - train_accuracy: 0.6641 - val_loss: 5.1342 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 141/1000 - train_loss: 5.0447 - train_accuracy: 0.6641 - val_loss: 5.1343 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 142/1000 - train_loss: 5.0447 - train_accuracy: 0.6639 - val_loss: 5.1344 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 143/1000 - train_loss: 5.0446 - train_accuracy: 0.6639 - val_loss: 5.1345 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 144/1000 - train_loss: 5.0445 - train_accuracy: 0.6639 - val_loss: 5.1345 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 145/1000 - train_loss: 5.0445 - train_accuracy: 0.6639 - val_loss: 5.1346 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 146/1000 - train_loss: 5.0444 - train_accuracy: 0.6639 - val_loss: 5.1347 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 147/1000 - train_loss: 5.0444 - train_accuracy: 0.6639 - val_loss: 5.1347 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 148/1000 - train_loss: 5.0443 - train_accuracy: 0.6641 - val_loss: 5.1348 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 149/1000 - train_loss: 5.0443 - train_accuracy: 0.6641 - val_loss: 5.1349 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 150/1000 - train_loss: 5.0442 - train_accuracy: 0.6641 - val_loss: 5.1349 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 151/1000 - train_loss: 5.0442 - train_accuracy: 0.6641 - val_loss: 5.1350 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 152/1000 - train_loss: 5.0441 - train_accuracy: 0.6641 - val_loss: 5.1351 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 153/1000 - train_loss: 5.0441 - train_accuracy: 0.6639 - val_loss: 5.1351 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 154/1000 - train_loss: 5.0440 - train_accuracy: 0.6639 - val_loss: 5.1352 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 155/1000 - train_loss: 5.0440 - train_accuracy: 0.6639 - val_loss: 5.1352 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 156/1000 - train_loss: 5.0439 - train_accuracy: 0.6639 - val_loss: 5.1353 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 157/1000 - train_loss: 5.0439 - train_accuracy: 0.6639 - val_loss: 5.1353 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 158/1000 - train_loss: 5.0438 - train_accuracy: 0.6639 - val_loss: 5.1354 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 159/1000 - train_loss: 5.0438 - train_accuracy: 0.6639 - val_loss: 5.1354 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 160/1000 - train_loss: 5.0437 - train_accuracy: 0.6639 - val_loss: 5.1355 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 161/1000 - train_loss: 5.0437 - train_accuracy: 0.6639 - val_loss: 5.1355 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 162/1000 - train_loss: 5.0436 - train_accuracy: 0.6639 - val_loss: 5.1356 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 163/1000 - train_loss: 5.0436 - train_accuracy: 0.6639 - val_loss: 5.1356 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 164/1000 - train_loss: 5.0435 - train_accuracy: 0.6639 - val_loss: 5.1357 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 165/1000 - train_loss: 5.0434 - train_accuracy: 0.6641 - val_loss: 5.1357 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 166/1000 - train_loss: 5.0434 - train_accuracy: 0.6641 - val_loss: 5.1358 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 167/1000 - train_loss: 5.0433 - train_accuracy: 0.6641 - val_loss: 5.1358 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 168/1000 - train_loss: 5.0433 - train_accuracy: 0.6641 - val_loss: 5.1358 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 169/1000 - train_loss: 5.0432 - train_accuracy: 0.6641 - val_loss: 5.1359 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 170/1000 - train_loss: 5.0432 - train_accuracy: 0.6641 - val_loss: 5.1359 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 171/1000 - train_loss: 5.0431 - train_accuracy: 0.6641 - val_loss: 5.1359 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 172/1000 - train_loss: 5.0431 - train_accuracy: 0.6641 - val_loss: 5.1360 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 173/1000 - train_loss: 5.0430 - train_accuracy: 0.6641 - val_loss: 5.1360 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 174/1000 - train_loss: 5.0430 - train_accuracy: 0.6641 - val_loss: 5.1360 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 175/1000 - train_loss: 5.0429 - train_accuracy: 0.6641 - val_loss: 5.1361 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 176/1000 - train_loss: 5.0428 - train_accuracy: 0.6641 - val_loss: 5.1361 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 177/1000 - train_loss: 5.0428 - train_accuracy: 0.6641 - val_loss: 5.1361 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 178/1000 - train_loss: 5.0427 - train_accuracy: 0.6641 - val_loss: 5.1361 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 179/1000 - train_loss: 5.0427 - train_accuracy: 0.6641 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 180/1000 - train_loss: 5.0426 - train_accuracy: 0.6641 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 181/1000 - train_loss: 5.0425 - train_accuracy: 0.6641 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 182/1000 - train_loss: 5.0425 - train_accuracy: 0.6641 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 183/1000 - train_loss: 5.0424 - train_accuracy: 0.6642 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 184/1000 - train_loss: 5.0424 - train_accuracy: 0.6642 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 185/1000 - train_loss: 5.0423 - train_accuracy: 0.6642 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 186/1000 - train_loss: 5.0422 - train_accuracy: 0.6642 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 187/1000 - train_loss: 5.0422 - train_accuracy: 0.6644 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 188/1000 - train_loss: 5.0421 - train_accuracy: 0.6644 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 189/1000 - train_loss: 5.0420 - train_accuracy: 0.6644 - val_loss: 5.1363 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 190/1000 - train_loss: 5.0420 - train_accuracy: 0.6644 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 191/1000 - train_loss: 5.0419 - train_accuracy: 0.6644 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 192/1000 - train_loss: 5.0418 - train_accuracy: 0.6644 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 193/1000 - train_loss: 5.0417 - train_accuracy: 0.6644 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 194/1000 - train_loss: 5.0417 - train_accuracy: 0.6644 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 195/1000 - train_loss: 5.0416 - train_accuracy: 0.6642 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 196/1000 - train_loss: 5.0415 - train_accuracy: 0.6642 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 197/1000 - train_loss: 5.0414 - train_accuracy: 0.6642 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 198/1000 - train_loss: 5.0414 - train_accuracy: 0.6644 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 199/1000 - train_loss: 5.0413 - train_accuracy: 0.6644 - val_loss: 5.1362 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 200/1000 - train_loss: 5.0412 - train_accuracy: 0.6644 - val_loss: 5.1361 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 201/1000 - train_loss: 5.0411 - train_accuracy: 0.6644 - val_loss: 5.1361 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 202/1000 - train_loss: 5.0410 - train_accuracy: 0.6644 - val_loss: 5.1361 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 203/1000 - train_loss: 5.0409 - train_accuracy: 0.6644 - val_loss: 5.1361 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 204/1000 - train_loss: 5.0408 - train_accuracy: 0.6644 - val_loss: 5.1360 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 205/1000 - train_loss: 5.0407 - train_accuracy: 0.6644 - val_loss: 5.1360 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 206/1000 - train_loss: 5.0407 - train_accuracy: 0.6644 - val_loss: 5.1360 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 207/1000 - train_loss: 5.0406 - train_accuracy: 0.6644 - val_loss: 5.1359 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 208/1000 - train_loss: 5.0405 - train_accuracy: 0.6647 - val_loss: 5.1359 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 209/1000 - train_loss: 5.0404 - train_accuracy: 0.6647 - val_loss: 5.1359 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 210/1000 - train_loss: 5.0403 - train_accuracy: 0.6647 - val_loss: 5.1358 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 211/1000 - train_loss: 5.0402 - train_accuracy: 0.6647 - val_loss: 5.1358 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 212/1000 - train_loss: 5.0401 - train_accuracy: 0.6648 - val_loss: 5.1357 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 213/1000 - train_loss: 5.0400 - train_accuracy: 0.6648 - val_loss: 5.1357 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 214/1000 - train_loss: 5.0398 - train_accuracy: 0.6648 - val_loss: 5.1357 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 215/1000 - train_loss: 5.0397 - train_accuracy: 0.6650 - val_loss: 5.1356 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 216/1000 - train_loss: 5.0396 - train_accuracy: 0.6650 - val_loss: 5.1356 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 217/1000 - train_loss: 5.0395 - train_accuracy: 0.6648 - val_loss: 5.1355 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 218/1000 - train_loss: 5.0394 - train_accuracy: 0.6648 - val_loss: 5.1355 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 219/1000 - train_loss: 5.0393 - train_accuracy: 0.6648 - val_loss: 5.1354 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 220/1000 - train_loss: 5.0392 - train_accuracy: 0.6648 - val_loss: 5.1354 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 221/1000 - train_loss: 5.0391 - train_accuracy: 0.6648 - val_loss: 5.1353 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 222/1000 - train_loss: 5.0389 - train_accuracy: 0.6648 - val_loss: 5.1353 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 223/1000 - train_loss: 5.0388 - train_accuracy: 0.6650 - val_loss: 5.1352 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 224/1000 - train_loss: 5.0387 - train_accuracy: 0.6650 - val_loss: 5.1351 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 225/1000 - train_loss: 5.0386 - train_accuracy: 0.6650 - val_loss: 5.1351 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 226/1000 - train_loss: 5.0384 - train_accuracy: 0.6650 - val_loss: 5.1350 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 227/1000 - train_loss: 5.0383 - train_accuracy: 0.6650 - val_loss: 5.1350 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 228/1000 - train_loss: 5.0382 - train_accuracy: 0.6650 - val_loss: 5.1349 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 229/1000 - train_loss: 5.0380 - train_accuracy: 0.6650 - val_loss: 5.1348 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 230/1000 - train_loss: 5.0379 - train_accuracy: 0.6650 - val_loss: 5.1348 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 231/1000 - train_loss: 5.0378 - train_accuracy: 0.6650 - val_loss: 5.1347 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 232/1000 - train_loss: 5.0376 - train_accuracy: 0.6652 - val_loss: 5.1347 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 233/1000 - train_loss: 5.0375 - train_accuracy: 0.6652 - val_loss: 5.1346 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 234/1000 - train_loss: 5.0373 - train_accuracy: 0.6652 - val_loss: 5.1345 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 235/1000 - train_loss: 5.0372 - train_accuracy: 0.6652 - val_loss: 5.1344 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 236/1000 - train_loss: 5.0370 - train_accuracy: 0.6652 - val_loss: 5.1344 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 237/1000 - train_loss: 5.0369 - train_accuracy: 0.6653 - val_loss: 5.1343 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 238/1000 - train_loss: 5.0367 - train_accuracy: 0.6653 - val_loss: 5.1342 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 239/1000 - train_loss: 5.0366 - train_accuracy: 0.6652 - val_loss: 5.1341 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 240/1000 - train_loss: 5.0364 - train_accuracy: 0.6652 - val_loss: 5.1340 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 241/1000 - train_loss: 5.0362 - train_accuracy: 0.6652 - val_loss: 5.1339 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 242/1000 - train_loss: 5.0361 - train_accuracy: 0.6652 - val_loss: 5.1338 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 243/1000 - train_loss: 5.0359 - train_accuracy: 0.6650 - val_loss: 5.1337 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 244/1000 - train_loss: 5.0357 - train_accuracy: 0.6648 - val_loss: 5.1336 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 245/1000 - train_loss: 5.0355 - train_accuracy: 0.6648 - val_loss: 5.1335 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 246/1000 - train_loss: 5.0353 - train_accuracy: 0.6648 - val_loss: 5.1334 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 247/1000 - train_loss: 5.0351 - train_accuracy: 0.6648 - val_loss: 5.1333 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 248/1000 - train_loss: 5.0349 - train_accuracy: 0.6647 - val_loss: 5.1331 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 249/1000 - train_loss: 5.0347 - train_accuracy: 0.6645 - val_loss: 5.1330 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 250/1000 - train_loss: 5.0345 - train_accuracy: 0.6645 - val_loss: 5.1328 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 251/1000 - train_loss: 5.0343 - train_accuracy: 0.6645 - val_loss: 5.1327 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 252/1000 - train_loss: 5.0341 - train_accuracy: 0.6645 - val_loss: 5.1325 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 253/1000 - train_loss: 5.0338 - train_accuracy: 0.6645 - val_loss: 5.1323 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 254/1000 - train_loss: 5.0336 - train_accuracy: 0.6645 - val_loss: 5.1321 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 255/1000 - train_loss: 5.0333 - train_accuracy: 0.6645 - val_loss: 5.1319 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 256/1000 - train_loss: 5.0331 - train_accuracy: 0.6645 - val_loss: 5.1317 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 257/1000 - train_loss: 5.0328 - train_accuracy: 0.6648 - val_loss: 5.1314 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 258/1000 - train_loss: 5.0325 - train_accuracy: 0.6648 - val_loss: 5.1312 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 259/1000 - train_loss: 5.0322 - train_accuracy: 0.6650 - val_loss: 5.1309 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 260/1000 - train_loss: 5.0319 - train_accuracy: 0.6650 - val_loss: 5.1306 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 261/1000 - train_loss: 5.0316 - train_accuracy: 0.6650 - val_loss: 5.1303 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 262/1000 - train_loss: 5.0312 - train_accuracy: 0.6650 - val_loss: 5.1300 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 263/1000 - train_loss: 5.0309 - train_accuracy: 0.6650 - val_loss: 5.1297 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 264/1000 - train_loss: 5.0305 - train_accuracy: 0.6650 - val_loss: 5.1294 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 265/1000 - train_loss: 5.0302 - train_accuracy: 0.6650 - val_loss: 5.1290 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 266/1000 - train_loss: 5.0298 - train_accuracy: 0.6650 - val_loss: 5.1286 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 267/1000 - train_loss: 5.0294 - train_accuracy: 0.6653 - val_loss: 5.1283 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 268/1000 - train_loss: 5.0290 - train_accuracy: 0.6653 - val_loss: 5.1279 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 269/1000 - train_loss: 5.0286 - train_accuracy: 0.6655 - val_loss: 5.1275 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 270/1000 - train_loss: 5.0282 - train_accuracy: 0.6655 - val_loss: 5.1271 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 271/1000 - train_loss: 5.0277 - train_accuracy: 0.6656 - val_loss: 5.1267 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 272/1000 - train_loss: 5.0273 - train_accuracy: 0.6656 - val_loss: 5.1263 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 273/1000 - train_loss: 5.0268 - train_accuracy: 0.6655 - val_loss: 5.1259 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 274/1000 - train_loss: 5.0264 - train_accuracy: 0.6656 - val_loss: 5.1255 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 275/1000 - train_loss: 5.0259 - train_accuracy: 0.6656 - val_loss: 5.1251 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 276/1000 - train_loss: 5.0254 - train_accuracy: 0.6656 - val_loss: 5.1247 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 277/1000 - train_loss: 5.0250 - train_accuracy: 0.6656 - val_loss: 5.1243 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 278/1000 - train_loss: 5.0245 - train_accuracy: 0.6656 - val_loss: 5.1239 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 279/1000 - train_loss: 5.0240 - train_accuracy: 0.6656 - val_loss: 5.1235 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 280/1000 - train_loss: 5.0235 - train_accuracy: 0.6656 - val_loss: 5.1231 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 281/1000 - train_loss: 5.0229 - train_accuracy: 0.6658 - val_loss: 5.1227 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 282/1000 - train_loss: 5.0224 - train_accuracy: 0.6656 - val_loss: 5.1223 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 283/1000 - train_loss: 5.0219 - train_accuracy: 0.6658 - val_loss: 5.1220 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 284/1000 - train_loss: 5.0213 - train_accuracy: 0.6658 - val_loss: 5.1216 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 285/1000 - train_loss: 5.0208 - train_accuracy: 0.6656 - val_loss: 5.1212 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 286/1000 - train_loss: 5.0202 - train_accuracy: 0.6656 - val_loss: 5.1209 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 287/1000 - train_loss: 5.0196 - train_accuracy: 0.6659 - val_loss: 5.1205 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 288/1000 - train_loss: 5.0191 - train_accuracy: 0.6661 - val_loss: 5.1202 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 289/1000 - train_loss: 5.0185 - train_accuracy: 0.6661 - val_loss: 5.1198 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 290/1000 - train_loss: 5.0179 - train_accuracy: 0.6661 - val_loss: 5.1195 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 291/1000 - train_loss: 5.0173 - train_accuracy: 0.6663 - val_loss: 5.1192 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 292/1000 - train_loss: 5.0167 - train_accuracy: 0.6666 - val_loss: 5.1188 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 293/1000 - train_loss: 5.0160 - train_accuracy: 0.6666 - val_loss: 5.1185 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 294/1000 - train_loss: 5.0154 - train_accuracy: 0.6666 - val_loss: 5.1182 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 295/1000 - train_loss: 5.0148 - train_accuracy: 0.6666 - val_loss: 5.1179 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 296/1000 - train_loss: 5.0141 - train_accuracy: 0.6667 - val_loss: 5.1175 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 297/1000 - train_loss: 5.0134 - train_accuracy: 0.6667 - val_loss: 5.1172 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 298/1000 - train_loss: 5.0128 - train_accuracy: 0.6669 - val_loss: 5.1169 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 299/1000 - train_loss: 5.0121 - train_accuracy: 0.6670 - val_loss: 5.1166 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 300/1000 - train_loss: 5.0114 - train_accuracy: 0.6670 - val_loss: 5.1163 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 301/1000 - train_loss: 5.0107 - train_accuracy: 0.6670 - val_loss: 5.1160 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 302/1000 - train_loss: 5.0099 - train_accuracy: 0.6670 - val_loss: 5.1157 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 303/1000 - train_loss: 5.0092 - train_accuracy: 0.6670 - val_loss: 5.1155 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 304/1000 - train_loss: 5.0085 - train_accuracy: 0.6670 - val_loss: 5.1152 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 305/1000 - train_loss: 5.0077 - train_accuracy: 0.6672 - val_loss: 5.1149 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 306/1000 - train_loss: 5.0069 - train_accuracy: 0.6672 - val_loss: 5.1146 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 307/1000 - train_loss: 5.0061 - train_accuracy: 0.6673 - val_loss: 5.1144 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 308/1000 - train_loss: 5.0053 - train_accuracy: 0.6672 - val_loss: 5.1141 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 309/1000 - train_loss: 5.0045 - train_accuracy: 0.6672 - val_loss: 5.1139 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 310/1000 - train_loss: 5.0037 - train_accuracy: 0.6672 - val_loss: 5.1136 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 311/1000 - train_loss: 5.0029 - train_accuracy: 0.6672 - val_loss: 5.1134 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 312/1000 - train_loss: 5.0020 - train_accuracy: 0.6670 - val_loss: 5.1132 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 313/1000 - train_loss: 5.0011 - train_accuracy: 0.6670 - val_loss: 5.1129 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 314/1000 - train_loss: 5.0003 - train_accuracy: 0.6670 - val_loss: 5.1127 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 315/1000 - train_loss: 4.9994 - train_accuracy: 0.6670 - val_loss: 5.1125 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 316/1000 - train_loss: 4.9985 - train_accuracy: 0.6670 - val_loss: 5.1123 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 317/1000 - train_loss: 4.9976 - train_accuracy: 0.6670 - val_loss: 5.1121 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 318/1000 - train_loss: 4.9966 - train_accuracy: 0.6670 - val_loss: 5.1119 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 319/1000 - train_loss: 4.9957 - train_accuracy: 0.6672 - val_loss: 5.1118 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 320/1000 - train_loss: 4.9948 - train_accuracy: 0.6672 - val_loss: 5.1116 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 321/1000 - train_loss: 4.9938 - train_accuracy: 0.6669 - val_loss: 5.1115 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 322/1000 - train_loss: 4.9928 - train_accuracy: 0.6669 - val_loss: 5.1113 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 323/1000 - train_loss: 4.9919 - train_accuracy: 0.6669 - val_loss: 5.1112 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 324/1000 - train_loss: 4.9909 - train_accuracy: 0.6670 - val_loss: 5.1111 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 325/1000 - train_loss: 4.9899 - train_accuracy: 0.6670 - val_loss: 5.1109 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 326/1000 - train_loss: 4.9890 - train_accuracy: 0.6670 - val_loss: 5.1108 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 327/1000 - train_loss: 4.9880 - train_accuracy: 0.6670 - val_loss: 5.1107 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 328/1000 - train_loss: 4.9870 - train_accuracy: 0.6669 - val_loss: 5.1106 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 329/1000 - train_loss: 4.9861 - train_accuracy: 0.6669 - val_loss: 5.1105 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 330/1000 - train_loss: 4.9851 - train_accuracy: 0.6670 - val_loss: 5.1105 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 331/1000 - train_loss: 4.9841 - train_accuracy: 0.6669 - val_loss: 5.1104 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 332/1000 - train_loss: 4.9832 - train_accuracy: 0.6672 - val_loss: 5.1103 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 333/1000 - train_loss: 4.9822 - train_accuracy: 0.6669 - val_loss: 5.1103 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 334/1000 - train_loss: 4.9813 - train_accuracy: 0.6670 - val_loss: 5.1102 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 335/1000 - train_loss: 4.9804 - train_accuracy: 0.6673 - val_loss: 5.1101 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 336/1000 - train_loss: 4.9794 - train_accuracy: 0.6673 - val_loss: 5.1101 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 337/1000 - train_loss: 4.9785 - train_accuracy: 0.6678 - val_loss: 5.1100 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 338/1000 - train_loss: 4.9776 - train_accuracy: 0.6675 - val_loss: 5.1100 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 339/1000 - train_loss: 4.9767 - train_accuracy: 0.6681 - val_loss: 5.1099 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 340/1000 - train_loss: 4.9759 - train_accuracy: 0.6687 - val_loss: 5.1098 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 341/1000 - train_loss: 4.9750 - train_accuracy: 0.6686 - val_loss: 5.1098 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 342/1000 - train_loss: 4.9742 - train_accuracy: 0.6681 - val_loss: 5.1097 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 343/1000 - train_loss: 4.9733 - train_accuracy: 0.6681 - val_loss: 5.1096 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 344/1000 - train_loss: 4.9725 - train_accuracy: 0.6683 - val_loss: 5.1096 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 345/1000 - train_loss: 4.9717 - train_accuracy: 0.6684 - val_loss: 5.1095 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 346/1000 - train_loss: 4.9709 - train_accuracy: 0.6689 - val_loss: 5.1094 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 347/1000 - train_loss: 4.9701 - train_accuracy: 0.6689 - val_loss: 5.1093 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 348/1000 - train_loss: 4.9693 - train_accuracy: 0.6687 - val_loss: 5.1092 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 349/1000 - train_loss: 4.9685 - train_accuracy: 0.6692 - val_loss: 5.1091 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 350/1000 - train_loss: 4.9678 - train_accuracy: 0.6694 - val_loss: 5.1090 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 351/1000 - train_loss: 4.9670 - train_accuracy: 0.6695 - val_loss: 5.1089 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 352/1000 - train_loss: 4.9663 - train_accuracy: 0.6697 - val_loss: 5.1087 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 353/1000 - train_loss: 4.9655 - train_accuracy: 0.6698 - val_loss: 5.1086 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 354/1000 - train_loss: 4.9648 - train_accuracy: 0.6702 - val_loss: 5.1085 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 355/1000 - train_loss: 4.9641 - train_accuracy: 0.6703 - val_loss: 5.1083 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 356/1000 - train_loss: 4.9634 - train_accuracy: 0.6702 - val_loss: 5.1082 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 357/1000 - train_loss: 4.9627 - train_accuracy: 0.6700 - val_loss: 5.1080 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 358/1000 - train_loss: 4.9620 - train_accuracy: 0.6703 - val_loss: 5.1079 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 359/1000 - train_loss: 4.9613 - train_accuracy: 0.6705 - val_loss: 5.1077 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 360/1000 - train_loss: 4.9607 - train_accuracy: 0.6705 - val_loss: 5.1075 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 361/1000 - train_loss: 4.9600 - train_accuracy: 0.6705 - val_loss: 5.1074 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 362/1000 - train_loss: 4.9593 - train_accuracy: 0.6702 - val_loss: 5.1072 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 363/1000 - train_loss: 4.9587 - train_accuracy: 0.6705 - val_loss: 5.1070 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 364/1000 - train_loss: 4.9580 - train_accuracy: 0.6706 - val_loss: 5.1068 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 365/1000 - train_loss: 4.9574 - train_accuracy: 0.6702 - val_loss: 5.1066 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 366/1000 - train_loss: 4.9567 - train_accuracy: 0.6703 - val_loss: 5.1065 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 367/1000 - train_loss: 4.9561 - train_accuracy: 0.6700 - val_loss: 5.1063 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 368/1000 - train_loss: 4.9554 - train_accuracy: 0.6695 - val_loss: 5.1061 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 369/1000 - train_loss: 4.9548 - train_accuracy: 0.6695 - val_loss: 5.1059 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 370/1000 - train_loss: 4.9542 - train_accuracy: 0.6695 - val_loss: 5.1057 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 371/1000 - train_loss: 4.9535 - train_accuracy: 0.6697 - val_loss: 5.1055 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 372/1000 - train_loss: 4.9529 - train_accuracy: 0.6700 - val_loss: 5.1053 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 373/1000 - train_loss: 4.9523 - train_accuracy: 0.6700 - val_loss: 5.1051 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 374/1000 - train_loss: 4.9517 - train_accuracy: 0.6702 - val_loss: 5.1049 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 375/1000 - train_loss: 4.9510 - train_accuracy: 0.6700 - val_loss: 5.1047 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 376/1000 - train_loss: 4.9504 - train_accuracy: 0.6697 - val_loss: 5.1045 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 377/1000 - train_loss: 4.9498 - train_accuracy: 0.6697 - val_loss: 5.1043 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 378/1000 - train_loss: 4.9492 - train_accuracy: 0.6694 - val_loss: 5.1041 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 379/1000 - train_loss: 4.9486 - train_accuracy: 0.6687 - val_loss: 5.1039 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 380/1000 - train_loss: 4.9480 - train_accuracy: 0.6689 - val_loss: 5.1037 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 381/1000 - train_loss: 4.9473 - train_accuracy: 0.6689 - val_loss: 5.1035 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 382/1000 - train_loss: 4.9467 - train_accuracy: 0.6689 - val_loss: 5.1033 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 383/1000 - train_loss: 4.9461 - train_accuracy: 0.6692 - val_loss: 5.1031 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 384/1000 - train_loss: 4.9455 - train_accuracy: 0.6695 - val_loss: 5.1029 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 385/1000 - train_loss: 4.9449 - train_accuracy: 0.6695 - val_loss: 5.1027 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 386/1000 - train_loss: 4.9443 - train_accuracy: 0.6702 - val_loss: 5.1025 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 387/1000 - train_loss: 4.9437 - train_accuracy: 0.6698 - val_loss: 5.1023 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 388/1000 - train_loss: 4.9431 - train_accuracy: 0.6695 - val_loss: 5.1022 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 389/1000 - train_loss: 4.9425 - train_accuracy: 0.6695 - val_loss: 5.1020 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 390/1000 - train_loss: 4.9419 - train_accuracy: 0.6694 - val_loss: 5.1018 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 391/1000 - train_loss: 4.9413 - train_accuracy: 0.6694 - val_loss: 5.1017 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 392/1000 - train_loss: 4.9407 - train_accuracy: 0.6695 - val_loss: 5.1015 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 393/1000 - train_loss: 4.9401 - train_accuracy: 0.6697 - val_loss: 5.1013 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 394/1000 - train_loss: 4.9394 - train_accuracy: 0.6700 - val_loss: 5.1012 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 395/1000 - train_loss: 4.9388 - train_accuracy: 0.6700 - val_loss: 5.1011 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 396/1000 - train_loss: 4.9383 - train_accuracy: 0.6698 - val_loss: 5.1009 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 397/1000 - train_loss: 4.9377 - train_accuracy: 0.6697 - val_loss: 5.1008 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 398/1000 - train_loss: 4.9371 - train_accuracy: 0.6700 - val_loss: 5.1006 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 399/1000 - train_loss: 4.9365 - train_accuracy: 0.6702 - val_loss: 5.1005 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 400/1000 - train_loss: 4.9359 - train_accuracy: 0.6702 - val_loss: 5.1004 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 401/1000 - train_loss: 4.9353 - train_accuracy: 0.6703 - val_loss: 5.1003 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 402/1000 - train_loss: 4.9347 - train_accuracy: 0.6703 - val_loss: 5.1002 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 403/1000 - train_loss: 4.9341 - train_accuracy: 0.6703 - val_loss: 5.1001 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 404/1000 - train_loss: 4.9335 - train_accuracy: 0.6703 - val_loss: 5.1000 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 405/1000 - train_loss: 4.9329 - train_accuracy: 0.6702 - val_loss: 5.0999 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 406/1000 - train_loss: 4.9324 - train_accuracy: 0.6705 - val_loss: 5.0998 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 407/1000 - train_loss: 4.9318 - train_accuracy: 0.6703 - val_loss: 5.0997 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 408/1000 - train_loss: 4.9312 - train_accuracy: 0.6705 - val_loss: 5.0997 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 409/1000 - train_loss: 4.9306 - train_accuracy: 0.6705 - val_loss: 5.0996 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 410/1000 - train_loss: 4.9301 - train_accuracy: 0.6706 - val_loss: 5.0996 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 411/1000 - train_loss: 4.9295 - train_accuracy: 0.6708 - val_loss: 5.0995 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 412/1000 - train_loss: 4.9290 - train_accuracy: 0.6708 - val_loss: 5.0995 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 413/1000 - train_loss: 4.9284 - train_accuracy: 0.6713 - val_loss: 5.0995 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 414/1000 - train_loss: 4.9279 - train_accuracy: 0.6714 - val_loss: 5.0995 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 415/1000 - train_loss: 4.9273 - train_accuracy: 0.6716 - val_loss: 5.0995 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 416/1000 - train_loss: 4.9268 - train_accuracy: 0.6719 - val_loss: 5.0995 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 417/1000 - train_loss: 4.9262 - train_accuracy: 0.6719 - val_loss: 5.0995 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 418/1000 - train_loss: 4.9257 - train_accuracy: 0.6725 - val_loss: 5.0995 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 419/1000 - train_loss: 4.9252 - train_accuracy: 0.6727 - val_loss: 5.0996 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 420/1000 - train_loss: 4.9246 - train_accuracy: 0.6728 - val_loss: 5.0996 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 421/1000 - train_loss: 4.9241 - train_accuracy: 0.6730 - val_loss: 5.0997 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 422/1000 - train_loss: 4.9236 - train_accuracy: 0.6730 - val_loss: 5.0997 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 423/1000 - train_loss: 4.9231 - train_accuracy: 0.6731 - val_loss: 5.0998 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 424/1000 - train_loss: 4.9226 - train_accuracy: 0.6734 - val_loss: 5.0999 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 425/1000 - train_loss: 4.9221 - train_accuracy: 0.6736 - val_loss: 5.1000 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 426/1000 - train_loss: 4.9216 - train_accuracy: 0.6739 - val_loss: 5.1001 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 427/1000 - train_loss: 4.9211 - train_accuracy: 0.6739 - val_loss: 5.1002 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 428/1000 - train_loss: 4.9206 - train_accuracy: 0.6737 - val_loss: 5.1003 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 429/1000 - train_loss: 4.9201 - train_accuracy: 0.6734 - val_loss: 5.1005 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 430/1000 - train_loss: 4.9196 - train_accuracy: 0.6741 - val_loss: 5.1006 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 431/1000 - train_loss: 4.9191 - train_accuracy: 0.6742 - val_loss: 5.1008 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 432/1000 - train_loss: 4.9187 - train_accuracy: 0.6747 - val_loss: 5.1009 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 433/1000 - train_loss: 4.9182 - train_accuracy: 0.6745 - val_loss: 5.1011 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 434/1000 - train_loss: 4.9178 - train_accuracy: 0.6747 - val_loss: 5.1013 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 435/1000 - train_loss: 4.9173 - train_accuracy: 0.6750 - val_loss: 5.1014 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 436/1000 - train_loss: 4.9169 - train_accuracy: 0.6748 - val_loss: 5.1016 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 437/1000 - train_loss: 4.9164 - train_accuracy: 0.6750 - val_loss: 5.1018 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 438/1000 - train_loss: 4.9160 - train_accuracy: 0.6752 - val_loss: 5.1020 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 439/1000 - train_loss: 4.9155 - train_accuracy: 0.6753 - val_loss: 5.1022 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 440/1000 - train_loss: 4.9151 - train_accuracy: 0.6752 - val_loss: 5.1024 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 441/1000 - train_loss: 4.9147 - train_accuracy: 0.6753 - val_loss: 5.1026 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 442/1000 - train_loss: 4.9143 - train_accuracy: 0.6756 - val_loss: 5.1029 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 443/1000 - train_loss: 4.9139 - train_accuracy: 0.6755 - val_loss: 5.1031 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 444/1000 - train_loss: 4.9135 - train_accuracy: 0.6750 - val_loss: 5.1033 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 445/1000 - train_loss: 4.9131 - train_accuracy: 0.6752 - val_loss: 5.1035 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 446/1000 - train_loss: 4.9127 - train_accuracy: 0.6752 - val_loss: 5.1038 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 447/1000 - train_loss: 4.9123 - train_accuracy: 0.6753 - val_loss: 5.1040 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 448/1000 - train_loss: 4.9119 - train_accuracy: 0.6753 - val_loss: 5.1043 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 449/1000 - train_loss: 4.9115 - train_accuracy: 0.6752 - val_loss: 5.1045 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 450/1000 - train_loss: 4.9111 - train_accuracy: 0.6753 - val_loss: 5.1048 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 451/1000 - train_loss: 4.9107 - train_accuracy: 0.6750 - val_loss: 5.1050 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 452/1000 - train_loss: 4.9104 - train_accuracy: 0.6747 - val_loss: 5.1053 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 453/1000 - train_loss: 4.9100 - train_accuracy: 0.6747 - val_loss: 5.1056 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 454/1000 - train_loss: 4.9096 - train_accuracy: 0.6745 - val_loss: 5.1058 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 455/1000 - train_loss: 4.9093 - train_accuracy: 0.6747 - val_loss: 5.1061 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 456/1000 - train_loss: 4.9089 - train_accuracy: 0.6750 - val_loss: 5.1064 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 457/1000 - train_loss: 4.9086 - train_accuracy: 0.6747 - val_loss: 5.1067 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 458/1000 - train_loss: 4.9082 - train_accuracy: 0.6748 - val_loss: 5.1069 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 459/1000 - train_loss: 4.9079 - train_accuracy: 0.6744 - val_loss: 5.1072 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 460/1000 - train_loss: 4.9076 - train_accuracy: 0.6745 - val_loss: 5.1075 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 461/1000 - train_loss: 4.9072 - train_accuracy: 0.6748 - val_loss: 5.1078 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 462/1000 - train_loss: 4.9069 - train_accuracy: 0.6747 - val_loss: 5.1081 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 463/1000 - train_loss: 4.9066 - train_accuracy: 0.6744 - val_loss: 5.1084 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 464/1000 - train_loss: 4.9062 - train_accuracy: 0.6744 - val_loss: 5.1087 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 465/1000 - train_loss: 4.9059 - train_accuracy: 0.6744 - val_loss: 5.1090 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 466/1000 - train_loss: 4.9056 - train_accuracy: 0.6744 - val_loss: 5.1093 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 467/1000 - train_loss: 4.9053 - train_accuracy: 0.6744 - val_loss: 5.1096 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 468/1000 - train_loss: 4.9049 - train_accuracy: 0.6745 - val_loss: 5.1100 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 469/1000 - train_loss: 4.9046 - train_accuracy: 0.6747 - val_loss: 5.1103 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 470/1000 - train_loss: 4.9043 - train_accuracy: 0.6744 - val_loss: 5.1106 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 471/1000 - train_loss: 4.9040 - train_accuracy: 0.6744 - val_loss: 5.1109 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 472/1000 - train_loss: 4.9037 - train_accuracy: 0.6745 - val_loss: 5.1112 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 473/1000 - train_loss: 4.9034 - train_accuracy: 0.6745 - val_loss: 5.1116 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 474/1000 - train_loss: 4.9031 - train_accuracy: 0.6747 - val_loss: 5.1119 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 475/1000 - train_loss: 4.9028 - train_accuracy: 0.6747 - val_loss: 5.1122 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 476/1000 - train_loss: 4.9025 - train_accuracy: 0.6747 - val_loss: 5.1126 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 477/1000 - train_loss: 4.9022 - train_accuracy: 0.6748 - val_loss: 5.1129 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 478/1000 - train_loss: 4.9019 - train_accuracy: 0.6750 - val_loss: 5.1132 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 479/1000 - train_loss: 4.9016 - train_accuracy: 0.6750 - val_loss: 5.1136 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 480/1000 - train_loss: 4.9013 - train_accuracy: 0.6750 - val_loss: 5.1139 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 481/1000 - train_loss: 4.9010 - train_accuracy: 0.6752 - val_loss: 5.1143 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 482/1000 - train_loss: 4.9007 - train_accuracy: 0.6752 - val_loss: 5.1146 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 483/1000 - train_loss: 4.9004 - train_accuracy: 0.6750 - val_loss: 5.1150 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 484/1000 - train_loss: 4.9001 - train_accuracy: 0.6750 - val_loss: 5.1153 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 485/1000 - train_loss: 4.8999 - train_accuracy: 0.6752 - val_loss: 5.1157 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 486/1000 - train_loss: 4.8996 - train_accuracy: 0.6752 - val_loss: 5.1160 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 487/1000 - train_loss: 4.8993 - train_accuracy: 0.6752 - val_loss: 5.1164 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 488/1000 - train_loss: 4.8990 - train_accuracy: 0.6752 - val_loss: 5.1167 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 489/1000 - train_loss: 4.8987 - train_accuracy: 0.6750 - val_loss: 5.1171 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 490/1000 - train_loss: 4.8985 - train_accuracy: 0.6752 - val_loss: 5.1175 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 491/1000 - train_loss: 4.8982 - train_accuracy: 0.6752 - val_loss: 5.1178 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 492/1000 - train_loss: 4.8979 - train_accuracy: 0.6753 - val_loss: 5.1182 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 493/1000 - train_loss: 4.8977 - train_accuracy: 0.6755 - val_loss: 5.1186 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 494/1000 - train_loss: 4.8974 - train_accuracy: 0.6755 - val_loss: 5.1189 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 495/1000 - train_loss: 4.8971 - train_accuracy: 0.6756 - val_loss: 5.1193 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 496/1000 - train_loss: 4.8969 - train_accuracy: 0.6756 - val_loss: 5.1197 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 497/1000 - train_loss: 4.8966 - train_accuracy: 0.6755 - val_loss: 5.1200 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 498/1000 - train_loss: 4.8963 - train_accuracy: 0.6755 - val_loss: 5.1204 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 499/1000 - train_loss: 4.8961 - train_accuracy: 0.6756 - val_loss: 5.1208 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 500/1000 - train_loss: 4.8958 - train_accuracy: 0.6758 - val_loss: 5.1211 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 501/1000 - train_loss: 4.8956 - train_accuracy: 0.6755 - val_loss: 5.1215 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 502/1000 - train_loss: 4.8953 - train_accuracy: 0.6756 - val_loss: 5.1219 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 503/1000 - train_loss: 4.8951 - train_accuracy: 0.6758 - val_loss: 5.1222 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 504/1000 - train_loss: 4.8948 - train_accuracy: 0.6758 - val_loss: 5.1226 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 505/1000 - train_loss: 4.8946 - train_accuracy: 0.6758 - val_loss: 5.1230 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 506/1000 - train_loss: 4.8943 - train_accuracy: 0.6758 - val_loss: 5.1233 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 507/1000 - train_loss: 4.8941 - train_accuracy: 0.6758 - val_loss: 5.1237 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 508/1000 - train_loss: 4.8938 - train_accuracy: 0.6758 - val_loss: 5.1240 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 509/1000 - train_loss: 4.8936 - train_accuracy: 0.6756 - val_loss: 5.1244 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 510/1000 - train_loss: 4.8933 - train_accuracy: 0.6758 - val_loss: 5.1248 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 511/1000 - train_loss: 4.8931 - train_accuracy: 0.6758 - val_loss: 5.1251 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 512/1000 - train_loss: 4.8929 - train_accuracy: 0.6761 - val_loss: 5.1255 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 513/1000 - train_loss: 4.8926 - train_accuracy: 0.6761 - val_loss: 5.1258 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 514/1000 - train_loss: 4.8924 - train_accuracy: 0.6763 - val_loss: 5.1262 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 515/1000 - train_loss: 4.8922 - train_accuracy: 0.6763 - val_loss: 5.1266 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 516/1000 - train_loss: 4.8919 - train_accuracy: 0.6764 - val_loss: 5.1269 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 517/1000 - train_loss: 4.8917 - train_accuracy: 0.6766 - val_loss: 5.1273 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 518/1000 - train_loss: 4.8915 - train_accuracy: 0.6767 - val_loss: 5.1276 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 519/1000 - train_loss: 4.8912 - train_accuracy: 0.6769 - val_loss: 5.1280 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 520/1000 - train_loss: 4.8910 - train_accuracy: 0.6767 - val_loss: 5.1283 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 521/1000 - train_loss: 4.8908 - train_accuracy: 0.6770 - val_loss: 5.1287 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 522/1000 - train_loss: 4.8905 - train_accuracy: 0.6773 - val_loss: 5.1290 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 523/1000 - train_loss: 4.8903 - train_accuracy: 0.6773 - val_loss: 5.1293 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 524/1000 - train_loss: 4.8901 - train_accuracy: 0.6775 - val_loss: 5.1297 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 525/1000 - train_loss: 4.8899 - train_accuracy: 0.6775 - val_loss: 5.1300 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 526/1000 - train_loss: 4.8896 - train_accuracy: 0.6775 - val_loss: 5.1303 - val_accuracy: 0.6583\n",
      "(800, 8) (800, 8)\n",
      "Epoch 527/1000 - train_loss: 4.8894 - train_accuracy: 0.6775 - val_loss: 5.1307 - val_accuracy: 0.6583\n",
      "(800, 8) (800, 8)\n",
      "Epoch 528/1000 - train_loss: 4.8892 - train_accuracy: 0.6777 - val_loss: 5.1310 - val_accuracy: 0.6583\n",
      "(800, 8) (800, 8)\n",
      "Epoch 529/1000 - train_loss: 4.8890 - train_accuracy: 0.6781 - val_loss: 5.1313 - val_accuracy: 0.6583\n",
      "(800, 8) (800, 8)\n",
      "Epoch 530/1000 - train_loss: 4.8887 - train_accuracy: 0.6783 - val_loss: 5.1316 - val_accuracy: 0.6583\n",
      "(800, 8) (800, 8)\n",
      "Epoch 531/1000 - train_loss: 4.8885 - train_accuracy: 0.6786 - val_loss: 5.1320 - val_accuracy: 0.6583\n",
      "(800, 8) (800, 8)\n",
      "Epoch 532/1000 - train_loss: 4.8883 - train_accuracy: 0.6786 - val_loss: 5.1323 - val_accuracy: 0.6583\n",
      "(800, 8) (800, 8)\n",
      "Epoch 533/1000 - train_loss: 4.8881 - train_accuracy: 0.6787 - val_loss: 5.1326 - val_accuracy: 0.6583\n",
      "(800, 8) (800, 8)\n",
      "Epoch 534/1000 - train_loss: 4.8878 - train_accuracy: 0.6787 - val_loss: 5.1329 - val_accuracy: 0.6583\n",
      "(800, 8) (800, 8)\n",
      "Epoch 535/1000 - train_loss: 4.8876 - train_accuracy: 0.6784 - val_loss: 5.1332 - val_accuracy: 0.6583\n",
      "(800, 8) (800, 8)\n",
      "Epoch 536/1000 - train_loss: 4.8874 - train_accuracy: 0.6784 - val_loss: 5.1335 - val_accuracy: 0.6583\n",
      "(800, 8) (800, 8)\n",
      "Epoch 537/1000 - train_loss: 4.8872 - train_accuracy: 0.6781 - val_loss: 5.1338 - val_accuracy: 0.6583\n",
      "(800, 8) (800, 8)\n",
      "Epoch 538/1000 - train_loss: 4.8870 - train_accuracy: 0.6781 - val_loss: 5.1341 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 539/1000 - train_loss: 4.8867 - train_accuracy: 0.6781 - val_loss: 5.1344 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 540/1000 - train_loss: 4.8865 - train_accuracy: 0.6780 - val_loss: 5.1347 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 541/1000 - train_loss: 4.8863 - train_accuracy: 0.6780 - val_loss: 5.1349 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 542/1000 - train_loss: 4.8861 - train_accuracy: 0.6784 - val_loss: 5.1352 - val_accuracy: 0.6575\n",
      "(800, 8) (800, 8)\n",
      "Epoch 543/1000 - train_loss: 4.8859 - train_accuracy: 0.6781 - val_loss: 5.1355 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 544/1000 - train_loss: 4.8857 - train_accuracy: 0.6781 - val_loss: 5.1358 - val_accuracy: 0.6567\n",
      "(800, 8) (800, 8)\n",
      "Epoch 545/1000 - train_loss: 4.8854 - train_accuracy: 0.6781 - val_loss: 5.1360 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 546/1000 - train_loss: 4.8852 - train_accuracy: 0.6781 - val_loss: 5.1363 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 547/1000 - train_loss: 4.8850 - train_accuracy: 0.6781 - val_loss: 5.1365 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 548/1000 - train_loss: 4.8848 - train_accuracy: 0.6780 - val_loss: 5.1368 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 549/1000 - train_loss: 4.8846 - train_accuracy: 0.6781 - val_loss: 5.1370 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 550/1000 - train_loss: 4.8844 - train_accuracy: 0.6781 - val_loss: 5.1373 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 551/1000 - train_loss: 4.8841 - train_accuracy: 0.6778 - val_loss: 5.1375 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 552/1000 - train_loss: 4.8839 - train_accuracy: 0.6775 - val_loss: 5.1378 - val_accuracy: 0.6558\n",
      "(800, 8) (800, 8)\n",
      "Epoch 553/1000 - train_loss: 4.8837 - train_accuracy: 0.6773 - val_loss: 5.1380 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 554/1000 - train_loss: 4.8835 - train_accuracy: 0.6773 - val_loss: 5.1382 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 555/1000 - train_loss: 4.8833 - train_accuracy: 0.6777 - val_loss: 5.1385 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 556/1000 - train_loss: 4.8831 - train_accuracy: 0.6775 - val_loss: 5.1387 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 557/1000 - train_loss: 4.8829 - train_accuracy: 0.6775 - val_loss: 5.1389 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 558/1000 - train_loss: 4.8827 - train_accuracy: 0.6775 - val_loss: 5.1391 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 559/1000 - train_loss: 4.8825 - train_accuracy: 0.6778 - val_loss: 5.1394 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 560/1000 - train_loss: 4.8822 - train_accuracy: 0.6773 - val_loss: 5.1396 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 561/1000 - train_loss: 4.8820 - train_accuracy: 0.6773 - val_loss: 5.1398 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 562/1000 - train_loss: 4.8818 - train_accuracy: 0.6773 - val_loss: 5.1400 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 563/1000 - train_loss: 4.8816 - train_accuracy: 0.6773 - val_loss: 5.1402 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 564/1000 - train_loss: 4.8814 - train_accuracy: 0.6773 - val_loss: 5.1404 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 565/1000 - train_loss: 4.8812 - train_accuracy: 0.6772 - val_loss: 5.1406 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 566/1000 - train_loss: 4.8810 - train_accuracy: 0.6773 - val_loss: 5.1408 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 567/1000 - train_loss: 4.8808 - train_accuracy: 0.6775 - val_loss: 5.1411 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 568/1000 - train_loss: 4.8806 - train_accuracy: 0.6780 - val_loss: 5.1413 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 569/1000 - train_loss: 4.8804 - train_accuracy: 0.6783 - val_loss: 5.1415 - val_accuracy: 0.6550\n",
      "(800, 8) (800, 8)\n",
      "Epoch 570/1000 - train_loss: 4.8802 - train_accuracy: 0.6784 - val_loss: 5.1417 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 571/1000 - train_loss: 4.8800 - train_accuracy: 0.6786 - val_loss: 5.1419 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 572/1000 - train_loss: 4.8798 - train_accuracy: 0.6787 - val_loss: 5.1421 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 573/1000 - train_loss: 4.8796 - train_accuracy: 0.6789 - val_loss: 5.1423 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 574/1000 - train_loss: 4.8794 - train_accuracy: 0.6792 - val_loss: 5.1425 - val_accuracy: 0.6542\n",
      "(800, 8) (800, 8)\n",
      "Epoch 575/1000 - train_loss: 4.8792 - train_accuracy: 0.6792 - val_loss: 5.1427 - val_accuracy: 0.6533\n",
      "(800, 8) (800, 8)\n",
      "Epoch 576/1000 - train_loss: 4.8790 - train_accuracy: 0.6794 - val_loss: 5.1429 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 577/1000 - train_loss: 4.8788 - train_accuracy: 0.6794 - val_loss: 5.1431 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 578/1000 - train_loss: 4.8786 - train_accuracy: 0.6792 - val_loss: 5.1433 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 579/1000 - train_loss: 4.8784 - train_accuracy: 0.6792 - val_loss: 5.1435 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 580/1000 - train_loss: 4.8782 - train_accuracy: 0.6794 - val_loss: 5.1437 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 581/1000 - train_loss: 4.8780 - train_accuracy: 0.6797 - val_loss: 5.1439 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 582/1000 - train_loss: 4.8778 - train_accuracy: 0.6797 - val_loss: 5.1441 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 583/1000 - train_loss: 4.8776 - train_accuracy: 0.6798 - val_loss: 5.1443 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 584/1000 - train_loss: 4.8774 - train_accuracy: 0.6800 - val_loss: 5.1445 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 585/1000 - train_loss: 4.8772 - train_accuracy: 0.6802 - val_loss: 5.1447 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 586/1000 - train_loss: 4.8770 - train_accuracy: 0.6802 - val_loss: 5.1449 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 587/1000 - train_loss: 4.8768 - train_accuracy: 0.6805 - val_loss: 5.1451 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 588/1000 - train_loss: 4.8766 - train_accuracy: 0.6805 - val_loss: 5.1453 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 589/1000 - train_loss: 4.8764 - train_accuracy: 0.6805 - val_loss: 5.1455 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 590/1000 - train_loss: 4.8763 - train_accuracy: 0.6805 - val_loss: 5.1457 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 591/1000 - train_loss: 4.8761 - train_accuracy: 0.6805 - val_loss: 5.1459 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 592/1000 - train_loss: 4.8759 - train_accuracy: 0.6805 - val_loss: 5.1461 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 593/1000 - train_loss: 4.8757 - train_accuracy: 0.6802 - val_loss: 5.1463 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 594/1000 - train_loss: 4.8755 - train_accuracy: 0.6800 - val_loss: 5.1465 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 595/1000 - train_loss: 4.8753 - train_accuracy: 0.6800 - val_loss: 5.1467 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 596/1000 - train_loss: 4.8751 - train_accuracy: 0.6800 - val_loss: 5.1469 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 597/1000 - train_loss: 4.8750 - train_accuracy: 0.6800 - val_loss: 5.1471 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 598/1000 - train_loss: 4.8748 - train_accuracy: 0.6797 - val_loss: 5.1473 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 599/1000 - train_loss: 4.8746 - train_accuracy: 0.6798 - val_loss: 5.1475 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 600/1000 - train_loss: 4.8744 - train_accuracy: 0.6800 - val_loss: 5.1477 - val_accuracy: 0.6525\n",
      "(800, 8) (800, 8)\n",
      "Epoch 601/1000 - train_loss: 4.8742 - train_accuracy: 0.6800 - val_loss: 5.1479 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 602/1000 - train_loss: 4.8740 - train_accuracy: 0.6800 - val_loss: 5.1481 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 603/1000 - train_loss: 4.8739 - train_accuracy: 0.6802 - val_loss: 5.1483 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 604/1000 - train_loss: 4.8737 - train_accuracy: 0.6803 - val_loss: 5.1485 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 605/1000 - train_loss: 4.8735 - train_accuracy: 0.6803 - val_loss: 5.1487 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 606/1000 - train_loss: 4.8733 - train_accuracy: 0.6806 - val_loss: 5.1489 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 607/1000 - train_loss: 4.8731 - train_accuracy: 0.6803 - val_loss: 5.1491 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 608/1000 - train_loss: 4.8730 - train_accuracy: 0.6802 - val_loss: 5.1493 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 609/1000 - train_loss: 4.8728 - train_accuracy: 0.6800 - val_loss: 5.1494 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 610/1000 - train_loss: 4.8726 - train_accuracy: 0.6800 - val_loss: 5.1496 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 611/1000 - train_loss: 4.8724 - train_accuracy: 0.6803 - val_loss: 5.1498 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 612/1000 - train_loss: 4.8722 - train_accuracy: 0.6802 - val_loss: 5.1500 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 613/1000 - train_loss: 4.8721 - train_accuracy: 0.6802 - val_loss: 5.1502 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 614/1000 - train_loss: 4.8719 - train_accuracy: 0.6802 - val_loss: 5.1504 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 615/1000 - train_loss: 4.8717 - train_accuracy: 0.6800 - val_loss: 5.1505 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 616/1000 - train_loss: 4.8715 - train_accuracy: 0.6800 - val_loss: 5.1507 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 617/1000 - train_loss: 4.8714 - train_accuracy: 0.6798 - val_loss: 5.1509 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 618/1000 - train_loss: 4.8712 - train_accuracy: 0.6798 - val_loss: 5.1511 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 619/1000 - train_loss: 4.8710 - train_accuracy: 0.6797 - val_loss: 5.1512 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 620/1000 - train_loss: 4.8708 - train_accuracy: 0.6797 - val_loss: 5.1514 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 621/1000 - train_loss: 4.8707 - train_accuracy: 0.6794 - val_loss: 5.1516 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 622/1000 - train_loss: 4.8705 - train_accuracy: 0.6792 - val_loss: 5.1517 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 623/1000 - train_loss: 4.8703 - train_accuracy: 0.6791 - val_loss: 5.1519 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 624/1000 - train_loss: 4.8701 - train_accuracy: 0.6792 - val_loss: 5.1520 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 625/1000 - train_loss: 4.8700 - train_accuracy: 0.6792 - val_loss: 5.1522 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 626/1000 - train_loss: 4.8698 - train_accuracy: 0.6791 - val_loss: 5.1524 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 627/1000 - train_loss: 4.8696 - train_accuracy: 0.6787 - val_loss: 5.1525 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 628/1000 - train_loss: 4.8694 - train_accuracy: 0.6787 - val_loss: 5.1527 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 629/1000 - train_loss: 4.8693 - train_accuracy: 0.6787 - val_loss: 5.1528 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 630/1000 - train_loss: 4.8691 - train_accuracy: 0.6787 - val_loss: 5.1530 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 631/1000 - train_loss: 4.8689 - train_accuracy: 0.6791 - val_loss: 5.1531 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 632/1000 - train_loss: 4.8687 - train_accuracy: 0.6791 - val_loss: 5.1532 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 633/1000 - train_loss: 4.8686 - train_accuracy: 0.6792 - val_loss: 5.1534 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 634/1000 - train_loss: 4.8684 - train_accuracy: 0.6792 - val_loss: 5.1535 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 635/1000 - train_loss: 4.8682 - train_accuracy: 0.6792 - val_loss: 5.1537 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 636/1000 - train_loss: 4.8680 - train_accuracy: 0.6794 - val_loss: 5.1538 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 637/1000 - train_loss: 4.8678 - train_accuracy: 0.6794 - val_loss: 5.1539 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 638/1000 - train_loss: 4.8677 - train_accuracy: 0.6792 - val_loss: 5.1541 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 639/1000 - train_loss: 4.8675 - train_accuracy: 0.6791 - val_loss: 5.1542 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 640/1000 - train_loss: 4.8673 - train_accuracy: 0.6787 - val_loss: 5.1543 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 641/1000 - train_loss: 4.8671 - train_accuracy: 0.6786 - val_loss: 5.1544 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 642/1000 - train_loss: 4.8670 - train_accuracy: 0.6787 - val_loss: 5.1546 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 643/1000 - train_loss: 4.8668 - train_accuracy: 0.6784 - val_loss: 5.1547 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 644/1000 - train_loss: 4.8666 - train_accuracy: 0.6784 - val_loss: 5.1548 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 645/1000 - train_loss: 4.8664 - train_accuracy: 0.6783 - val_loss: 5.1549 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 646/1000 - train_loss: 4.8662 - train_accuracy: 0.6781 - val_loss: 5.1550 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 647/1000 - train_loss: 4.8661 - train_accuracy: 0.6781 - val_loss: 5.1551 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 648/1000 - train_loss: 4.8659 - train_accuracy: 0.6780 - val_loss: 5.1553 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 649/1000 - train_loss: 4.8657 - train_accuracy: 0.6780 - val_loss: 5.1554 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 650/1000 - train_loss: 4.8655 - train_accuracy: 0.6780 - val_loss: 5.1555 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 651/1000 - train_loss: 4.8653 - train_accuracy: 0.6780 - val_loss: 5.1556 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 652/1000 - train_loss: 4.8651 - train_accuracy: 0.6780 - val_loss: 5.1557 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 653/1000 - train_loss: 4.8649 - train_accuracy: 0.6781 - val_loss: 5.1558 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 654/1000 - train_loss: 4.8648 - train_accuracy: 0.6780 - val_loss: 5.1559 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 655/1000 - train_loss: 4.8646 - train_accuracy: 0.6780 - val_loss: 5.1560 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 656/1000 - train_loss: 4.8644 - train_accuracy: 0.6778 - val_loss: 5.1561 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 657/1000 - train_loss: 4.8642 - train_accuracy: 0.6780 - val_loss: 5.1562 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 658/1000 - train_loss: 4.8640 - train_accuracy: 0.6781 - val_loss: 5.1563 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 659/1000 - train_loss: 4.8638 - train_accuracy: 0.6783 - val_loss: 5.1564 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 660/1000 - train_loss: 4.8636 - train_accuracy: 0.6783 - val_loss: 5.1565 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 661/1000 - train_loss: 4.8634 - train_accuracy: 0.6784 - val_loss: 5.1566 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 662/1000 - train_loss: 4.8632 - train_accuracy: 0.6784 - val_loss: 5.1568 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 663/1000 - train_loss: 4.8630 - train_accuracy: 0.6787 - val_loss: 5.1569 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 664/1000 - train_loss: 4.8628 - train_accuracy: 0.6787 - val_loss: 5.1570 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 665/1000 - train_loss: 4.8626 - train_accuracy: 0.6789 - val_loss: 5.1571 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 666/1000 - train_loss: 4.8624 - train_accuracy: 0.6789 - val_loss: 5.1572 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 667/1000 - train_loss: 4.8622 - train_accuracy: 0.6789 - val_loss: 5.1573 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 668/1000 - train_loss: 4.8620 - train_accuracy: 0.6794 - val_loss: 5.1574 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 669/1000 - train_loss: 4.8618 - train_accuracy: 0.6792 - val_loss: 5.1575 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 670/1000 - train_loss: 4.8616 - train_accuracy: 0.6789 - val_loss: 5.1576 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 671/1000 - train_loss: 4.8614 - train_accuracy: 0.6789 - val_loss: 5.1578 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 672/1000 - train_loss: 4.8612 - train_accuracy: 0.6791 - val_loss: 5.1579 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 673/1000 - train_loss: 4.8610 - train_accuracy: 0.6791 - val_loss: 5.1580 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 674/1000 - train_loss: 4.8608 - train_accuracy: 0.6789 - val_loss: 5.1581 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 675/1000 - train_loss: 4.8606 - train_accuracy: 0.6787 - val_loss: 5.1582 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 676/1000 - train_loss: 4.8604 - train_accuracy: 0.6783 - val_loss: 5.1584 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 677/1000 - train_loss: 4.8602 - train_accuracy: 0.6781 - val_loss: 5.1585 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 678/1000 - train_loss: 4.8600 - train_accuracy: 0.6778 - val_loss: 5.1586 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 679/1000 - train_loss: 4.8598 - train_accuracy: 0.6778 - val_loss: 5.1588 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 680/1000 - train_loss: 4.8596 - train_accuracy: 0.6781 - val_loss: 5.1589 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 681/1000 - train_loss: 4.8594 - train_accuracy: 0.6780 - val_loss: 5.1591 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 682/1000 - train_loss: 4.8592 - train_accuracy: 0.6780 - val_loss: 5.1592 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 683/1000 - train_loss: 4.8589 - train_accuracy: 0.6780 - val_loss: 5.1594 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 684/1000 - train_loss: 4.8587 - train_accuracy: 0.6780 - val_loss: 5.1595 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 685/1000 - train_loss: 4.8585 - train_accuracy: 0.6781 - val_loss: 5.1597 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 686/1000 - train_loss: 4.8583 - train_accuracy: 0.6781 - val_loss: 5.1598 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 687/1000 - train_loss: 4.8581 - train_accuracy: 0.6783 - val_loss: 5.1600 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 688/1000 - train_loss: 4.8579 - train_accuracy: 0.6781 - val_loss: 5.1601 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 689/1000 - train_loss: 4.8577 - train_accuracy: 0.6781 - val_loss: 5.1603 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 690/1000 - train_loss: 4.8575 - train_accuracy: 0.6778 - val_loss: 5.1605 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 691/1000 - train_loss: 4.8572 - train_accuracy: 0.6778 - val_loss: 5.1606 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 692/1000 - train_loss: 4.8570 - train_accuracy: 0.6778 - val_loss: 5.1608 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 693/1000 - train_loss: 4.8568 - train_accuracy: 0.6777 - val_loss: 5.1610 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 694/1000 - train_loss: 4.8566 - train_accuracy: 0.6780 - val_loss: 5.1612 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 695/1000 - train_loss: 4.8564 - train_accuracy: 0.6778 - val_loss: 5.1613 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 696/1000 - train_loss: 4.8562 - train_accuracy: 0.6777 - val_loss: 5.1615 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 697/1000 - train_loss: 4.8559 - train_accuracy: 0.6777 - val_loss: 5.1617 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 698/1000 - train_loss: 4.8557 - train_accuracy: 0.6778 - val_loss: 5.1619 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 699/1000 - train_loss: 4.8555 - train_accuracy: 0.6777 - val_loss: 5.1621 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 700/1000 - train_loss: 4.8553 - train_accuracy: 0.6777 - val_loss: 5.1623 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 701/1000 - train_loss: 4.8551 - train_accuracy: 0.6777 - val_loss: 5.1624 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 702/1000 - train_loss: 4.8549 - train_accuracy: 0.6778 - val_loss: 5.1626 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 703/1000 - train_loss: 4.8547 - train_accuracy: 0.6780 - val_loss: 5.1628 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 704/1000 - train_loss: 4.8544 - train_accuracy: 0.6778 - val_loss: 5.1630 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 705/1000 - train_loss: 4.8542 - train_accuracy: 0.6775 - val_loss: 5.1632 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 706/1000 - train_loss: 4.8540 - train_accuracy: 0.6775 - val_loss: 5.1634 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 707/1000 - train_loss: 4.8538 - train_accuracy: 0.6773 - val_loss: 5.1636 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 708/1000 - train_loss: 4.8536 - train_accuracy: 0.6770 - val_loss: 5.1638 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 709/1000 - train_loss: 4.8533 - train_accuracy: 0.6766 - val_loss: 5.1640 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 710/1000 - train_loss: 4.8531 - train_accuracy: 0.6763 - val_loss: 5.1642 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 711/1000 - train_loss: 4.8529 - train_accuracy: 0.6761 - val_loss: 5.1644 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 712/1000 - train_loss: 4.8527 - train_accuracy: 0.6759 - val_loss: 5.1646 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 713/1000 - train_loss: 4.8525 - train_accuracy: 0.6759 - val_loss: 5.1648 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 714/1000 - train_loss: 4.8523 - train_accuracy: 0.6758 - val_loss: 5.1650 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 715/1000 - train_loss: 4.8520 - train_accuracy: 0.6759 - val_loss: 5.1652 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 716/1000 - train_loss: 4.8518 - train_accuracy: 0.6759 - val_loss: 5.1654 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 717/1000 - train_loss: 4.8516 - train_accuracy: 0.6758 - val_loss: 5.1656 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 718/1000 - train_loss: 4.8514 - train_accuracy: 0.6756 - val_loss: 5.1657 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 719/1000 - train_loss: 4.8512 - train_accuracy: 0.6753 - val_loss: 5.1659 - val_accuracy: 0.6458\n",
      "(800, 8) (800, 8)\n",
      "Epoch 720/1000 - train_loss: 4.8509 - train_accuracy: 0.6752 - val_loss: 5.1661 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 721/1000 - train_loss: 4.8507 - train_accuracy: 0.6748 - val_loss: 5.1663 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 722/1000 - train_loss: 4.8505 - train_accuracy: 0.6748 - val_loss: 5.1665 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 723/1000 - train_loss: 4.8503 - train_accuracy: 0.6745 - val_loss: 5.1667 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 724/1000 - train_loss: 4.8501 - train_accuracy: 0.6742 - val_loss: 5.1669 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 725/1000 - train_loss: 4.8498 - train_accuracy: 0.6745 - val_loss: 5.1671 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 726/1000 - train_loss: 4.8496 - train_accuracy: 0.6748 - val_loss: 5.1673 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 727/1000 - train_loss: 4.8494 - train_accuracy: 0.6745 - val_loss: 5.1675 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 728/1000 - train_loss: 4.8492 - train_accuracy: 0.6745 - val_loss: 5.1677 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 729/1000 - train_loss: 4.8490 - train_accuracy: 0.6748 - val_loss: 5.1679 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 730/1000 - train_loss: 4.8487 - train_accuracy: 0.6750 - val_loss: 5.1681 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 731/1000 - train_loss: 4.8485 - train_accuracy: 0.6748 - val_loss: 5.1683 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 732/1000 - train_loss: 4.8483 - train_accuracy: 0.6748 - val_loss: 5.1685 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 733/1000 - train_loss: 4.8481 - train_accuracy: 0.6745 - val_loss: 5.1687 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 734/1000 - train_loss: 4.8478 - train_accuracy: 0.6745 - val_loss: 5.1689 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 735/1000 - train_loss: 4.8476 - train_accuracy: 0.6742 - val_loss: 5.1691 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 736/1000 - train_loss: 4.8474 - train_accuracy: 0.6744 - val_loss: 5.1693 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 737/1000 - train_loss: 4.8472 - train_accuracy: 0.6741 - val_loss: 5.1695 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 738/1000 - train_loss: 4.8470 - train_accuracy: 0.6744 - val_loss: 5.1696 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 739/1000 - train_loss: 4.8467 - train_accuracy: 0.6747 - val_loss: 5.1698 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 740/1000 - train_loss: 4.8465 - train_accuracy: 0.6747 - val_loss: 5.1700 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 741/1000 - train_loss: 4.8463 - train_accuracy: 0.6745 - val_loss: 5.1702 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 742/1000 - train_loss: 4.8461 - train_accuracy: 0.6747 - val_loss: 5.1704 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 743/1000 - train_loss: 4.8458 - train_accuracy: 0.6748 - val_loss: 5.1706 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 744/1000 - train_loss: 4.8456 - train_accuracy: 0.6750 - val_loss: 5.1708 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 745/1000 - train_loss: 4.8454 - train_accuracy: 0.6752 - val_loss: 5.1709 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 746/1000 - train_loss: 4.8452 - train_accuracy: 0.6748 - val_loss: 5.1711 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 747/1000 - train_loss: 4.8449 - train_accuracy: 0.6745 - val_loss: 5.1713 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 748/1000 - train_loss: 4.8447 - train_accuracy: 0.6742 - val_loss: 5.1715 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 749/1000 - train_loss: 4.8445 - train_accuracy: 0.6742 - val_loss: 5.1717 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 750/1000 - train_loss: 4.8442 - train_accuracy: 0.6744 - val_loss: 5.1718 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 751/1000 - train_loss: 4.8440 - train_accuracy: 0.6745 - val_loss: 5.1720 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 752/1000 - train_loss: 4.8438 - train_accuracy: 0.6745 - val_loss: 5.1722 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 753/1000 - train_loss: 4.8436 - train_accuracy: 0.6747 - val_loss: 5.1724 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 754/1000 - train_loss: 4.8433 - train_accuracy: 0.6745 - val_loss: 5.1725 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 755/1000 - train_loss: 4.8431 - train_accuracy: 0.6744 - val_loss: 5.1727 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 756/1000 - train_loss: 4.8429 - train_accuracy: 0.6744 - val_loss: 5.1729 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 757/1000 - train_loss: 4.8426 - train_accuracy: 0.6744 - val_loss: 5.1730 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 758/1000 - train_loss: 4.8424 - train_accuracy: 0.6748 - val_loss: 5.1732 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 759/1000 - train_loss: 4.8422 - train_accuracy: 0.6752 - val_loss: 5.1734 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 760/1000 - train_loss: 4.8419 - train_accuracy: 0.6750 - val_loss: 5.1735 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 761/1000 - train_loss: 4.8417 - train_accuracy: 0.6750 - val_loss: 5.1737 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 762/1000 - train_loss: 4.8415 - train_accuracy: 0.6748 - val_loss: 5.1739 - val_accuracy: 0.6458\n",
      "(800, 8) (800, 8)\n",
      "Epoch 763/1000 - train_loss: 4.8412 - train_accuracy: 0.6745 - val_loss: 5.1740 - val_accuracy: 0.6458\n",
      "(800, 8) (800, 8)\n",
      "Epoch 764/1000 - train_loss: 4.8410 - train_accuracy: 0.6750 - val_loss: 5.1742 - val_accuracy: 0.6458\n",
      "(800, 8) (800, 8)\n",
      "Epoch 765/1000 - train_loss: 4.8407 - train_accuracy: 0.6744 - val_loss: 5.1743 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 766/1000 - train_loss: 4.8405 - train_accuracy: 0.6744 - val_loss: 5.1745 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 767/1000 - train_loss: 4.8403 - train_accuracy: 0.6741 - val_loss: 5.1747 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 768/1000 - train_loss: 4.8400 - train_accuracy: 0.6736 - val_loss: 5.1748 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 769/1000 - train_loss: 4.8398 - train_accuracy: 0.6736 - val_loss: 5.1750 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 770/1000 - train_loss: 4.8395 - train_accuracy: 0.6736 - val_loss: 5.1751 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 771/1000 - train_loss: 4.8393 - train_accuracy: 0.6737 - val_loss: 5.1753 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 772/1000 - train_loss: 4.8391 - train_accuracy: 0.6737 - val_loss: 5.1754 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 773/1000 - train_loss: 4.8388 - train_accuracy: 0.6736 - val_loss: 5.1756 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 774/1000 - train_loss: 4.8386 - train_accuracy: 0.6733 - val_loss: 5.1757 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 775/1000 - train_loss: 4.8383 - train_accuracy: 0.6730 - val_loss: 5.1759 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 776/1000 - train_loss: 4.8381 - train_accuracy: 0.6731 - val_loss: 5.1760 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 777/1000 - train_loss: 4.8378 - train_accuracy: 0.6733 - val_loss: 5.1762 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 778/1000 - train_loss: 4.8376 - train_accuracy: 0.6734 - val_loss: 5.1763 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 779/1000 - train_loss: 4.8373 - train_accuracy: 0.6734 - val_loss: 5.1765 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 780/1000 - train_loss: 4.8371 - train_accuracy: 0.6736 - val_loss: 5.1766 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 781/1000 - train_loss: 4.8368 - train_accuracy: 0.6734 - val_loss: 5.1768 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 782/1000 - train_loss: 4.8366 - train_accuracy: 0.6733 - val_loss: 5.1769 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 783/1000 - train_loss: 4.8363 - train_accuracy: 0.6733 - val_loss: 5.1771 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 784/1000 - train_loss: 4.8361 - train_accuracy: 0.6733 - val_loss: 5.1772 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 785/1000 - train_loss: 4.8358 - train_accuracy: 0.6734 - val_loss: 5.1774 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 786/1000 - train_loss: 4.8356 - train_accuracy: 0.6737 - val_loss: 5.1775 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 787/1000 - train_loss: 4.8353 - train_accuracy: 0.6736 - val_loss: 5.1777 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 788/1000 - train_loss: 4.8351 - train_accuracy: 0.6734 - val_loss: 5.1778 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 789/1000 - train_loss: 4.8348 - train_accuracy: 0.6731 - val_loss: 5.1780 - val_accuracy: 0.6408\n",
      "(800, 8) (800, 8)\n",
      "Epoch 790/1000 - train_loss: 4.8345 - train_accuracy: 0.6730 - val_loss: 5.1781 - val_accuracy: 0.6400\n",
      "(800, 8) (800, 8)\n",
      "Epoch 791/1000 - train_loss: 4.8343 - train_accuracy: 0.6730 - val_loss: 5.1783 - val_accuracy: 0.6400\n",
      "(800, 8) (800, 8)\n",
      "Epoch 792/1000 - train_loss: 4.8340 - train_accuracy: 0.6731 - val_loss: 5.1784 - val_accuracy: 0.6392\n",
      "(800, 8) (800, 8)\n",
      "Epoch 793/1000 - train_loss: 4.8338 - train_accuracy: 0.6730 - val_loss: 5.1786 - val_accuracy: 0.6375\n",
      "(800, 8) (800, 8)\n",
      "Epoch 794/1000 - train_loss: 4.8335 - train_accuracy: 0.6731 - val_loss: 5.1787 - val_accuracy: 0.6375\n",
      "(800, 8) (800, 8)\n",
      "Epoch 795/1000 - train_loss: 4.8332 - train_accuracy: 0.6730 - val_loss: 5.1789 - val_accuracy: 0.6375\n",
      "(800, 8) (800, 8)\n",
      "Epoch 796/1000 - train_loss: 4.8330 - train_accuracy: 0.6728 - val_loss: 5.1791 - val_accuracy: 0.6375\n",
      "(800, 8) (800, 8)\n",
      "Epoch 797/1000 - train_loss: 4.8327 - train_accuracy: 0.6728 - val_loss: 5.1792 - val_accuracy: 0.6375\n",
      "(800, 8) (800, 8)\n",
      "Epoch 798/1000 - train_loss: 4.8324 - train_accuracy: 0.6730 - val_loss: 5.1794 - val_accuracy: 0.6375\n",
      "(800, 8) (800, 8)\n",
      "Epoch 799/1000 - train_loss: 4.8322 - train_accuracy: 0.6730 - val_loss: 5.1795 - val_accuracy: 0.6375\n",
      "(800, 8) (800, 8)\n",
      "Epoch 800/1000 - train_loss: 4.8319 - train_accuracy: 0.6728 - val_loss: 5.1797 - val_accuracy: 0.6375\n",
      "(800, 8) (800, 8)\n",
      "Epoch 801/1000 - train_loss: 4.8316 - train_accuracy: 0.6730 - val_loss: 5.1799 - val_accuracy: 0.6375\n",
      "(800, 8) (800, 8)\n",
      "Epoch 802/1000 - train_loss: 4.8313 - train_accuracy: 0.6730 - val_loss: 5.1800 - val_accuracy: 0.6375\n",
      "(800, 8) (800, 8)\n",
      "Epoch 803/1000 - train_loss: 4.8311 - train_accuracy: 0.6730 - val_loss: 5.1802 - val_accuracy: 0.6375\n",
      "(800, 8) (800, 8)\n",
      "Epoch 804/1000 - train_loss: 4.8308 - train_accuracy: 0.6727 - val_loss: 5.1804 - val_accuracy: 0.6383\n",
      "(800, 8) (800, 8)\n",
      "Epoch 805/1000 - train_loss: 4.8305 - train_accuracy: 0.6727 - val_loss: 5.1806 - val_accuracy: 0.6383\n",
      "(800, 8) (800, 8)\n",
      "Epoch 806/1000 - train_loss: 4.8302 - train_accuracy: 0.6728 - val_loss: 5.1807 - val_accuracy: 0.6392\n",
      "(800, 8) (800, 8)\n",
      "Epoch 807/1000 - train_loss: 4.8300 - train_accuracy: 0.6731 - val_loss: 5.1809 - val_accuracy: 0.6392\n",
      "(800, 8) (800, 8)\n",
      "Epoch 808/1000 - train_loss: 4.8297 - train_accuracy: 0.6733 - val_loss: 5.1811 - val_accuracy: 0.6392\n",
      "(800, 8) (800, 8)\n",
      "Epoch 809/1000 - train_loss: 4.8294 - train_accuracy: 0.6733 - val_loss: 5.1813 - val_accuracy: 0.6392\n",
      "(800, 8) (800, 8)\n",
      "Epoch 810/1000 - train_loss: 4.8291 - train_accuracy: 0.6730 - val_loss: 5.1814 - val_accuracy: 0.6392\n",
      "(800, 8) (800, 8)\n",
      "Epoch 811/1000 - train_loss: 4.8288 - train_accuracy: 0.6731 - val_loss: 5.1816 - val_accuracy: 0.6392\n",
      "(800, 8) (800, 8)\n",
      "Epoch 812/1000 - train_loss: 4.8285 - train_accuracy: 0.6727 - val_loss: 5.1818 - val_accuracy: 0.6408\n",
      "(800, 8) (800, 8)\n",
      "Epoch 813/1000 - train_loss: 4.8282 - train_accuracy: 0.6730 - val_loss: 5.1820 - val_accuracy: 0.6400\n",
      "(800, 8) (800, 8)\n",
      "Epoch 814/1000 - train_loss: 4.8280 - train_accuracy: 0.6728 - val_loss: 5.1822 - val_accuracy: 0.6408\n",
      "(800, 8) (800, 8)\n",
      "Epoch 815/1000 - train_loss: 4.8277 - train_accuracy: 0.6728 - val_loss: 5.1824 - val_accuracy: 0.6408\n",
      "(800, 8) (800, 8)\n",
      "Epoch 816/1000 - train_loss: 4.8274 - train_accuracy: 0.6731 - val_loss: 5.1826 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 817/1000 - train_loss: 4.8271 - train_accuracy: 0.6731 - val_loss: 5.1828 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 818/1000 - train_loss: 4.8268 - train_accuracy: 0.6733 - val_loss: 5.1830 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 819/1000 - train_loss: 4.8265 - train_accuracy: 0.6733 - val_loss: 5.1832 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 820/1000 - train_loss: 4.8262 - train_accuracy: 0.6733 - val_loss: 5.1834 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 821/1000 - train_loss: 4.8259 - train_accuracy: 0.6736 - val_loss: 5.1836 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 822/1000 - train_loss: 4.8256 - train_accuracy: 0.6739 - val_loss: 5.1838 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 823/1000 - train_loss: 4.8253 - train_accuracy: 0.6741 - val_loss: 5.1841 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 824/1000 - train_loss: 4.8250 - train_accuracy: 0.6741 - val_loss: 5.1843 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 825/1000 - train_loss: 4.8247 - train_accuracy: 0.6742 - val_loss: 5.1845 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 826/1000 - train_loss: 4.8244 - train_accuracy: 0.6742 - val_loss: 5.1847 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 827/1000 - train_loss: 4.8241 - train_accuracy: 0.6745 - val_loss: 5.1850 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 828/1000 - train_loss: 4.8238 - train_accuracy: 0.6747 - val_loss: 5.1852 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 829/1000 - train_loss: 4.8235 - train_accuracy: 0.6747 - val_loss: 5.1854 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 830/1000 - train_loss: 4.8232 - train_accuracy: 0.6745 - val_loss: 5.1857 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 831/1000 - train_loss: 4.8229 - train_accuracy: 0.6747 - val_loss: 5.1859 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 832/1000 - train_loss: 4.8226 - train_accuracy: 0.6745 - val_loss: 5.1862 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 833/1000 - train_loss: 4.8223 - train_accuracy: 0.6748 - val_loss: 5.1864 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 834/1000 - train_loss: 4.8220 - train_accuracy: 0.6745 - val_loss: 5.1867 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 835/1000 - train_loss: 4.8217 - train_accuracy: 0.6750 - val_loss: 5.1869 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 836/1000 - train_loss: 4.8214 - train_accuracy: 0.6752 - val_loss: 5.1872 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 837/1000 - train_loss: 4.8211 - train_accuracy: 0.6750 - val_loss: 5.1874 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 838/1000 - train_loss: 4.8208 - train_accuracy: 0.6750 - val_loss: 5.1877 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 839/1000 - train_loss: 4.8205 - train_accuracy: 0.6745 - val_loss: 5.1880 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 840/1000 - train_loss: 4.8202 - train_accuracy: 0.6744 - val_loss: 5.1882 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 841/1000 - train_loss: 4.8199 - train_accuracy: 0.6742 - val_loss: 5.1885 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 842/1000 - train_loss: 4.8196 - train_accuracy: 0.6742 - val_loss: 5.1888 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 843/1000 - train_loss: 4.8193 - train_accuracy: 0.6747 - val_loss: 5.1891 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 844/1000 - train_loss: 4.8190 - train_accuracy: 0.6748 - val_loss: 5.1894 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 845/1000 - train_loss: 4.8186 - train_accuracy: 0.6748 - val_loss: 5.1896 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 846/1000 - train_loss: 4.8183 - train_accuracy: 0.6748 - val_loss: 5.1899 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 847/1000 - train_loss: 4.8180 - train_accuracy: 0.6748 - val_loss: 5.1902 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 848/1000 - train_loss: 4.8177 - train_accuracy: 0.6752 - val_loss: 5.1905 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 849/1000 - train_loss: 4.8174 - train_accuracy: 0.6750 - val_loss: 5.1908 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 850/1000 - train_loss: 4.8171 - train_accuracy: 0.6753 - val_loss: 5.1911 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 851/1000 - train_loss: 4.8168 - train_accuracy: 0.6753 - val_loss: 5.1914 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 852/1000 - train_loss: 4.8165 - train_accuracy: 0.6750 - val_loss: 5.1917 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 853/1000 - train_loss: 4.8162 - train_accuracy: 0.6750 - val_loss: 5.1920 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 854/1000 - train_loss: 4.8159 - train_accuracy: 0.6748 - val_loss: 5.1923 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 855/1000 - train_loss: 4.8156 - train_accuracy: 0.6748 - val_loss: 5.1926 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 856/1000 - train_loss: 4.8153 - train_accuracy: 0.6750 - val_loss: 5.1929 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 857/1000 - train_loss: 4.8150 - train_accuracy: 0.6752 - val_loss: 5.1932 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 858/1000 - train_loss: 4.8147 - train_accuracy: 0.6755 - val_loss: 5.1935 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 859/1000 - train_loss: 4.8144 - train_accuracy: 0.6755 - val_loss: 5.1938 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 860/1000 - train_loss: 4.8141 - train_accuracy: 0.6755 - val_loss: 5.1941 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 861/1000 - train_loss: 4.8138 - train_accuracy: 0.6755 - val_loss: 5.1944 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 862/1000 - train_loss: 4.8135 - train_accuracy: 0.6755 - val_loss: 5.1947 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 863/1000 - train_loss: 4.8132 - train_accuracy: 0.6750 - val_loss: 5.1950 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 864/1000 - train_loss: 4.8129 - train_accuracy: 0.6750 - val_loss: 5.1953 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 865/1000 - train_loss: 4.8127 - train_accuracy: 0.6752 - val_loss: 5.1957 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 866/1000 - train_loss: 4.8124 - train_accuracy: 0.6750 - val_loss: 5.1959 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 867/1000 - train_loss: 4.8121 - train_accuracy: 0.6748 - val_loss: 5.1962 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 868/1000 - train_loss: 4.8118 - train_accuracy: 0.6748 - val_loss: 5.1965 - val_accuracy: 0.6417\n",
      "(800, 8) (800, 8)\n",
      "Epoch 869/1000 - train_loss: 4.8115 - train_accuracy: 0.6748 - val_loss: 5.1968 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 870/1000 - train_loss: 4.8112 - train_accuracy: 0.6748 - val_loss: 5.1971 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 871/1000 - train_loss: 4.8109 - train_accuracy: 0.6750 - val_loss: 5.1974 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 872/1000 - train_loss: 4.8106 - train_accuracy: 0.6752 - val_loss: 5.1977 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 873/1000 - train_loss: 4.8104 - train_accuracy: 0.6752 - val_loss: 5.1980 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 874/1000 - train_loss: 4.8101 - train_accuracy: 0.6752 - val_loss: 5.1983 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 875/1000 - train_loss: 4.8098 - train_accuracy: 0.6752 - val_loss: 5.1986 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 876/1000 - train_loss: 4.8095 - train_accuracy: 0.6753 - val_loss: 5.1988 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 877/1000 - train_loss: 4.8093 - train_accuracy: 0.6753 - val_loss: 5.1991 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 878/1000 - train_loss: 4.8090 - train_accuracy: 0.6750 - val_loss: 5.1994 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 879/1000 - train_loss: 4.8087 - train_accuracy: 0.6752 - val_loss: 5.1997 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 880/1000 - train_loss: 4.8084 - train_accuracy: 0.6752 - val_loss: 5.1999 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 881/1000 - train_loss: 4.8082 - train_accuracy: 0.6752 - val_loss: 5.2002 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 882/1000 - train_loss: 4.8079 - train_accuracy: 0.6750 - val_loss: 5.2005 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 883/1000 - train_loss: 4.8076 - train_accuracy: 0.6750 - val_loss: 5.2007 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 884/1000 - train_loss: 4.8074 - train_accuracy: 0.6750 - val_loss: 5.2010 - val_accuracy: 0.6433\n",
      "(800, 8) (800, 8)\n",
      "Epoch 885/1000 - train_loss: 4.8071 - train_accuracy: 0.6750 - val_loss: 5.2012 - val_accuracy: 0.6425\n",
      "(800, 8) (800, 8)\n",
      "Epoch 886/1000 - train_loss: 4.8069 - train_accuracy: 0.6750 - val_loss: 5.2015 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 887/1000 - train_loss: 4.8066 - train_accuracy: 0.6752 - val_loss: 5.2017 - val_accuracy: 0.6442\n",
      "(800, 8) (800, 8)\n",
      "Epoch 888/1000 - train_loss: 4.8063 - train_accuracy: 0.6750 - val_loss: 5.2020 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 889/1000 - train_loss: 4.8061 - train_accuracy: 0.6748 - val_loss: 5.2022 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 890/1000 - train_loss: 4.8058 - train_accuracy: 0.6748 - val_loss: 5.2025 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 891/1000 - train_loss: 4.8056 - train_accuracy: 0.6750 - val_loss: 5.2027 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 892/1000 - train_loss: 4.8053 - train_accuracy: 0.6753 - val_loss: 5.2029 - val_accuracy: 0.6450\n",
      "(800, 8) (800, 8)\n",
      "Epoch 893/1000 - train_loss: 4.8051 - train_accuracy: 0.6753 - val_loss: 5.2032 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 894/1000 - train_loss: 4.8048 - train_accuracy: 0.6753 - val_loss: 5.2034 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 895/1000 - train_loss: 4.8046 - train_accuracy: 0.6753 - val_loss: 5.2036 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 896/1000 - train_loss: 4.8044 - train_accuracy: 0.6755 - val_loss: 5.2038 - val_accuracy: 0.6458\n",
      "(800, 8) (800, 8)\n",
      "Epoch 897/1000 - train_loss: 4.8041 - train_accuracy: 0.6756 - val_loss: 5.2041 - val_accuracy: 0.6458\n",
      "(800, 8) (800, 8)\n",
      "Epoch 898/1000 - train_loss: 4.8039 - train_accuracy: 0.6756 - val_loss: 5.2043 - val_accuracy: 0.6467\n",
      "(800, 8) (800, 8)\n",
      "Epoch 899/1000 - train_loss: 4.8036 - train_accuracy: 0.6756 - val_loss: 5.2045 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 900/1000 - train_loss: 4.8034 - train_accuracy: 0.6756 - val_loss: 5.2047 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 901/1000 - train_loss: 4.8032 - train_accuracy: 0.6758 - val_loss: 5.2049 - val_accuracy: 0.6475\n",
      "(800, 8) (800, 8)\n",
      "Epoch 902/1000 - train_loss: 4.8030 - train_accuracy: 0.6758 - val_loss: 5.2051 - val_accuracy: 0.6483\n",
      "(800, 8) (800, 8)\n",
      "Epoch 903/1000 - train_loss: 4.8027 - train_accuracy: 0.6758 - val_loss: 5.2053 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 904/1000 - train_loss: 4.8025 - train_accuracy: 0.6758 - val_loss: 5.2055 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 905/1000 - train_loss: 4.8023 - train_accuracy: 0.6759 - val_loss: 5.2057 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 906/1000 - train_loss: 4.8021 - train_accuracy: 0.6759 - val_loss: 5.2059 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 907/1000 - train_loss: 4.8018 - train_accuracy: 0.6761 - val_loss: 5.2061 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 908/1000 - train_loss: 4.8016 - train_accuracy: 0.6761 - val_loss: 5.2063 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 909/1000 - train_loss: 4.8014 - train_accuracy: 0.6761 - val_loss: 5.2065 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 910/1000 - train_loss: 4.8012 - train_accuracy: 0.6763 - val_loss: 5.2067 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 911/1000 - train_loss: 4.8010 - train_accuracy: 0.6763 - val_loss: 5.2069 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 912/1000 - train_loss: 4.8008 - train_accuracy: 0.6766 - val_loss: 5.2071 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 913/1000 - train_loss: 4.8005 - train_accuracy: 0.6766 - val_loss: 5.2073 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 914/1000 - train_loss: 4.8003 - train_accuracy: 0.6764 - val_loss: 5.2074 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 915/1000 - train_loss: 4.8001 - train_accuracy: 0.6764 - val_loss: 5.2076 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 916/1000 - train_loss: 4.7999 - train_accuracy: 0.6766 - val_loss: 5.2078 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 917/1000 - train_loss: 4.7997 - train_accuracy: 0.6766 - val_loss: 5.2080 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 918/1000 - train_loss: 4.7995 - train_accuracy: 0.6769 - val_loss: 5.2082 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 919/1000 - train_loss: 4.7993 - train_accuracy: 0.6770 - val_loss: 5.2083 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 920/1000 - train_loss: 4.7991 - train_accuracy: 0.6770 - val_loss: 5.2085 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 921/1000 - train_loss: 4.7989 - train_accuracy: 0.6773 - val_loss: 5.2087 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 922/1000 - train_loss: 4.7987 - train_accuracy: 0.6773 - val_loss: 5.2088 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 923/1000 - train_loss: 4.7985 - train_accuracy: 0.6772 - val_loss: 5.2090 - val_accuracy: 0.6492\n",
      "(800, 8) (800, 8)\n",
      "Epoch 924/1000 - train_loss: 4.7983 - train_accuracy: 0.6770 - val_loss: 5.2092 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 925/1000 - train_loss: 4.7982 - train_accuracy: 0.6770 - val_loss: 5.2094 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 926/1000 - train_loss: 4.7980 - train_accuracy: 0.6772 - val_loss: 5.2095 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 927/1000 - train_loss: 4.7978 - train_accuracy: 0.6772 - val_loss: 5.2097 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 928/1000 - train_loss: 4.7976 - train_accuracy: 0.6773 - val_loss: 5.2099 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 929/1000 - train_loss: 4.7974 - train_accuracy: 0.6775 - val_loss: 5.2100 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 930/1000 - train_loss: 4.7972 - train_accuracy: 0.6775 - val_loss: 5.2102 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 931/1000 - train_loss: 4.7970 - train_accuracy: 0.6778 - val_loss: 5.2104 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 932/1000 - train_loss: 4.7969 - train_accuracy: 0.6780 - val_loss: 5.2105 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 933/1000 - train_loss: 4.7967 - train_accuracy: 0.6778 - val_loss: 5.2107 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 934/1000 - train_loss: 4.7965 - train_accuracy: 0.6778 - val_loss: 5.2109 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 935/1000 - train_loss: 4.7963 - train_accuracy: 0.6778 - val_loss: 5.2110 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 936/1000 - train_loss: 4.7961 - train_accuracy: 0.6778 - val_loss: 5.2112 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 937/1000 - train_loss: 4.7960 - train_accuracy: 0.6778 - val_loss: 5.2114 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 938/1000 - train_loss: 4.7958 - train_accuracy: 0.6780 - val_loss: 5.2115 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 939/1000 - train_loss: 4.7956 - train_accuracy: 0.6781 - val_loss: 5.2117 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 940/1000 - train_loss: 4.7954 - train_accuracy: 0.6783 - val_loss: 5.2119 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 941/1000 - train_loss: 4.7953 - train_accuracy: 0.6783 - val_loss: 5.2120 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 942/1000 - train_loss: 4.7951 - train_accuracy: 0.6783 - val_loss: 5.2122 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 943/1000 - train_loss: 4.7949 - train_accuracy: 0.6783 - val_loss: 5.2123 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 944/1000 - train_loss: 4.7948 - train_accuracy: 0.6786 - val_loss: 5.2125 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 945/1000 - train_loss: 4.7946 - train_accuracy: 0.6786 - val_loss: 5.2127 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 946/1000 - train_loss: 4.7944 - train_accuracy: 0.6786 - val_loss: 5.2128 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 947/1000 - train_loss: 4.7943 - train_accuracy: 0.6786 - val_loss: 5.2130 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 948/1000 - train_loss: 4.7941 - train_accuracy: 0.6786 - val_loss: 5.2132 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 949/1000 - train_loss: 4.7939 - train_accuracy: 0.6786 - val_loss: 5.2133 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 950/1000 - train_loss: 4.7938 - train_accuracy: 0.6789 - val_loss: 5.2135 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 951/1000 - train_loss: 4.7936 - train_accuracy: 0.6789 - val_loss: 5.2137 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 952/1000 - train_loss: 4.7934 - train_accuracy: 0.6789 - val_loss: 5.2138 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 953/1000 - train_loss: 4.7933 - train_accuracy: 0.6791 - val_loss: 5.2140 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 954/1000 - train_loss: 4.7931 - train_accuracy: 0.6791 - val_loss: 5.2142 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 955/1000 - train_loss: 4.7929 - train_accuracy: 0.6791 - val_loss: 5.2143 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 956/1000 - train_loss: 4.7928 - train_accuracy: 0.6791 - val_loss: 5.2145 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 957/1000 - train_loss: 4.7926 - train_accuracy: 0.6791 - val_loss: 5.2147 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 958/1000 - train_loss: 4.7925 - train_accuracy: 0.6791 - val_loss: 5.2148 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 959/1000 - train_loss: 4.7923 - train_accuracy: 0.6791 - val_loss: 5.2150 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 960/1000 - train_loss: 4.7921 - train_accuracy: 0.6789 - val_loss: 5.2152 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 961/1000 - train_loss: 4.7920 - train_accuracy: 0.6791 - val_loss: 5.2153 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 962/1000 - train_loss: 4.7918 - train_accuracy: 0.6791 - val_loss: 5.2155 - val_accuracy: 0.6500\n",
      "(800, 8) (800, 8)\n",
      "Epoch 963/1000 - train_loss: 4.7917 - train_accuracy: 0.6791 - val_loss: 5.2157 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 964/1000 - train_loss: 4.7915 - train_accuracy: 0.6791 - val_loss: 5.2158 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 965/1000 - train_loss: 4.7913 - train_accuracy: 0.6791 - val_loss: 5.2160 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 966/1000 - train_loss: 4.7912 - train_accuracy: 0.6787 - val_loss: 5.2162 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 967/1000 - train_loss: 4.7910 - train_accuracy: 0.6789 - val_loss: 5.2163 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 968/1000 - train_loss: 4.7909 - train_accuracy: 0.6789 - val_loss: 5.2165 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 969/1000 - train_loss: 4.7907 - train_accuracy: 0.6789 - val_loss: 5.2166 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 970/1000 - train_loss: 4.7906 - train_accuracy: 0.6789 - val_loss: 5.2168 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 971/1000 - train_loss: 4.7904 - train_accuracy: 0.6791 - val_loss: 5.2170 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 972/1000 - train_loss: 4.7903 - train_accuracy: 0.6791 - val_loss: 5.2171 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 973/1000 - train_loss: 4.7901 - train_accuracy: 0.6789 - val_loss: 5.2173 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 974/1000 - train_loss: 4.7899 - train_accuracy: 0.6789 - val_loss: 5.2175 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 975/1000 - train_loss: 4.7898 - train_accuracy: 0.6789 - val_loss: 5.2176 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 976/1000 - train_loss: 4.7896 - train_accuracy: 0.6794 - val_loss: 5.2178 - val_accuracy: 0.6508\n",
      "(800, 8) (800, 8)\n",
      "Epoch 977/1000 - train_loss: 4.7895 - train_accuracy: 0.6795 - val_loss: 5.2180 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 978/1000 - train_loss: 4.7893 - train_accuracy: 0.6794 - val_loss: 5.2181 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 979/1000 - train_loss: 4.7892 - train_accuracy: 0.6792 - val_loss: 5.2183 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 980/1000 - train_loss: 4.7890 - train_accuracy: 0.6791 - val_loss: 5.2184 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 981/1000 - train_loss: 4.7889 - train_accuracy: 0.6791 - val_loss: 5.2186 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 982/1000 - train_loss: 4.7887 - train_accuracy: 0.6791 - val_loss: 5.2187 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 983/1000 - train_loss: 4.7886 - train_accuracy: 0.6789 - val_loss: 5.2189 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 984/1000 - train_loss: 4.7884 - train_accuracy: 0.6786 - val_loss: 5.2191 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 985/1000 - train_loss: 4.7883 - train_accuracy: 0.6787 - val_loss: 5.2192 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 986/1000 - train_loss: 4.7881 - train_accuracy: 0.6787 - val_loss: 5.2194 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 987/1000 - train_loss: 4.7880 - train_accuracy: 0.6787 - val_loss: 5.2195 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 988/1000 - train_loss: 4.7878 - train_accuracy: 0.6786 - val_loss: 5.2197 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 989/1000 - train_loss: 4.7876 - train_accuracy: 0.6786 - val_loss: 5.2198 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 990/1000 - train_loss: 4.7875 - train_accuracy: 0.6786 - val_loss: 5.2200 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 991/1000 - train_loss: 4.7873 - train_accuracy: 0.6786 - val_loss: 5.2201 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 992/1000 - train_loss: 4.7872 - train_accuracy: 0.6786 - val_loss: 5.2202 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 993/1000 - train_loss: 4.7870 - train_accuracy: 0.6784 - val_loss: 5.2204 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 994/1000 - train_loss: 4.7869 - train_accuracy: 0.6786 - val_loss: 5.2205 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 995/1000 - train_loss: 4.7867 - train_accuracy: 0.6786 - val_loss: 5.2207 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 996/1000 - train_loss: 4.7866 - train_accuracy: 0.6786 - val_loss: 5.2208 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 997/1000 - train_loss: 4.7864 - train_accuracy: 0.6787 - val_loss: 5.2209 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 998/1000 - train_loss: 4.7863 - train_accuracy: 0.6787 - val_loss: 5.2211 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 999/1000 - train_loss: 4.7861 - train_accuracy: 0.6789 - val_loss: 5.2212 - val_accuracy: 0.6517\n",
      "(800, 8) (800, 8)\n",
      "Epoch 1000/1000 - train_loss: 4.7859 - train_accuracy: 0.6789 - val_loss: 5.2213 - val_accuracy: 0.6517\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier_multilabel(learning_rate=0.01, activation='sigmoid', optimizer='sgd', hidden_layers=[8, 6], epochs=1000)\n",
    "\n",
    "# Train the model\n",
    "mlp.fit(train_x.astype(np.float64),train_y.astype(np.float64),val_x.astype(np.float64),val_y.astype(np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872667f0",
   "metadata": {},
   "source": [
    "### Test Report for multilabel classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3062fb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 10) (50, 8)\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape,test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbe14dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.6475\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = \",mlp.inference(test_x.astype(np.float64),test_y.astype(np.float64)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
